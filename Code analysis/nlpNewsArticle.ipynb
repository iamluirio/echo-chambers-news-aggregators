{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create a news article.txt\n",
    "2. read this file by python\n",
    "3. paragraph to sentences\n",
    "4.  sentences to words #tokenize\n",
    "\n",
    "for all of the text things (point 3,4) use nltk library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf55a6824e154ff5beecc50e8284c986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 12:32:13 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-04-20 12:32:14 INFO: File exists: /home/pierluigi/stanza_resources/en/default.zip\n",
      "2023-04-20 12:32:20 INFO: Finished downloading models and saved to /home/pierluigi/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import stanza\n",
    "stanza.download('en')  # Download the English model\n",
    "\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting a link into txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_article_as_txt(url, file_name=None):\n",
    "    # Send a request to the website\n",
    "    r = requests.get(url)\n",
    "\n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "    # Find the article content and extract the text\n",
    "    article = soup.find('article')\n",
    "    article_text = article.get_text()\n",
    "\n",
    "    # If no file name is specified, use the article title as the file name\n",
    "    if not file_name:\n",
    "        title = soup.find('title').get_text()\n",
    "        file_name = f\"{title}.txt\"\n",
    "\n",
    "    # Write the article text to the file\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(article_text)\n",
    "\n",
    "    print(f\"Article saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article saved to Federal judge strikes down controversial Missouri gun law | CNN Politics.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = input('https://edition.cnn.com/2023/03/07/politics/missouri-gun-law-unconstitutional/index.html')\n",
    "file_name = input(\"Enter a file name for the output text file (optional): \")\n",
    "save_article_as_txt(url, file_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the .txt file and converting it into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Republican presidential candidate Nikki Haley stepped into the hallway after speaking at the Conservative Political Action Conference on Friday to supporters asking for selfies and autographs — and, from others, a less friendly greeting.\n",
      "\n",
      "“We love Trump, we love Trump!” a crowd around her started chanting. Some Haley supporters shouted her name back as the former U.N. ambassador escaped with staff to an elevator.\n",
      "\n",
      "The dust-up showed the risks of taking the primary fight to what has clearly become Trump’s home turf. Though CPAC has long been seen as a big-tent forum for the conservative movement and a mandatory cattle call for presidential hopefuls, the annual conference has increasingly grown into a stomping ground for the 45th president and his “Make America Great Again” wing of the GOP. Trump will speak at the event Saturday.\n",
      "\n",
      "“Remember, you’re not at CPAC, you’re at TPAC,” John Fredericks, a pro-Trump talk radio host broadcasting from the sidelines here, said in an interview Wednesday. He said potential 2024 rivals opted to skip the conference rather than risk getting booed or losing the straw poll. “We own this thing, it’s ours,” he said. “No Trump, no CPAC.”\n",
      "\n",
      "This year’s lineup was heavy with Trump family members and acolytes — such as Lara Trump, Donald Trump Jr., former White House strategist Stephen K. Bannon, losing 2022 Arizona gubernatorial candidate Kari Lake, Sens. J.D. Vance (R-Ohio) and Ted Cruz (R-Tex.), and Reps. Marjorie Taylor Greene (R-Ga.), Lauren Boebert (R-Colo.) and Matt Gaetz (R-Fla.) — to the near-total exclusion of the party’s other voices.\n",
      "\n",
      "Florida Gov. Ron DeSantis, who polls show as Trump’s biggest competitor for the 2024 primary though he has not yet announced whether he is running, opted to spend the week far away at his own events promoting his new book.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the article variable\n",
    "article = \"\"\n",
    "\n",
    "# read input file\n",
    "filepath = input(\"/home/pierluigi/Documents/echo_chambers_intership/newsArticle.txt\")\n",
    "with open(filepath, 'r') as file:\n",
    "    # assign the contents of the file to the article variable\n",
    "    article = file.read()\n",
    "\n",
    "# print the contents of the file\n",
    "print(article)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Republican presidential candidate Nikki Haley stepped into the hallway after speaking at the Conservative Political Action Conference on Friday to supporters asking for selfies and autographs — and, from others, a less friendly greeting.', '“We love Trump, we love Trump!” a crowd around her started chanting.', 'Some Haley supporters shouted her name back as the former U.N. ambassador escaped with staff to an elevator.', 'The dust-up showed the risks of taking the primary fight to what has clearly become Trump’s home turf.', 'Though CPAC has long been seen as a big-tent forum for the conservative movement and a mandatory cattle call for presidential hopefuls, the annual conference has increasingly grown into a stomping ground for the 45th president and his “Make America Great Again” wing of the GOP.', 'Trump will speak at the event Saturday.', '“Remember, you’re not at CPAC, you’re at TPAC,” John Fredericks, a pro-Trump talk radio host broadcasting from the sidelines here, said in an interview Wednesday.', 'He said potential 2024 rivals opted to skip the conference rather than risk getting booed or losing the straw poll.', '“We own this thing, it’s ours,” he said.', '“No Trump, no CPAC.”\\n\\nThis year’s lineup was heavy with Trump family members and acolytes — such as Lara Trump, Donald Trump Jr., former White House strategist Stephen K. Bannon, losing 2022 Arizona gubernatorial candidate Kari Lake, Sens.', 'J.D.', 'Vance (R-Ohio) and Ted Cruz (R-Tex.', '), and Reps. Marjorie Taylor Greene (R-Ga.), Lauren Boebert (R-Colo.) and Matt Gaetz (R-Fla.) — to the near-total exclusion of the party’s other voices.', 'Florida Gov.', 'Ron DeSantis, who polls show as Trump’s biggest competitor for the 2024 primary though he has not yet announced whether he is running, opted to spend the week far away at his own events promoting his new book.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(article)\n",
    "print(sentences)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to print sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Republican presidential candidate Nikki Haley stepped into the hallway after speaking at the Conservative Political Action Conference on Friday to supporters asking for selfies and autographs — and, from others, a less friendly greeting.\n",
      "“We love Trump, we love Trump!” a crowd around her started chanting.\n",
      "Some Haley supporters shouted her name back as the former U.N. ambassador escaped with staff to an elevator.\n",
      "The dust-up showed the risks of taking the primary fight to what has clearly become Trump’s home turf.\n",
      "Though CPAC has long been seen as a big-tent forum for the conservative movement and a mandatory cattle call for presidential hopefuls, the annual conference has increasingly grown into a stomping ground for the 45th president and his “Make America Great Again” wing of the GOP.\n",
      "Trump will speak at the event Saturday.\n",
      "“Remember, you’re not at CPAC, you’re at TPAC,” John Fredericks, a pro-Trump talk radio host broadcasting from the sidelines here, said in an interview Wednesday.\n",
      "He said potential 2024 rivals opted to skip the conference rather than risk getting booed or losing the straw poll.\n",
      "“We own this thing, it’s ours,” he said.\n",
      "“No Trump, no CPAC.”\n",
      "\n",
      "This year’s lineup was heavy with Trump family members and acolytes — such as Lara Trump, Donald Trump Jr., former White House strategist Stephen K. Bannon, losing 2022 Arizona gubernatorial candidate Kari Lake, Sens.\n",
      "J.D.\n",
      "Vance (R-Ohio) and Ted Cruz (R-Tex.\n",
      "), and Reps. Marjorie Taylor Greene (R-Ga.), Lauren Boebert (R-Colo.) and Matt Gaetz (R-Fla.) — to the near-total exclusion of the party’s other voices.\n",
      "Florida Gov.\n",
      "Ron DeSantis, who polls show as Trump’s biggest competitor for the 2024 primary though he has not yet announced whether he is running, opted to spend the week far away at his own events promoting his new book.\n"
     ]
    }
   ],
   "source": [
    "# Print out each sentence\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Republican', 'presidential', 'candidate', 'Nikki', 'Haley', 'stepped', 'into', 'the', 'hallway', 'after', 'speaking', 'at', 'the', 'Conservative', 'Political', 'Action', 'Conference', 'on', 'Friday', 'to', 'supporters', 'asking', 'for', 'selfies', 'and', 'autographs', '—', 'and', ',', 'from', 'others', ',', 'a', 'less', 'friendly', 'greeting', '.']\n",
      "['“', 'We', 'love', 'Trump', ',', 'we', 'love', 'Trump', '!', '”', 'a', 'crowd', 'around', 'her', 'started', 'chanting', '.']\n",
      "['Some', 'Haley', 'supporters', 'shouted', 'her', 'name', 'back', 'as', 'the', 'former', 'U.N.', 'ambassador', 'escaped', 'with', 'staff', 'to', 'an', 'elevator', '.']\n",
      "['The', 'dust-up', 'showed', 'the', 'risks', 'of', 'taking', 'the', 'primary', 'fight', 'to', 'what', 'has', 'clearly', 'become', 'Trump', '’', 's', 'home', 'turf', '.']\n",
      "['Though', 'CPAC', 'has', 'long', 'been', 'seen', 'as', 'a', 'big-tent', 'forum', 'for', 'the', 'conservative', 'movement', 'and', 'a', 'mandatory', 'cattle', 'call', 'for', 'presidential', 'hopefuls', ',', 'the', 'annual', 'conference', 'has', 'increasingly', 'grown', 'into', 'a', 'stomping', 'ground', 'for', 'the', '45th', 'president', 'and', 'his', '“', 'Make', 'America', 'Great', 'Again', '”', 'wing', 'of', 'the', 'GOP', '.']\n",
      "['Trump', 'will', 'speak', 'at', 'the', 'event', 'Saturday', '.']\n",
      "['“', 'Remember', ',', 'you', '’', 're', 'not', 'at', 'CPAC', ',', 'you', '’', 're', 'at', 'TPAC', ',', '”', 'John', 'Fredericks', ',', 'a', 'pro-Trump', 'talk', 'radio', 'host', 'broadcasting', 'from', 'the', 'sidelines', 'here', ',', 'said', 'in', 'an', 'interview', 'Wednesday', '.']\n",
      "['He', 'said', 'potential', '2024', 'rivals', 'opted', 'to', 'skip', 'the', 'conference', 'rather', 'than', 'risk', 'getting', 'booed', 'or', 'losing', 'the', 'straw', 'poll', '.']\n",
      "['“', 'We', 'own', 'this', 'thing', ',', 'it', '’', 's', 'ours', ',', '”', 'he', 'said', '.']\n",
      "['“', 'No', 'Trump', ',', 'no', 'CPAC.', '”', 'This', 'year', '’', 's', 'lineup', 'was', 'heavy', 'with', 'Trump', 'family', 'members', 'and', 'acolytes', '—', 'such', 'as', 'Lara', 'Trump', ',', 'Donald', 'Trump', 'Jr.', ',', 'former', 'White', 'House', 'strategist', 'Stephen', 'K.', 'Bannon', ',', 'losing', '2022', 'Arizona', 'gubernatorial', 'candidate', 'Kari', 'Lake', ',', 'Sens', '.']\n",
      "['J.D', '.']\n",
      "['Vance', '(', 'R-Ohio', ')', 'and', 'Ted', 'Cruz', '(', 'R-Tex', '.']\n",
      "[')', ',', 'and', 'Reps.', 'Marjorie', 'Taylor', 'Greene', '(', 'R-Ga.', ')', ',', 'Lauren', 'Boebert', '(', 'R-Colo.', ')', 'and', 'Matt', 'Gaetz', '(', 'R-Fla.', ')', '—', 'to', 'the', 'near-total', 'exclusion', 'of', 'the', 'party', '’', 's', 'other', 'voices', '.']\n",
      "['Florida', 'Gov', '.']\n",
      "['Ron', 'DeSantis', ',', 'who', 'polls', 'show', 'as', 'Trump', '’', 's', 'biggest', 'competitor', 'for', 'the', '2024', 'primary', 'though', 'he', 'has', 'not', 'yet', 'announced', 'whether', 'he', 'is', 'running', ',', 'opted', 'to', 'spend', 'the', 'week', 'far', 'away', 'at', 'his', 'own', 'events', 'promoting', 'his', 'new', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "\n",
    "    # Tokenize the sentence into words\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    print(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Stop word removal\n",
    "2. lemmatization and stemming \n",
    "3. Sentiment analysis a) stanza library, b) vader, c) senti wordnet and d)mpqa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence  1\n",
      "Total words: 37\n",
      "Filtered words: ['Republican', 'presidential', 'candidate', 'Nikki', 'Haley', 'stepped', 'hallway', 'speaking', 'Conservative', 'Political', 'Action', 'Conference', 'Friday', 'supporters', 'asking', 'selfies', 'autographs', '—', ',', 'others', ',', 'less', 'friendly', 'greeting', '.']\n",
      "Number of filtered words: 25\n",
      "Stop words identified: ['into', 'the', 'after', 'at', 'the', 'on', 'to', 'for', 'and', 'and', 'from', 'a']\n",
      "Number of stop words identified: 12\n",
      "\n",
      "Sentence  2\n",
      "Total words: 17\n",
      "Filtered words: ['“', 'love', 'Trump', ',', 'love', 'Trump', '!', '”', 'crowd', 'around', 'started', 'chanting', '.']\n",
      "Number of filtered words: 13\n",
      "Stop words identified: ['We', 'we', 'a', 'her']\n",
      "Number of stop words identified: 4\n",
      "\n",
      "Sentence  3\n",
      "Total words: 19\n",
      "Filtered words: ['Haley', 'supporters', 'shouted', 'name', 'back', 'former', 'U.N.', 'ambassador', 'escaped', 'staff', 'elevator', '.']\n",
      "Number of filtered words: 12\n",
      "Stop words identified: ['Some', 'her', 'as', 'the', 'with', 'to', 'an']\n",
      "Number of stop words identified: 7\n",
      "\n",
      "Sentence  4\n",
      "Total words: 21\n",
      "Filtered words: ['dust-up', 'showed', 'risks', 'taking', 'primary', 'fight', 'clearly', 'become', 'Trump', '’', 'home', 'turf', '.']\n",
      "Number of filtered words: 13\n",
      "Stop words identified: ['The', 'the', 'of', 'the', 'to', 'what', 'has', 's']\n",
      "Number of stop words identified: 8\n",
      "\n",
      "Sentence  5\n",
      "Total words: 50\n",
      "Filtered words: ['Though', 'CPAC', 'long', 'seen', 'big-tent', 'forum', 'conservative', 'movement', 'mandatory', 'cattle', 'call', 'presidential', 'hopefuls', ',', 'annual', 'conference', 'increasingly', 'grown', 'stomping', 'ground', '45th', 'president', '“', 'Make', 'America', 'Great', '”', 'wing', 'GOP', '.']\n",
      "Number of filtered words: 30\n",
      "Stop words identified: ['has', 'been', 'as', 'a', 'for', 'the', 'and', 'a', 'for', 'the', 'has', 'into', 'a', 'for', 'the', 'and', 'his', 'Again', 'of', 'the']\n",
      "Number of stop words identified: 20\n",
      "\n",
      "Sentence  6\n",
      "Total words: 8\n",
      "Filtered words: ['Trump', 'speak', 'event', 'Saturday', '.']\n",
      "Number of filtered words: 5\n",
      "Stop words identified: ['will', 'at', 'the']\n",
      "Number of stop words identified: 3\n",
      "\n",
      "Sentence  7\n",
      "Total words: 37\n",
      "Filtered words: ['“', 'Remember', ',', '’', 'CPAC', ',', '’', 'TPAC', ',', '”', 'John', 'Fredericks', ',', 'pro-Trump', 'talk', 'radio', 'host', 'broadcasting', 'sidelines', ',', 'said', 'interview', 'Wednesday', '.']\n",
      "Number of filtered words: 24\n",
      "Stop words identified: ['you', 're', 'not', 'at', 'you', 're', 'at', 'a', 'from', 'the', 'here', 'in', 'an']\n",
      "Number of stop words identified: 13\n",
      "\n",
      "Sentence  8\n",
      "Total words: 21\n",
      "Filtered words: ['said', 'potential', '2024', 'rivals', 'opted', 'skip', 'conference', 'rather', 'risk', 'getting', 'booed', 'losing', 'straw', 'poll', '.']\n",
      "Number of filtered words: 15\n",
      "Stop words identified: ['He', 'to', 'the', 'than', 'or', 'the']\n",
      "Number of stop words identified: 6\n",
      "\n",
      "Sentence  9\n",
      "Total words: 15\n",
      "Filtered words: ['“', 'thing', ',', '’', ',', '”', 'said', '.']\n",
      "Number of filtered words: 8\n",
      "Stop words identified: ['We', 'own', 'this', 'it', 's', 'ours', 'he']\n",
      "Number of stop words identified: 7\n",
      "\n",
      "Sentence  10\n",
      "Total words: 48\n",
      "Filtered words: ['“', 'Trump', ',', 'CPAC.', '”', 'year', '’', 'lineup', 'heavy', 'Trump', 'family', 'members', 'acolytes', '—', 'Lara', 'Trump', ',', 'Donald', 'Trump', 'Jr.', ',', 'former', 'White', 'House', 'strategist', 'Stephen', 'K.', 'Bannon', ',', 'losing', '2022', 'Arizona', 'gubernatorial', 'candidate', 'Kari', 'Lake', ',', 'Sens', '.']\n",
      "Number of filtered words: 39\n",
      "Stop words identified: ['No', 'no', 'This', 's', 'was', 'with', 'and', 'such', 'as']\n",
      "Number of stop words identified: 9\n",
      "\n",
      "Sentence  11\n",
      "Total words: 2\n",
      "Filtered words: ['J.D', '.']\n",
      "Number of filtered words: 2\n",
      "Stop words identified: []\n",
      "Number of stop words identified: 0\n",
      "\n",
      "Sentence  12\n",
      "Total words: 10\n",
      "Filtered words: ['Vance', '(', 'R-Ohio', ')', 'Ted', 'Cruz', '(', 'R-Tex', '.']\n",
      "Number of filtered words: 9\n",
      "Stop words identified: ['and']\n",
      "Number of stop words identified: 1\n",
      "\n",
      "Sentence  13\n",
      "Total words: 35\n",
      "Filtered words: [')', ',', 'Reps.', 'Marjorie', 'Taylor', 'Greene', '(', 'R-Ga.', ')', ',', 'Lauren', 'Boebert', '(', 'R-Colo.', ')', 'Matt', 'Gaetz', '(', 'R-Fla.', ')', '—', 'near-total', 'exclusion', 'party', '’', 'voices', '.']\n",
      "Number of filtered words: 27\n",
      "Stop words identified: ['and', 'and', 'to', 'the', 'of', 'the', 's', 'other']\n",
      "Number of stop words identified: 8\n",
      "\n",
      "Sentence  14\n",
      "Total words: 3\n",
      "Filtered words: ['Florida', 'Gov', '.']\n",
      "Number of filtered words: 3\n",
      "Stop words identified: []\n",
      "Number of stop words identified: 0\n",
      "\n",
      "Sentence  15\n",
      "Total words: 43\n",
      "Filtered words: ['Ron', 'DeSantis', ',', 'polls', 'show', 'Trump', '’', 'biggest', 'competitor', '2024', 'primary', 'though', 'yet', 'announced', 'whether', 'running', ',', 'opted', 'spend', 'week', 'far', 'away', 'events', 'promoting', 'new', 'book', '.']\n",
      "Number of filtered words: 27\n",
      "Stop words identified: ['who', 'as', 's', 'for', 'the', 'he', 'has', 'not', 'he', 'is', 'to', 'the', 'at', 'his', 'own', 'his']\n",
      "Number of stop words identified: 16\n",
      "\n",
      "Total number of words: 366\n"
     ]
    }
   ],
   "source": [
    "total_words = 0\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    # Tokenize the sentence into words\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    # Identify the stop words in the sentence\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    stop_words_found = [word for word in words if word.lower() in stop_words]\n",
    "    \n",
    "    # Count all the words in the sentence\n",
    "    all_words = len(words)\n",
    "    total_words += all_words  # add the count of all_words to the total_words variable\n",
    "\n",
    "    # Count all the stop words in the sentence\n",
    "    all_stop_words = len(stop_words_found)\n",
    "\n",
    "    # Print out the results for each sentence\n",
    "    print(\"Sentence \", i+1)\n",
    "    print(\"Total words:\", all_words)\n",
    "    print(\"Filtered words:\", filtered_words)\n",
    "    print(\"Number of filtered words:\", len(filtered_words))\n",
    "    print(\"Stop words identified:\", stop_words_found)\n",
    "    print(\"Number of stop words identified:\", all_stop_words)\n",
    "    print()\n",
    "\n",
    "print(\"Total number of words:\", total_words)  # print the total sum of all words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Identify the stop words for each sentence\n",
    "    num_stop_words_per_sentence = []\n",
    "    stop_words_per_sentence = []\n",
    "    filtered_sentences = []\n",
    "    num_words_per_sentence = []\n",
    "    avg_stop_words_per_sentence = []\n",
    "    total_words = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence into words\n",
    "        words = word_tokenize(sentence)\n",
    "        num_words = len(words)\n",
    "        total_words += num_words\n",
    "        \n",
    "        # Identify the stop words in the sentence\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [w for w in words if not w.lower() in stop_words]\n",
    "        \n",
    "        # Add the number of stop words and filtered sentence to the output\n",
    "        num_stop_words = num_words - len(filtered_words)\n",
    "        num_stop_words_per_sentence.append(num_stop_words)\n",
    "        stop_words_per_sentence.append(filtered_words)\n",
    "        filtered_sentences.append(\" \".join(filtered_words))\n",
    "        num_words_per_sentence.append(num_words)\n",
    "        \n",
    "        # Calculate the average number of stop words per sentence\n",
    "        avg_stop_words_per_sentence.append(num_stop_words / num_words)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    num_stop_words = sum(num_stop_words_per_sentence)\n",
    "    num_sentences = len(sentences)\n",
    "    avg_stop_words_per_sentence_all = num_stop_words / num_sentences\n",
    "    max_stop_words_per_sentence = max(num_stop_words_per_sentence)\n",
    "    min_stop_words_per_sentence = min(num_stop_words_per_sentence)\n",
    "    avg_stop_words_per_word = num_stop_words / sum(num_words_per_sentence)\n",
    "    \n",
    "    # Calculate the average number of stop words per article\n",
    "    avg_stop_words_per_sentence_avg = sum(avg_stop_words_per_sentence) / len(avg_stop_words_per_sentence)\n",
    "    \n",
    "    # Return the output\n",
    "    return {\n",
    "        'num_stop_words': num_stop_words,\n",
    "        \"total_words\": total_words,\n",
    "        'avg_stop_words_per_sentence_all': avg_stop_words_per_sentence_all,\n",
    "        'max_stop_words_per_sentence': max_stop_words_per_sentence,\n",
    "        'min_stop_words_per_sentence': min_stop_words_per_sentence,\n",
    "        'avg_stop_words_per_word': avg_stop_words_per_word,\n",
    "        'avg_stop_words_per_sentence': avg_stop_words_per_sentence,\n",
    "        'avg_stop_words_per_sentence_avg': avg_stop_words_per_sentence_avg,\n",
    "        'filtered_sentences': filtered_sentences,\n",
    "        'stop_words_per_sentence': stop_words_per_sentence,\n",
    "        'num_words_per_sentence': num_words_per_sentence,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered sentences:\n",
      "Republican presidential candidate Nikki Haley stepped hallway speaking Conservative Political Action Conference Friday supporters asking selfies autographs — , others , less friendly greeting .\n",
      "Average number of stop words per sentence: 0.32\n",
      "\n",
      "“ love Trump , love Trump ! ” crowd around started chanting .\n",
      "Average number of stop words per sentence: 0.24\n",
      "\n",
      "Haley supporters shouted name back former U.N. ambassador escaped staff elevator .\n",
      "Average number of stop words per sentence: 0.37\n",
      "\n",
      "dust-up showed risks taking primary fight clearly become Trump ’ home turf .\n",
      "Average number of stop words per sentence: 0.38\n",
      "\n",
      "Though CPAC long seen big-tent forum conservative movement mandatory cattle call presidential hopefuls , annual conference increasingly grown stomping ground 45th president “ Make America Great ” wing GOP .\n",
      "Average number of stop words per sentence: 0.4\n",
      "\n",
      "Trump speak event Saturday .\n",
      "Average number of stop words per sentence: 0.38\n",
      "\n",
      "“ Remember , ’ CPAC , ’ TPAC , ” John Fredericks , pro-Trump talk radio host broadcasting sidelines , said interview Wednesday .\n",
      "Average number of stop words per sentence: 0.35\n",
      "\n",
      "said potential 2024 rivals opted skip conference rather risk getting booed losing straw poll .\n",
      "Average number of stop words per sentence: 0.29\n",
      "\n",
      "“ thing , ’ , ” said .\n",
      "Average number of stop words per sentence: 0.47\n",
      "\n",
      "“ Trump , CPAC. ” year ’ lineup heavy Trump family members acolytes — Lara Trump , Donald Trump Jr. , former White House strategist Stephen K. Bannon , losing 2022 Arizona gubernatorial candidate Kari Lake , Sens .\n",
      "Average number of stop words per sentence: 0.19\n",
      "\n",
      "J.D .\n",
      "Average number of stop words per sentence: 0.0\n",
      "\n",
      "Vance ( R-Ohio ) Ted Cruz ( R-Tex .\n",
      "Average number of stop words per sentence: 0.1\n",
      "\n",
      ") , Reps. Marjorie Taylor Greene ( R-Ga. ) , Lauren Boebert ( R-Colo. ) Matt Gaetz ( R-Fla. ) — near-total exclusion party ’ voices .\n",
      "Average number of stop words per sentence: 0.23\n",
      "\n",
      "Florida Gov .\n",
      "Average number of stop words per sentence: 0.0\n",
      "\n",
      "Ron DeSantis , polls show Trump ’ biggest competitor 2024 primary though yet announced whether running , opted spend week far away events promoting new book .\n",
      "Average number of stop words per sentence: 0.37\n",
      "\n",
      "Statistics on stop words:\n",
      "Total number of words: 366\n",
      "Number of stop words: 114\n",
      "Maximum number of stop words per sentence: 20\n",
      "Minimum number of stop words per sentence: 0\n",
      "Average number of stop words per article: 0.31\n"
     ]
    }
   ],
   "source": [
    "results = remove_stop_words(article)\n",
    "\n",
    "print(\"Filtered sentences:\")\n",
    "for sentence in results[\"filtered_sentences\"]:\n",
    "    print(sentence)\n",
    "    print(\"Average number of stop words per sentence:\", round(results[\"avg_stop_words_per_sentence\"][results[\"filtered_sentences\"].index(sentence)], 2))\n",
    "    print()\n",
    "\n",
    "print(\"Statistics on stop words:\")\n",
    "print(\"Total number of words:\", results[\"total_words\"])\n",
    "print(\"Number of stop words:\", results[\"num_stop_words\"])\n",
    "print(\"Maximum number of stop words per sentence:\", results[\"max_stop_words_per_sentence\"])\n",
    "print(\"Minimum number of stop words per sentence:\", results[\"min_stop_words_per_sentence\"])\n",
    "print(\"Average number of stop words per article:\", round(results[\"avg_stop_words_per_word\"], 2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming analysis is a natural language processing technique that involves reducing words to their base or root form. The goal of stemming is to normalize variations of words by mapping all forms of a word to a common base or root word, which can help improve text analysis and information retrieval.\n",
    "\n",
    "For example, the words \"jumping,\" \"jumps,\" and \"jumped\" can be stemmed to the root word \"jump.\" This allows us to treat all variations of the word \"jump\" as the same word, which can simplify text analysis and improve the accuracy of search results.\n",
    "\n",
    "There are several popular stemming algorithms, such as the Porter stemming algorithm and the Snowball stemming algorithm, that use various rules and heuristics to determine the base form of a word. Stemming is often used as a preprocessing step in natural language processing tasks such as text classification, sentiment analysis, and information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "republican presidenti candid nikki haley step into the hallway after speak at the conserv polit action confer on friday to support ask for selfi and autograph — and , from other , a less friendli greet . “ we love trump , we love trump ! ” a crowd around her start chant . some haley support shout her name back as the former u.n. ambassador escap with staff to an elev . the dust-up show the risk of take the primari fight to what ha clearli becom trump ’ s home turf . though cpac ha long been seen as a big-tent forum for the conserv movement and a mandatori cattl call for presidenti hope , the annual confer ha increasingli grown into a stomp ground for the 45th presid and hi “ make america great again ” wing of the gop . trump will speak at the event saturday . “ rememb , you ’ re not at cpac , you ’ re at tpac , ” john frederick , a pro-trump talk radio host broadcast from the sidelin here , said in an interview wednesday . he said potenti 2024 rival opt to skip the confer rather than risk get boo or lose the straw poll . “ we own thi thing , it ’ s our , ” he said . “ no trump , no cpac. ” thi year ’ s lineup wa heavi with trump famili member and acolyt — such as lara trump , donald trump jr. , former white hous strategist stephen k. bannon , lose 2022 arizona gubernatori candid kari lake , sen . j.d . vanc ( r-ohio ) and ted cruz ( r-tex . ) , and reps. marjori taylor green ( r-ga. ) , lauren boebert ( r-colo. ) and matt gaetz ( r-fla. ) — to the near-tot exclus of the parti ’ s other voic . florida gov . ron desanti , who poll show as trump ’ s biggest competitor for the 2024 primari though he ha not yet announc whether he is run , opt to spend the week far away at hi own event promot hi new book .\n"
     ]
    }
   ],
   "source": [
    "# Create a Porter stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = word_tokenize(article)\n",
    "\n",
    "# Perform stemming on each word using the Porter stemmer\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Combine the stemmed words back into a single string\n",
    "output_text = ' '.join(stemmed_words)\n",
    "\n",
    "# Write the output text to a new file\n",
    "# with open('output.txt', 'w') as f:\n",
    "#    f.write(output_text)\n",
    "\n",
    "print(output_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment and Subjecitivity Markup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using the Stanza Library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanza is an open-source natural language processing (NLP) library for Python that provides a range of tools and models for tasks such as tokenization, part-of-speech tagging, dependency parsing, named entity recognition, and sentiment analysis.\n",
    "\n",
    "Stanza is useful for a variety of NLP applications, including but not limited to:\n",
    "\n",
    "- Sentiment Analysis: Stanza provides pre-trained models for sentiment analysis that can be used to classify the sentiment of a given text as positive, negative or neutral.\n",
    "\n",
    "- Named Entity Recognition (NER): Stanza can be used to identify and extract named entities such as people, organizations, and locations from text.\n",
    "\n",
    "- Part-of-Speech Tagging (POS): Stanza can be used to identify the part of speech of each word in a sentence, such as noun, verb, adjective, etc. This information can be used in a variety of downstream NLP tasks.\n",
    "\n",
    "- Dependency Parsing: Stanza can be used to parse the grammatical structure of a sentence and identify the relationships between words. This can be useful for tasks such as information extraction and text summarization.\n",
    "\n",
    "- Text Classification: Stanza can be used to classify text into predefined categories, such as spam vs. non-spam, positive vs. negative, etc.\n",
    "\n",
    "- Machine Translation: Stanza can be used to translate text from one language to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 10:57:51 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66aa7f45e343470aba8aadf5c2c4b6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 10:57:52 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| sentiment | sstplus  |\n",
      "========================\n",
      "\n",
      "2023-04-20 10:57:52 INFO: Using device: cuda\n",
      "2023-04-20 10:57:52 INFO: Loading: tokenize\n",
      "2023-04-20 10:57:52 INFO: Loading: sentiment\n",
      "2023-04-20 10:57:53 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment', tokenize_no_ssplit=False, max_split_size_mb=128)\n",
    "\n",
    "# Define a function to analyze the sentiment of a given text\n",
    "def analyze_sentiment(sentences):    \n",
    "    # Process each sentence and extract sentiment\n",
    "    results = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        sentiment = doc.sentences[0].sentiment\n",
    "        score = sentiment.score if hasattr(sentiment, 'score') else 0.0\n",
    "        \n",
    "        # Map the sentiment score to a label\n",
    "        if score < 0.0:\n",
    "            label = 'negative'\n",
    "        elif score > 0.0:\n",
    "            label = 'positive'\n",
    "        else:\n",
    "            label = 'neutral'\n",
    "        \n",
    "        # Add the sentence sentiment to the results list\n",
    "        results.append((sentence, label, score))\n",
    "    \n",
    "    # Return the list of sentence sentiments\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Republican presidential candidate Nikki Haley stepped into the hallway after speaking at the Conservative Political Action Conference on Friday to supporters asking for selfies and autographs — and, from others, a less friendly greeting.\n",
      "Sentiment: neutral with a score of 0.0\n",
      "\n",
      "Sentence: “We love Trump, we love Trump!” a crowd around her started chanting.\n",
      "Sentiment: neutral with a score of 0.0\n",
      "\n",
      "Sentence: Some Haley supporters shouted her name back as the former U.N. ambassador escaped with staff to an elevator.\n",
      "Sentiment: neutral with a score of 0.0\n",
      "\n",
      "Sentence: The dust-up showed the risks of taking the primary fight to what has clearly become Trump’s home turf.\n",
      "Sentiment: neutral with a score of 0.0\n",
      "\n",
      "Sentence: Though CPAC has long been seen as a big-tent forum for the conservative movement and a mandatory cattle call for presidential hopefuls, the annual conference has increasingly grown into a stomping ground for the 45th president and his “Make America Great Again” wing of the GOP.\n",
      "Sentiment: neutral with a score of 0.0\n",
      "\n",
      "Sentence: Trump will speak at the event Saturday.\n",
      "Sentiment: neutral with a score of 0.0\n",
      "\n",
      "Sentence: “Remember, you’re not at CPAC, you’re at TPAC,” John Fredericks, a pro-Trump talk radio host broadcasting from the sidelines here, said in an interview Wednesday.\n",
      "Sentiment: neutral with a score of 0.0\n",
      "\n",
      "Sentence: He said potential 2024 rivals opted to skip the conference rather than risk getting booed or losing the straw poll.\n",
      "Sentiment: neutral with a score of 0.0\n",
      "\n",
      "Sentence: “We own this thing, it’s ours,” he said.\n",
      "Sentiment: neutral with a score of 0.0\n",
      "\n",
      "Sentence: “No Trump, no CPAC.”\n",
      "\n",
      "This year’s lineup was heavy with Trump family members and acolytes — such as Lara Trump, Donald Trump Jr., former White House strategist Stephen K. Bannon, losing 2022 Arizona gubernatorial candidate Kari Lake, Sens.\n",
      "Sentiment: neutral with a score of 0.0\n",
      "\n",
      "Sentence: J.D.\n",
      "Sentiment: neutral with a score of 0.0\n",
      "\n",
      "Sentence: Vance (R-Ohio) and Ted Cruz (R-Tex.\n",
      "Sentiment: neutral with a score of 0.0\n",
      "\n",
      "Sentence: ), and Reps. Marjorie Taylor Greene (R-Ga.), Lauren Boebert (R-Colo.) and Matt Gaetz (R-Fla.) — to the near-total exclusion of the party’s other voices.\n",
      "Sentiment: neutral with a score of 0.0\n",
      "\n",
      "Sentence: Florida Gov.\n",
      "Sentiment: neutral with a score of 0.0\n",
      "\n",
      "Sentence: Ron DeSantis, who polls show as Trump’s biggest competitor for the 2024 primary though he has not yet announced whether he is running, opted to spend the week far away at his own events promoting his new book.\n",
      "Sentiment: neutral with a score of 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = analyze_sentiment(sentences)\n",
    "for sentence, label, score in results:\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Sentiment: {label} with a score of {score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_scores(text, nlp):\n",
    "    doc = nlp(text)\n",
    "    sentiment_scores = []\n",
    "    for sentence in doc.sentences:\n",
    "        sentiment_scores.append(sentence.sentiment)\n",
    "    if len(sentiment_scores) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return {\n",
    "            'average': sum(sentiment_scores) / len(sentiment_scores),\n",
    "            'maximum': max(sentiment_scores),\n",
    "            'sd': statistics.stdev(sentiment_scores),\n",
    "            'minimum': min(sentiment_scores)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average': 0.6666666666666666, 'maximum': 1, 'sd': 0.49236596391733095, 'minimum': 0}\n"
     ]
    }
   ],
   "source": [
    "sentiment_scores = get_sentiment_scores(article, nlp)\n",
    "print(sentiment_scores)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Vader library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically designed to handle sentiment analysis for social media text. It is part of the Natural Language Toolkit (nltk) library in Python.\n",
    "\n",
    "The VADER library uses a combination of sentiment lexicon (a list of words and their valence scores) and rule-based approach to analyze the sentiment of a piece of text. Unlike traditional sentiment analysis tools that use machine learning techniques, VADER doesn't require any training data to analyze sentiment. Instead, it uses a set of pre-defined rules and patterns to determine the sentiment of a piece of text.\n",
    "\n",
    "VADER is particularly useful for analyzing the sentiment of short and informal texts, such as tweets, online reviews, and chat messages. It takes into account both the polarity and intensity of the sentiment in the text, which makes it more accurate than traditional sentiment analysis tools in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.052, 'neu': 0.852, 'pos': 0.096, 'compound': 0.9425}\n"
     ]
    }
   ],
   "source": [
    "# initialize the sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# analyze the sentiment of the text\n",
    "scores = analyzer.polarity_scores(article)\n",
    "\n",
    "# print the sentiment scores\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vader_stats(text):\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores_list = []\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        scores = analyzer.polarity_scores(sentence)\n",
    "        score_list = [scores['neg'], scores['neu'], scores['pos']]\n",
    "        scores_list.append(score_list)\n",
    "\n",
    "    if not scores_list:\n",
    "        return None\n",
    "\n",
    "    scores_array = np.array(scores_list)\n",
    "    avg_scores = np.mean(scores_array, axis=0)\n",
    "    max_scores = np.max(scores_array, axis=0)\n",
    "    min_scores = np.min(scores_array, axis=0)\n",
    "    std_scores = np.std(scores_array, axis=0)\n",
    "\n",
    "    return avg_scores, max_scores, min_scores, std_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores: [0.04193333 0.88106667 0.07706667]\n",
      "Maximum scores: [0.21  1.    0.491]\n",
      "Minimum scores: [0.    0.509 0.   ]\n",
      "Standard deviation scores: [0.07460783 0.13911073 0.12995767]\n"
     ]
    }
   ],
   "source": [
    "avg_scores, max_scores, min_scores, std_scores = get_vader_stats(article)\n",
    "\n",
    "print(\"Average scores:\", avg_scores)\n",
    "print(\"Maximum scores:\", max_scores)\n",
    "print(\"Minimum scores:\", min_scores)\n",
    "print(\"Standard deviation scores:\", std_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjectivity analysis using MPQA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores we calculate in this program are subjectivity scores, which indicate the degree to which a word expresses a subjective or emotional meaning, as opposed to an objective or factual meaning.\n",
    "\n",
    "Specifically, we are using the MPQA (Multi-Perspective Question Answering) subjectivity lexicon, which assigns each word in the lexicon a polarity score of either positive, negative, or neutral (i.e., a score of 1, -1, or 0, respectively) based on the word's emotional connotation.\n",
    "\n",
    "In the program, we first load the MPQA lexicon and preprocess it so that we can easily look up the polarity score of each word in the lexicon. Then, we define a function subjectivity_analysis that takes a file path as input, reads in the text data from the file, and calculates the subjectivity score for each word in the text by looking up the polarity score of the word in the MPQA lexicon.\n",
    "\n",
    "The average subjectivity score that we calculate is simply the average of all the subjectivity scores of the words in the text. The maximum and minimum subjectivity scores represent the most and least subjective words in the text, respectively. Finally, the standard deviation of the subjectivity scores measures how much the subjectivity scores vary from the average subjectivity score, and thus gives us an indication of the overall degree of subjectivity in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MPQA lexicon\n",
    "lexicon = pd.read_csv(\"subjclueslen1-HLTEMNLP05.tff\", sep=\" \", header=None, \n",
    "                      names=[\"type\", \"len\", \"word\", \"pos\", \"stemmed\", \"polarity\", \"strength\"])\n",
    "\n",
    "lexicon[\"type\"] = lexicon[\"type\"].str[5:]\n",
    "lexicon[\"word\"] = lexicon[\"word\"].str[len(\"word1=\"):]\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].str[len(\"priorpolarity=\"):]\n",
    "cols_to_remove = [\"len\", \"pos\", \"stemmed\", \"strength\"]\n",
    "lexicon = lexicon.drop(columns=cols_to_remove)\n",
    "lexicon[\"type\"] = lexicon[\"type\"].replace(\"weaksubj\", 1)\n",
    "lexicon[\"type\"] = lexicon[\"type\"].replace(\"strongsubj\", 2)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"negative\", -1)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"positive\", 1)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"both\", 0)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"neutral\", 0)\n",
    "\n",
    "# Define a function to perform subjectivity analysis on a text file\n",
    "def subjectivity_analysis(file_path):\n",
    "    # Load the text file\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Perform subjectivity analysis\n",
    "    scores = []\n",
    "    for word in text.split():\n",
    "        word = word.strip().lower()\n",
    "        if word in lexicon.word.tolist():\n",
    "            polarity = lexicon[lexicon.word == word].polarity.values[0]\n",
    "            scores.append(polarity)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_score = np.mean(scores)\n",
    "    max_score = np.max(scores)\n",
    "    min_score = np.min(scores)\n",
    "    sd_score = np.std(scores)\n",
    "    \n",
    "    return avg_score, max_score, min_score, sd_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>word</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>abandoned</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>abandon</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>abase</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>abasement</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8217</th>\n",
       "      <td>2</td>\n",
       "      <td>zealot</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8218</th>\n",
       "      <td>2</td>\n",
       "      <td>zealous</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8219</th>\n",
       "      <td>2</td>\n",
       "      <td>zealously</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8220</th>\n",
       "      <td>2</td>\n",
       "      <td>zenith</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8221</th>\n",
       "      <td>2</td>\n",
       "      <td>zest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8222 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type         word polarity\n",
       "0        1    abandoned       -1\n",
       "1        1  abandonment       -1\n",
       "2        1      abandon       -1\n",
       "3        2        abase       -1\n",
       "4        2    abasement       -1\n",
       "...    ...          ...      ...\n",
       "8217     2       zealot       -1\n",
       "8218     2      zealous       -1\n",
       "8219     2    zealously       -1\n",
       "8220     2       zenith        1\n",
       "8221     2         zest        1\n",
       "\n",
       "[8222 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average subjectivity score: 0.18518518518518517\n",
      "Maximum subjectivity score: 1\n",
      "Minimum subjectivity score: -1\n",
      "Standard deviation of subjectivity scores: 0.8181748901620192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = \"/home/pierluigi/Documents/echo_chambers_intership/newsArticle.txt\"\n",
    "avg_score, max_score, min_score, sd_score = subjectivity_analysis(file_path)\n",
    "\n",
    "print(\"Average subjectivity score:\", avg_score)\n",
    "print(\"Maximum subjectivity score:\", max_score)\n",
    "print(\"Minimum subjectivity score:\", min_score)\n",
    "print(\"Standard deviation of subjectivity scores:\", sd_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis using SentiWordNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis using SentiWordNet involves using a lexical resource called SentiWordNet to perform sentiment analysis on text data. SentiWordNet is a publicly available lexical resource that assigns a sentiment score to each synset (set of synonyms) in WordNet, a large English lexical database.\n",
    "\n",
    "To perform sentiment analysis using SentiWordNet, the text data is first preprocessed to remove any noise and convert it into a format that can be analyzed. Then, each word in the text data is assigned a synset based on its meaning. The sentiment score of each synset is then retrieved from SentiWordNet, and a sentiment score for the entire text is calculated by aggregating the scores of all the synsets in the text.\n",
    "\n",
    "The sentiment score can be used to determine the overall sentiment of the text, such as whether it is positive, negative, or neutral. This can be useful for a wide range of applications, including social media monitoring, market research, and customer feedback analysis. However, it is important to note that SentiWordNet is based on WordNet, which is a English-centric lexical database, and may not be suitable for sentiment analysis of other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiwordnet_vector(file_path):\n",
    "  with open(file_path, 'r') as f:\n",
    "    text = f.read()\n",
    "  text = text.lower()\n",
    "  tokens = word_tokenize(text)\n",
    "  tokens = [token for token in tokens if token.isalnum()]\n",
    "  tokens = [token for token in tokens if not token in nltk.corpus.stopwords.words('english')]\n",
    "  sentiwordnet_scores = []\n",
    "  for token in tokens:\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    synsets = swn.senti_synsets(token)\n",
    "    for synset in synsets:\n",
    "      pos_score += synset.pos_score()\n",
    "      neg_score += synset.neg_score()\n",
    "    if pos_score > neg_score:\n",
    "      sentiment_score = 1\n",
    "    elif neg_score > pos_score:\n",
    "      sentiment_score = -1\n",
    "    else:\n",
    "      sentiment_score = 0\n",
    "    sentiwordnet_scores.append(sentiment_score)\n",
    "  assert(len(sentiwordnet_scores) == len(tokens))\n",
    "  return {'tokens': tokens, 'sentiwordnet_scores': sentiwordnet_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Token: republican, Sentiment Score: 0\n",
      "2. Token: presidential, Sentiment Score: -1\n",
      "3. Token: candidate, Sentiment Score: 0\n",
      "4. Token: nikki, Sentiment Score: 0\n",
      "5. Token: haley, Sentiment Score: 0\n",
      "6. Token: stepped, Sentiment Score: -1\n",
      "7. Token: hallway, Sentiment Score: 0\n",
      "8. Token: speaking, Sentiment Score: 1\n",
      "9. Token: conservative, Sentiment Score: -1\n",
      "10. Token: political, Sentiment Score: 0\n",
      "11. Token: action, Sentiment Score: 1\n",
      "12. Token: conference, Sentiment Score: 0\n",
      "13. Token: friday, Sentiment Score: 0\n",
      "14. Token: supporters, Sentiment Score: 0\n",
      "15. Token: asking, Sentiment Score: 1\n",
      "16. Token: selfies, Sentiment Score: 0\n",
      "17. Token: autographs, Sentiment Score: 0\n",
      "18. Token: others, Sentiment Score: 0\n",
      "19. Token: less, Sentiment Score: -1\n",
      "20. Token: friendly, Sentiment Score: 1\n",
      "21. Token: greeting, Sentiment Score: 1\n",
      "22. Token: love, Sentiment Score: 1\n",
      "23. Token: trump, Sentiment Score: 1\n",
      "24. Token: love, Sentiment Score: 1\n",
      "25. Token: trump, Sentiment Score: 1\n",
      "26. Token: crowd, Sentiment Score: 0\n",
      "27. Token: around, Sentiment Score: 1\n",
      "28. Token: started, Sentiment Score: -1\n",
      "29. Token: chanting, Sentiment Score: 0\n",
      "30. Token: haley, Sentiment Score: 0\n",
      "31. Token: supporters, Sentiment Score: 0\n",
      "32. Token: shouted, Sentiment Score: -1\n",
      "33. Token: name, Sentiment Score: 1\n",
      "34. Token: back, Sentiment Score: 1\n",
      "35. Token: former, Sentiment Score: -1\n",
      "36. Token: ambassador, Sentiment Score: 1\n",
      "37. Token: escaped, Sentiment Score: -1\n",
      "38. Token: staff, Sentiment Score: 0\n",
      "39. Token: elevator, Sentiment Score: 0\n",
      "40. Token: showed, Sentiment Score: 1\n",
      "41. Token: risks, Sentiment Score: -1\n",
      "42. Token: taking, Sentiment Score: 1\n",
      "43. Token: primary, Sentiment Score: 1\n",
      "44. Token: fight, Sentiment Score: -1\n",
      "45. Token: clearly, Sentiment Score: 1\n",
      "46. Token: become, Sentiment Score: 1\n",
      "47. Token: trump, Sentiment Score: 1\n",
      "48. Token: home, Sentiment Score: 1\n",
      "49. Token: turf, Sentiment Score: 0\n",
      "50. Token: though, Sentiment Score: 0\n",
      "51. Token: cpac, Sentiment Score: 0\n",
      "52. Token: long, Sentiment Score: -1\n",
      "53. Token: seen, Sentiment Score: 1\n",
      "54. Token: forum, Sentiment Score: 0\n",
      "55. Token: conservative, Sentiment Score: -1\n",
      "56. Token: movement, Sentiment Score: -1\n",
      "57. Token: mandatory, Sentiment Score: 1\n",
      "58. Token: cattle, Sentiment Score: 0\n",
      "59. Token: call, Sentiment Score: 1\n",
      "60. Token: presidential, Sentiment Score: -1\n",
      "61. Token: hopefuls, Sentiment Score: 1\n",
      "62. Token: annual, Sentiment Score: 0\n",
      "63. Token: conference, Sentiment Score: 0\n",
      "64. Token: increasingly, Sentiment Score: 0\n",
      "65. Token: grown, Sentiment Score: 0\n",
      "66. Token: stomping, Sentiment Score: 0\n",
      "67. Token: ground, Sentiment Score: 0\n",
      "68. Token: 45th, Sentiment Score: 0\n",
      "69. Token: president, Sentiment Score: 0\n",
      "70. Token: make, Sentiment Score: 1\n",
      "71. Token: america, Sentiment Score: 0\n",
      "72. Token: great, Sentiment Score: 1\n",
      "73. Token: wing, Sentiment Score: 0\n",
      "74. Token: gop, Sentiment Score: 0\n",
      "75. Token: trump, Sentiment Score: 1\n",
      "76. Token: speak, Sentiment Score: 0\n",
      "77. Token: event, Sentiment Score: 0\n",
      "78. Token: saturday, Sentiment Score: 0\n",
      "79. Token: remember, Sentiment Score: 1\n",
      "80. Token: cpac, Sentiment Score: 0\n",
      "81. Token: tpac, Sentiment Score: 0\n",
      "82. Token: john, Sentiment Score: 0\n",
      "83. Token: fredericks, Sentiment Score: 0\n",
      "84. Token: talk, Sentiment Score: -1\n",
      "85. Token: radio, Sentiment Score: 0\n",
      "86. Token: host, Sentiment Score: -1\n",
      "87. Token: broadcasting, Sentiment Score: 0\n",
      "88. Token: sidelines, Sentiment Score: 0\n",
      "89. Token: said, Sentiment Score: 1\n",
      "90. Token: interview, Sentiment Score: 1\n",
      "91. Token: wednesday, Sentiment Score: 0\n",
      "92. Token: said, Sentiment Score: 1\n",
      "93. Token: potential, Sentiment Score: -1\n",
      "94. Token: 2024, Sentiment Score: 0\n",
      "95. Token: rivals, Sentiment Score: 1\n",
      "96. Token: opted, Sentiment Score: 0\n",
      "97. Token: skip, Sentiment Score: -1\n",
      "98. Token: conference, Sentiment Score: 0\n",
      "99. Token: rather, Sentiment Score: -1\n",
      "100. Token: risk, Sentiment Score: -1\n",
      "101. Token: getting, Sentiment Score: 1\n",
      "102. Token: booed, Sentiment Score: 0\n",
      "103. Token: losing, Sentiment Score: -1\n",
      "104. Token: straw, Sentiment Score: 0\n",
      "105. Token: poll, Sentiment Score: -1\n",
      "106. Token: thing, Sentiment Score: -1\n",
      "107. Token: said, Sentiment Score: 1\n",
      "108. Token: trump, Sentiment Score: 1\n",
      "109. Token: year, Sentiment Score: 0\n",
      "110. Token: lineup, Sentiment Score: 0\n",
      "111. Token: heavy, Sentiment Score: -1\n",
      "112. Token: trump, Sentiment Score: 1\n",
      "113. Token: family, Sentiment Score: 0\n",
      "114. Token: members, Sentiment Score: 0\n",
      "115. Token: acolytes, Sentiment Score: -1\n",
      "116. Token: lara, Sentiment Score: 0\n",
      "117. Token: trump, Sentiment Score: 1\n",
      "118. Token: donald, Sentiment Score: 0\n",
      "119. Token: trump, Sentiment Score: 1\n",
      "120. Token: former, Sentiment Score: -1\n",
      "121. Token: white, Sentiment Score: 1\n",
      "122. Token: house, Sentiment Score: 0\n",
      "123. Token: strategist, Sentiment Score: 0\n",
      "124. Token: stephen, Sentiment Score: 0\n",
      "125. Token: bannon, Sentiment Score: 0\n",
      "126. Token: losing, Sentiment Score: -1\n",
      "127. Token: 2022, Sentiment Score: 0\n",
      "128. Token: arizona, Sentiment Score: 0\n",
      "129. Token: gubernatorial, Sentiment Score: 0\n",
      "130. Token: candidate, Sentiment Score: 0\n",
      "131. Token: kari, Sentiment Score: 0\n",
      "132. Token: lake, Sentiment Score: 0\n",
      "133. Token: sens, Sentiment Score: 0\n",
      "134. Token: vance, Sentiment Score: 0\n",
      "135. Token: ted, Sentiment Score: -1\n",
      "136. Token: cruz, Sentiment Score: 0\n",
      "137. Token: marjorie, Sentiment Score: 0\n",
      "138. Token: taylor, Sentiment Score: 1\n",
      "139. Token: greene, Sentiment Score: 0\n",
      "140. Token: lauren, Sentiment Score: 0\n",
      "141. Token: boebert, Sentiment Score: 0\n",
      "142. Token: matt, Sentiment Score: -1\n",
      "143. Token: gaetz, Sentiment Score: 0\n",
      "144. Token: exclusion, Sentiment Score: 1\n",
      "145. Token: party, Sentiment Score: 0\n",
      "146. Token: voices, Sentiment Score: 1\n",
      "147. Token: florida, Sentiment Score: 0\n",
      "148. Token: gov, Sentiment Score: 0\n",
      "149. Token: ron, Sentiment Score: 0\n",
      "150. Token: desantis, Sentiment Score: 0\n",
      "151. Token: polls, Sentiment Score: -1\n",
      "152. Token: show, Sentiment Score: 1\n",
      "153. Token: trump, Sentiment Score: 1\n",
      "154. Token: biggest, Sentiment Score: 1\n",
      "155. Token: competitor, Sentiment Score: 0\n",
      "156. Token: 2024, Sentiment Score: 0\n",
      "157. Token: primary, Sentiment Score: 1\n",
      "158. Token: though, Sentiment Score: 0\n",
      "159. Token: yet, Sentiment Score: -1\n",
      "160. Token: announced, Sentiment Score: 0\n",
      "161. Token: whether, Sentiment Score: 0\n",
      "162. Token: running, Sentiment Score: -1\n",
      "163. Token: opted, Sentiment Score: 0\n",
      "164. Token: spend, Sentiment Score: 1\n",
      "165. Token: week, Sentiment Score: 0\n",
      "166. Token: far, Sentiment Score: 1\n",
      "167. Token: away, Sentiment Score: -1\n",
      "168. Token: events, Sentiment Score: 0\n",
      "169. Token: promoting, Sentiment Score: 1\n",
      "170. Token: new, Sentiment Score: 1\n",
      "171. Token: book, Sentiment Score: 1\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/pierluigi/Documents/echo_chambers_intership/newsArticle.txt'\n",
    "result = get_sentiwordnet_vector(file_path)\n",
    "for i, token in enumerate(result['tokens']):\n",
    "  print(f\"{i+1}. Token: {token}, Sentiment Score: {result['sentiwordnet_scores'][i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
