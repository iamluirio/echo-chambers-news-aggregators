{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6702d59f14649f189b54c7ad8695f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 12:15:28 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-04-29 12:15:30 INFO: File exists: /home/pierluigi/stanza_resources/en/default.zip\n",
      "2023-04-29 12:15:35 INFO: Finished downloading models and saved to /home/pierluigi/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import stanza\n",
    "stanza.download('en')  # Download the English model\n",
    "\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import newspaper\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 12:15:36 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ccd273106604b15ab9b4e2cbfa9f5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 12:15:36 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| sentiment | sstplus  |\n",
      "========================\n",
      "\n",
      "2023-04-29 12:15:36 INFO: Using device: cuda\n",
      "2023-04-29 12:15:36 INFO: Loading: tokenize\n",
      "2023-04-29 12:15:39 INFO: Loading: sentiment\n",
      "2023-04-29 12:15:40 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment', tokenize_no_ssplit=False, max_split_size_mb=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MPQA lexicon\n",
    "lexicon = pd.read_csv(\"/home/pierluigi/Documents/echo_chambers_intership/Code analysis/NLP/Single modules/subjclueslen1-HLTEMNLP05.tff\", sep=\" \", header=None, \n",
    "                      names=[\"type\", \"len\", \"word\", \"pos\", \"stemmed\", \"polarity\", \"strength\"])\n",
    "\n",
    "lexicon[\"type\"] = lexicon[\"type\"].str[5:]\n",
    "lexicon[\"word\"] = lexicon[\"word\"].str[len(\"word1=\"):]\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].str[len(\"priorpolarity=\"):]\n",
    "cols_to_remove = [\"len\", \"pos\", \"stemmed\", \"strength\"]\n",
    "lexicon = lexicon.drop(columns=cols_to_remove)\n",
    "lexicon[\"type\"] = lexicon[\"type\"].replace(\"weaksubj\", 1)\n",
    "lexicon[\"type\"] = lexicon[\"type\"].replace(\"strongsubj\", 2)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"negative\", -1)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"positive\", 1)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"both\", 0)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"neutral\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_article(url):\n",
    "    # Create a newspaper Article object\n",
    "    article = newspaper.Article(url)\n",
    "\n",
    "    # Download and parse the article\n",
    "    article.download()\n",
    "    article.parse()\n",
    "\n",
    "    # Extract the title, subtitle, description, and main text\n",
    "    title = article.title.strip()\n",
    "    subtitle = article.meta_data.get(\"description\", \"\").strip()\n",
    "    description = article.meta_description.strip()\n",
    "    text = article.text.strip()\n",
    "\n",
    "    # Set the subtitle to the description if it is empty\n",
    "    if not subtitle:\n",
    "        subtitle = description.strip()\n",
    "\n",
    "    # Concatenate the extracted strings\n",
    "    article_text = f\"{title}\\n\\n{subtitle}\\n\\n{text}\"\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(article_text)\n",
    "    \n",
    "    # Identify the stop words for each sentence\n",
    "    num_stop_words_per_sentence = []\n",
    "    stop_words_per_sentence = []\n",
    "    filtered_sentences = []\n",
    "    num_words_per_sentence = []\n",
    "    avg_stop_words_per_sentence = []\n",
    "    total_words = 0\n",
    "\n",
    "    # Create a Porter stemmer object\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_sentences = []\n",
    "\n",
    "    # Process the text with the pipeline and extract the sentiment for each sentence\n",
    "    doc = nlp(text)\n",
    "    sentiment_scores = []\n",
    "\n",
    "    # initialize the Vader sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    v_scores_list = []\n",
    "\n",
    "    # MPQA analysis\n",
    "    mpqa_scores = []\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Tokenize the sentence into words\n",
    "        words = word_tokenize(sentence)\n",
    "        all_words = len(words)\n",
    "        total_words += all_words\n",
    "        \n",
    "        # Identify the stop words in the sentence\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words_found = [word for word in words if word.lower() in stop_words]\n",
    "        all_stop_words = len(stop_words_found)\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        \n",
    "        # Add the number of stop words and filtered sentence to the output\n",
    "        num_stop_words = all_words - len(filtered_words)\n",
    "        num_stop_words_per_sentence.append(num_stop_words)\n",
    "        stop_words_per_sentence.append(stop_words_found)\n",
    "        filtered_sentences.append(\" \".join(filtered_words))\n",
    "        num_words_per_sentence.append(all_words)\n",
    "        \n",
    "        # Calculate the average number of stop words per sentence\n",
    "        avg_stop_words_per_sentence.append(num_stop_words / all_words)\n",
    "\n",
    "        # Perform stemming on each word using the Porter stemmer\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "        # Combine the stemmed words back into a single string\n",
    "        stemmed_sentence = ' '.join(stemmed_words)\n",
    "        stemmed_sentences.append(stemmed_sentence)\n",
    "        output_text = '\\n'.join(stemmed_sentences)\n",
    "\n",
    "        v_scores = analyzer.polarity_scores(sentence)\n",
    "        v_score_list = [v_scores['neg'], v_scores['neu'], v_scores['pos']]\n",
    "        v_scores_list.append(v_score_list)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        sentiment_scores.append(sentence.sentiment)\n",
    "\n",
    "    for word in article_text.split():\n",
    "        word = word.strip().lower()\n",
    "        if word in lexicon.word.tolist():\n",
    "            polarity = lexicon[lexicon.word == word].polarity.values[0]\n",
    "            mpqa_scores.append(polarity)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    num_stop_words = sum(num_stop_words_per_sentence)\n",
    "    num_sentences = len(sentences)\n",
    "    avg_stop_words_per_sentence_all = num_stop_words / num_sentences\n",
    "    max_stop_words_per_sentence = max(num_stop_words_per_sentence)\n",
    "    min_stop_words_per_sentence = min(num_stop_words_per_sentence)\n",
    "    avg_stop_words_per_word = num_stop_words / total_words\n",
    "    \n",
    "    # Calculate the average number of stop words per article\n",
    "    avg_stop_words_per_sentence_avg = sum(avg_stop_words_per_sentence) / len(avg_stop_words_per_sentence)\n",
    "\n",
    "    v_scores_array = np.array(v_scores_list)\n",
    "    v_avg_scores = np.mean(v_scores_array, axis=0)\n",
    "    v_max_scores = np.max(v_scores_array, axis=0)\n",
    "    v_min_scores = np.min(v_scores_array, axis=0)\n",
    "    v_std_scores = np.std(v_scores_array, axis=0)\n",
    "\n",
    "    mpqa_avg_score = np.mean(mpqa_scores)\n",
    "    mpqa_max_score = np.max(mpqa_scores)\n",
    "    mpqa_min_score = np.min(mpqa_scores)\n",
    "    mpqa_sd_score = np.std(mpqa_scores)\n",
    "\n",
    "    sentiwordnet_text = article_text.lower()\n",
    "    tokens = word_tokenize(sentiwordnet_text)\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    tokens = [token for token in tokens if not token in nltk.corpus.stopwords.words('english')]\n",
    "    sentiwordnet_scores = []\n",
    "\n",
    "    for token in tokens:\n",
    "        pos_score = 0\n",
    "        neg_score = 0\n",
    "        synsets = swn.senti_synsets(token)\n",
    "        for synset in synsets:\n",
    "            pos_score += synset.pos_score()\n",
    "            neg_score += synset.neg_score()\n",
    "        if pos_score > neg_score:\n",
    "            sentiment_score = 1\n",
    "        elif neg_score > pos_score:\n",
    "            sentiment_score = -1\n",
    "        else:\n",
    "            sentiment_score = 0\n",
    "        sentiwordnet_scores.append(sentiment_score)\n",
    "    assert(len(sentiwordnet_scores) == len(tokens))\n",
    "\n",
    "\n",
    "    # Return the output\n",
    "    return {\n",
    "        'title': title,\n",
    "        'num_stop_words': num_stop_words,\n",
    "        'total_words': total_words,\n",
    "        'stop_words_found': stop_words_found,\n",
    "        'all_stop_words': all_stop_words,\n",
    "        'avg_stop_words_per_sentence_all': avg_stop_words_per_sentence_all,\n",
    "        'max_stop_words_per_sentence': max_stop_words_per_sentence,\n",
    "        'min_stop_words_per_sentence': min_stop_words_per_sentence,\n",
    "        'avg_stop_words_per_word': avg_stop_words_per_word,\n",
    "        'avg_stop_words_per_sentence': avg_stop_words_per_sentence,\n",
    "        'avg_stop_words_per_sentence_avg': avg_stop_words_per_sentence_avg,\n",
    "        'filtered_sentences': filtered_sentences,\n",
    "        'stop_words_per_sentence': stop_words_per_sentence,\n",
    "        'num_words_per_sentence': num_words_per_sentence,\n",
    "        'num_stop_words_per_sentence': num_stop_words_per_sentence,\n",
    "        'output_text': output_text,\n",
    "        'sentiment_scores': sentiment_scores,\n",
    "        'v_avg_scores': v_avg_scores,\n",
    "        'v_max_scores': v_max_scores,\n",
    "        'v_min_scores': v_min_scores,\n",
    "        'v_std_scores': v_std_scores,\n",
    "        'mpqa_avg_score': mpqa_avg_score,\n",
    "        'mpqa_max_score': mpqa_max_score,\n",
    "        'mpqa_min_score': mpqa_min_score,\n",
    "        'mpqa_sd_score': mpqa_sd_score,\n",
    "        'tokens': tokens,\n",
    "        'sentiwordnet_scores': sentiwordnet_scores\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_articles(urls, directory):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Create a single file to store the preprocessed articles\n",
    "    file_path = f'{directory}/preprocessed_articles.txt'\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        for i, url in enumerate(urls):\n",
    "            results = preprocess_article(url)\n",
    "            # Write preprocessed article results to the file\n",
    "            f.write(f\"Article {i}:\\n\")\n",
    "            f.write(f\"Title: {results['title']}\\n\\n\")\n",
    "\n",
    "            # Save the general statistics on stop words to the file\n",
    "            f.write(f\"Total number of words: {results['total_words']}\\n\")\n",
    "            f.write(f\"Total number of stop words: {results['num_stop_words']}\\n\")\n",
    "            f.write(f\"Maximum number of stop words per sentence: {results['max_stop_words_per_sentence']}\\n\")\n",
    "            f.write(f\"Minimum number of stop words per sentence: {results['min_stop_words_per_sentence']}\\n\")\n",
    "            f.write(f\"Average number of stop words per article: {round(results['avg_stop_words_per_sentence_avg'], 2)}\\n\\n\")\n",
    "            \n",
    "            # Sentiment score\n",
    "            f.write(f\"Sentiment score:\\n\")\n",
    "            f.write(f\"Average of sentiment score for all sentences: {sum(results['sentiment_scores']) / len(results['sentiment_scores'])}\\n\")\n",
    "            f.write(f\"Maximum sentiment score: {max(results['sentiment_scores'])}\\n\")\n",
    "            f.write(f\"Minimum sentiment score: {min(results['sentiment_scores'])}\\n\")\n",
    "            f.write(f\"Standard deviation: {statistics.stdev(results['sentiment_scores'])}\\n\\n\")\n",
    "\n",
    "            f.write(f\"Vader average scores: {results['v_avg_scores']}\\n\")\n",
    "            f.write(f\"Vader maximum scores: {results['v_max_scores']}\\n\")\n",
    "            f.write(f\"Vader minimum scores: {results['v_min_scores']}\\n\")\n",
    "            f.write(f\"Vader standard deviation scores: {results['v_std_scores']}\\n\\n\")\n",
    "\n",
    "            f.write(f\"MPQA average scores: {results['mpqa_avg_score']}\\n\")\n",
    "            f.write(f\"MPQA maximum scores: {results['mpqa_max_score']}\\n\")\n",
    "            f.write(f\"MPQA minimum scores: {results['mpqa_min_score']}\\n\")\n",
    "            f.write(f\"MPQA standard deviation scores: {results['mpqa_sd_score']}\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 594.00 MiB (GPU 0; 1.95 GiB total capacity; 613.72 MiB already allocated; 472.38 MiB free; 924.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m urls \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mhttps://www.foxnews.com/politics/republicans-respond-after-irs-whistleblower-says-hunter-biden-investigation-being-mishandled\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://news.yahoo.com/alabama-education-director-ousted-over-234450832.html\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://news.yahoo.com/samantha-cameron-remind-david-steer-050000235.html\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m preprocess_articles(urls, directory\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpreprocessed articles\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mpreprocess_articles\u001b[0;34m(urls, directory)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_path, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m     \u001b[39mfor\u001b[39;00m i, url \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(urls):\n\u001b[0;32m---> 10\u001b[0m         results \u001b[39m=\u001b[39m preprocess_article(url)\n\u001b[1;32m     11\u001b[0m         \u001b[39m# Write preprocessed article results to the file\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         f\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mArticle \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m, in \u001b[0;36mpreprocess_article\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     34\u001b[0m stemmed_sentences \u001b[39m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m \u001b[39m# Process the text with the pipeline and extract the sentiment for each sentence\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[1;32m     38\u001b[0m sentiment_scores \u001b[39m=\u001b[39m []\n\u001b[1;32m     40\u001b[0m \u001b[39m# initialize the Vader sentiment analyzer\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stanza/pipeline/core.py:464\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, doc, processors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess(doc, processors)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stanza/pipeline/core.py:415\u001b[0m, in \u001b[0;36mPipeline.process\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors\u001b[39m.\u001b[39mget(processor_name):\n\u001b[1;32m    414\u001b[0m         process \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name]\u001b[39m.\u001b[39mbulk_process \u001b[39mif\u001b[39;00m bulk \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name]\u001b[39m.\u001b[39mprocess\n\u001b[0;32m--> 415\u001b[0m         doc \u001b[39m=\u001b[39m process(doc)\n\u001b[1;32m    416\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stanza/pipeline/sentiment_processor.py:63\u001b[0m, in \u001b[0;36mSentimentProcessor.process\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, document):\n\u001b[1;32m     62\u001b[0m     sentences \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mextract_sentences(document)\n\u001b[0;32m---> 63\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49mlabel_sentences(sentences, batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_size)\n\u001b[1;32m     64\u001b[0m     \u001b[39m# TODO: allow a classifier processor for any attribute, not just sentiment\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     document\u001b[39m.\u001b[39mset(SENTIMENT, labels, to_sentence\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stanza/models/classifiers/base_classifier.py:53\u001b[0m, in \u001b[0;36mBaseClassifier.label_sentences\u001b[0;34m(self, sentences, batch_size)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m interval[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m interval[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[39m# this can happen for empty text\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(sentences[interval[\u001b[39m0\u001b[39;49m]:interval[\u001b[39m1\u001b[39;49m]])\n\u001b[1;32m     54\u001b[0m predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(output, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m labels\u001b[39m.\u001b[39mextend(predicted\u001b[39m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stanza/models/classifiers/cnn_classifier.py:440\u001b[0m, in \u001b[0;36mCNNClassifier.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    437\u001b[0m input_vectors \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(all_inputs, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mbilstm:\n\u001b[0;32m--> 440\u001b[0m     input_vectors, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbilstm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(input_vectors))\n\u001b[1;32m    442\u001b[0m \u001b[39m# reshape to fit the input tensors\u001b[39;00m\n\u001b[1;32m    443\u001b[0m x \u001b[39m=\u001b[39m input_vectors\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 594.00 MiB (GPU 0; 1.95 GiB total capacity; 613.72 MiB already allocated; 472.38 MiB free; 924.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "urls = ['https://www.foxnews.com/politics/republicans-respond-after-irs-whistleblower-says-hunter-biden-investigation-being-mishandled',\n",
    "        'https://news.yahoo.com/alabama-education-director-ousted-over-234450832.html',\n",
    "        'https://news.yahoo.com/samantha-cameron-remind-david-steer-050000235.html']\n",
    "\n",
    "preprocess_articles(urls, directory='preprocessed articles')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
