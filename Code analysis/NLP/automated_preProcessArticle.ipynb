{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee249b117b474795a14b987bfed99f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 15:26:42 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-04-20 15:26:44 INFO: File exists: /home/pierluigi/stanza_resources/en/default.zip\n",
      "2023-04-20 15:26:49 INFO: Finished downloading models and saved to /home/pierluigi/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import stanza\n",
    "stanza.download('en')  # Download the English model\n",
    "\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 15:52:35 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3a8f2e1787484caa1991f89dcd995a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 15:52:36 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| sentiment | sstplus  |\n",
      "========================\n",
      "\n",
      "2023-04-20 15:52:36 INFO: Using device: cuda\n",
      "2023-04-20 15:52:36 INFO: Loading: tokenize\n",
      "2023-04-20 15:52:38 INFO: Loading: sentiment\n",
      "2023-04-20 15:52:39 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment', tokenize_no_ssplit=False, max_split_size_mb=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MPQA lexicon\n",
    "lexicon = pd.read_csv(\"subjclueslen1-HLTEMNLP05.tff\", sep=\" \", header=None, \n",
    "                      names=[\"type\", \"len\", \"word\", \"pos\", \"stemmed\", \"polarity\", \"strength\"])\n",
    "\n",
    "lexicon[\"type\"] = lexicon[\"type\"].str[5:]\n",
    "lexicon[\"word\"] = lexicon[\"word\"].str[len(\"word1=\"):]\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].str[len(\"priorpolarity=\"):]\n",
    "cols_to_remove = [\"len\", \"pos\", \"stemmed\", \"strength\"]\n",
    "lexicon = lexicon.drop(columns=cols_to_remove)\n",
    "lexicon[\"type\"] = lexicon[\"type\"].replace(\"weaksubj\", 1)\n",
    "lexicon[\"type\"] = lexicon[\"type\"].replace(\"strongsubj\", 2)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"negative\", -1)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"positive\", 1)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"both\", 0)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"neutral\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.foxnews.com/politics/republicans-respond-after-irs-whistleblower-says-hunter-biden-investigation-being-mishandled\"\n",
    "\n",
    "num_stop_words_per_sentence = []\n",
    "stop_words_per_sentence = []\n",
    "filtered_sentences = []\n",
    "num_words_per_sentence = []\n",
    "avg_stop_words_per_sentence = []\n",
    "total_words = 0\n",
    "results = []\n",
    "scores_list = []\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_article(url):\n",
    "    # Create a newspaper Article object\n",
    "    article = newspaper.Article(url)\n",
    "\n",
    "    # Download and parse the article\n",
    "    article.download()\n",
    "    article.parse()\n",
    "\n",
    "    # Extract the title, subtitle, description, and main text\n",
    "    title = article.title.strip()\n",
    "    subtitle = article.meta_data.get(\"description\", \"\").strip()\n",
    "    description = article.meta_description.strip()\n",
    "    text = article.text.strip()\n",
    "\n",
    "    # Set the subtitle to the description if it is empty\n",
    "    if not subtitle:\n",
    "        subtitle = description.strip()\n",
    "\n",
    "    # Concatenate the extracted strings\n",
    "    article_text = f\"{title}\\n\\n{subtitle}\\n\\n{text}\"\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(article_text)\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence into words\n",
    "        words = word_tokenize(sentence)\n",
    "        num_words = len(words)\n",
    "        total_words += num_words\n",
    "        \n",
    "        # Identify the stop words in the sentence\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [w for w in words if not w.lower() in stop_words]\n",
    "        \n",
    "        # Add the number of stop words and filtered sentence to the output\n",
    "        num_stop_words = num_words - len(filtered_words)\n",
    "        num_stop_words_per_sentence.append(num_stop_words)\n",
    "        stop_words_per_sentence.append(filtered_words)\n",
    "        filtered_sentences.append(\" \".join(filtered_words))\n",
    "        num_words_per_sentence.append(num_words)\n",
    "        \n",
    "        # Calculate the average number of stop words per sentence\n",
    "        avg_stop_words_per_sentence.append(num_stop_words / num_words)\n",
    "\n",
    "        # Perform stemming on each word using the Porter stemmer\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "        # Combine the stemmed words back into a single string\n",
    "        output_text = ' '.join(stemmed_words)\n",
    "\n",
    "        doc = nlp(sentence)\n",
    "        sentiment = doc.sentences[0].sentiment\n",
    "        score = sentiment.score if hasattr(sentiment, 'score') else 0.0\n",
    "        \n",
    "        # Map the sentiment score to a label\n",
    "        if score < 0.0:\n",
    "            label = 'negative'\n",
    "        elif score > 0.0:\n",
    "            label = 'positive'\n",
    "        else:\n",
    "            label = 'neutral'\n",
    "        \n",
    "        # Add the sentence sentiment to the results list\n",
    "        results.append((sentence, label, score))\n",
    "\n",
    "        scores = analyzer.polarity_scores(sentence)\n",
    "        score_list = [scores['neg'], scores['neu'], scores['pos']]\n",
    "        scores_list.append(score_list)\n",
    "    if not scores_list:\n",
    "        return None\n",
    "    \n",
    "    for word in article_text.split():\n",
    "        word = word.strip().lower()\n",
    "        if word in lexicon.word.tolist():\n",
    "            polarity = lexicon[lexicon.word == word].polarity.values[0]\n",
    "            scores.append(polarity)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    num_stop_words = sum(num_stop_words_per_sentence)\n",
    "    num_sentences = len(sentences)\n",
    "    avg_stop_words_per_sentence_all = num_stop_words / num_sentences\n",
    "    max_stop_words_per_sentence = max(num_stop_words_per_sentence)\n",
    "    min_stop_words_per_sentence = min(num_stop_words_per_sentence)\n",
    "    avg_stop_words_per_word = num_stop_words / sum(num_words_per_sentence)\n",
    "\n",
    "    scores_array = np.array(scores_list)\n",
    "    avg_scores = np.mean(scores_array, axis=0)\n",
    "    max_scores = np.max(scores_array, axis=0)\n",
    "    min_scores = np.min(scores_array, axis=0)\n",
    "    std_scores = np.std(scores_array, axis=0)\n",
    "\n",
    "    # Calculate statistics\n",
    "    avg_score = np.mean(scores)\n",
    "    max_score = np.max(scores)\n",
    "    min_score = np.min(scores)\n",
    "    sd_score = np.std(scores)\n",
    "    \n",
    "    # Calculate the average number of stop words per article\n",
    "    avg_stop_words_per_sentence_avg = sum(avg_stop_words_per_sentence) / len(avg_stop_words_per_sentence)\n",
    "\n",
    "    # initialize the sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # analyze the sentiment of the text\n",
    "    scores = analyzer.polarity_scores(article_text)\n",
    "\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    tokens = [token for token in tokens if not token in nltk.corpus.stopwords.words('english')]\n",
    "    sentiwordnet_scores = []\n",
    "    for token in tokens:\n",
    "        pos_score = 0\n",
    "        neg_score = 0\n",
    "        synsets = swn.senti_synsets(token)\n",
    "        for synset in synsets:\n",
    "            pos_score += synset.pos_score()\n",
    "            neg_score += synset.neg_score()\n",
    "        if pos_score > neg_score:\n",
    "          sentiment_score = 1\n",
    "        elif neg_score > pos_score:\n",
    "            sentiment_score = -1\n",
    "        else:\n",
    "            sentiment_score = 0\n",
    "        sentiwordnet_scores.append(sentiment_score)\n",
    "    assert(len(sentiwordnet_scores) == len(tokens))\n",
    "\n",
    "    return {\n",
    "        'article_text': article_text,\n",
    "        'sentences': sentences,\n",
    "        'num_stop_words': num_stop_words,\n",
    "        \"total_words\": total_words,\n",
    "        'avg_stop_words_per_sentence_all': avg_stop_words_per_sentence_all,\n",
    "        'max_stop_words_per_sentence': max_stop_words_per_sentence,\n",
    "        'min_stop_words_per_sentence': min_stop_words_per_sentence,\n",
    "        'avg_stop_words_per_word': avg_stop_words_per_word,\n",
    "        'avg_stop_words_per_sentence': avg_stop_words_per_sentence,\n",
    "        'avg_stop_words_per_sentence_avg': avg_stop_words_per_sentence_avg,\n",
    "        'filtered_sentences': filtered_sentences,\n",
    "        'stop_words_per_sentence': stop_words_per_sentence,\n",
    "        'num_words_per_sentence': num_words_per_sentence,\n",
    "        'output_text': output_text\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'total_words' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[39m=\u001b[39m preprocess_article(url)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mArticle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mpreprocess_article\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     29\u001b[0m words \u001b[39m=\u001b[39m word_tokenize(sentence)\n\u001b[1;32m     30\u001b[0m num_words \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(words)\n\u001b[0;32m---> 31\u001b[0m total_words \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m num_words\n\u001b[1;32m     33\u001b[0m \u001b[39m# Identify the stop words in the sentence\u001b[39;00m\n\u001b[1;32m     34\u001b[0m stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'total_words' referenced before assignment"
     ]
    }
   ],
   "source": [
    "results = preprocess_article(url)\n",
    "\n",
    "print(\"Article\")\n",
    "print()\n",
    "print(results[\"article_text\"])\n",
    "print()\n",
    "print(\"Sentences\")\n",
    "print()\n",
    "for sentence in results[\"filtered_sentences\"]:\n",
    "    print(sentence)\n",
    "    print(\"Average number of stop words per sentence:\", round(results[\"avg_stop_words_per_sentence\"][results[\"filtered_sentences\"].index(sentence)], 2))\n",
    "    print()\n",
    "for sentence in results[\"filtered_sentences\"]:\n",
    "        words = word_tokenize(sentence)\n",
    "        print(words)\n",
    "\n",
    "print(\"Statistics on stop words:\")\n",
    "print(\"Total number of words:\", results[\"total_words\"])\n",
    "print(\"Number of stop words:\", results[\"num_stop_words\"])\n",
    "print(\"Maximum number of stop words per sentence:\", results[\"max_stop_words_per_sentence\"])\n",
    "print(\"Minimum number of stop words per sentence:\", results[\"min_stop_words_per_sentence\"])\n",
    "print(\"Average number of stop words per article:\", round(results[\"avg_stop_words_per_word\"], 2))\n",
    "\n",
    "print(\"Total number of words:\", total_words)  # print the total sum of all words\n",
    "print()\n",
    "print(\"Stemmed text\")\n",
    "print()\n",
    "print(results[\"output_text\"])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
