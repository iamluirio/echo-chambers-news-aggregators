{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Pre-processing Text Article, and saving the score in a txt file for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155d0140d7a84588a8b5596451cb01d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 17:59:32 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-05-03 17:59:34 INFO: File exists: /home/pierluigi/stanza_resources/en/default.zip\n",
      "2023-05-03 17:59:40 INFO: Finished downloading models and saved to /home/pierluigi/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import stanza\n",
    "stanza.download('en')  # Download the English model\n",
    "\n",
    "from readability import Readability\n",
    "\n",
    "import spacy\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "import csv\n",
    "from tabulate import tabulate\n",
    "import newspaper\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 17:59:41 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294601737b2d451e88bc0e4118a1bcb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 17:59:42 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| sentiment | sstplus  |\n",
      "========================\n",
      "\n",
      "2023-05-03 17:59:42 INFO: Using device: cpu\n",
      "2023-05-03 17:59:42 INFO: Loading: tokenize\n",
      "2023-05-03 17:59:42 INFO: Loading: sentiment\n",
      "2023-05-03 17:59:42 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Setting the use_gpu=False, it uses the CPU instead of the GPU for calculating stuff, and also for printing the results. And it couldn't run out of memory.\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment', tokenize_no_ssplit=False, max_split_size_mb=15, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MPQA lexicon\n",
    "lexicon = pd.read_csv(\"/home/pierluigi/Documents/echo_chambers_intership/Code analysis/NLP/Single modules/subjclueslen1-HLTEMNLP05.tff\", sep=\" \", header=None, \n",
    "                      names=[\"type\", \"len\", \"word\", \"pos\", \"stemmed\", \"polarity\", \"strength\"])\n",
    "\n",
    "lexicon[\"type\"] = lexicon[\"type\"].str[5:]\n",
    "lexicon[\"word\"] = lexicon[\"word\"].str[len(\"word1=\"):]\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].str[len(\"priorpolarity=\"):]\n",
    "cols_to_remove = [\"len\", \"pos\", \"stemmed\", \"strength\"]\n",
    "lexicon = lexicon.drop(columns=cols_to_remove)\n",
    "lexicon[\"type\"] = lexicon[\"type\"].replace(\"weaksubj\", 1)\n",
    "lexicon[\"type\"] = lexicon[\"type\"].replace(\"strongsubj\", 2)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"negative\", -1)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"positive\", 1)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"both\", 0)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"neutral\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(url):\n",
    "    # Create a newspaper Article object\n",
    "    article = newspaper.Article(url)\n",
    "\n",
    "    # Download and parse the article\n",
    "    article.download()\n",
    "    article.parse()\n",
    "\n",
    "    # Extract the title, subtitle, description, and main text\n",
    "    title = article.title.strip()\n",
    "    subtitle = article.meta_data.get(\"description\", \"\").strip()\n",
    "    description = article.meta_description.strip()\n",
    "    text = article.text.strip()\n",
    "\n",
    "    # Set the subtitle to the description if it is empty\n",
    "    if not subtitle:\n",
    "        subtitle = description.strip()\n",
    "\n",
    "    # Concatenate the extracted strings\n",
    "    article_text = f\"{title}\\n\\n{subtitle}\\n\\n{text}\"\n",
    "\n",
    "    # Return the concatenated string\n",
    "    return article_text, title, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(article):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(article)\n",
    "\n",
    "    num_stop_words_per_sentence = []\n",
    "    stop_words_per_sentence = []\n",
    "    filtered_sentences = []\n",
    "    filtered_words = []\n",
    "    num_words_per_sentence = []\n",
    "    avg_stop_words_per_sentence = []\n",
    "    total_words = 0\n",
    "    total_adjectives = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Tokenize the sentence into words\n",
    "        words = word_tokenize(sentence)\n",
    "        all_words = len(words)\n",
    "        total_words += all_words\n",
    "        \n",
    "        # Identify the stop words in the sentence\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words_found = [word for word in words if word.lower() in stop_words]\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        \n",
    "        # Add the number of stop words and filtered sentence to the output\n",
    "        num_stop_words = all_words - len(filtered_words)\n",
    "        num_stop_words_per_sentence.append(num_stop_words)\n",
    "        stop_words_per_sentence.append(stop_words_found)\n",
    "        filtered_sentences.append(\" \".join(filtered_words))\n",
    "        num_words_per_sentence.append(all_words)\n",
    "        \n",
    "        # Calculate the average number of stop words per sentence\n",
    "        avg_stop_words_per_sentence.append(num_stop_words / all_words)\n",
    "\n",
    "        #POS tagging calculations\n",
    "        tagged_words = pos_tag(words)\n",
    "        num_adjectives = len([word for word, tag in tagged_words if tag.startswith('JJ')])\n",
    "        total_adjectives += num_adjectives\n",
    "\n",
    "    # Calculate summary statistics\n",
    "    num_stop_words = sum(num_stop_words_per_sentence)\n",
    "    max_stop_words_per_sentence = max(num_stop_words_per_sentence)\n",
    "    min_stop_words_per_sentence = min(num_stop_words_per_sentence)\n",
    "    \n",
    "    # Calculate the average number of stop words per article\n",
    "    avg_stop_words_per_sentence_avg = sum(avg_stop_words_per_sentence) / len(avg_stop_words_per_sentence)\n",
    "    \n",
    "    # POS tagging \n",
    "    avg_adjectives = total_adjectives / total_words\n",
    "\n",
    "    return sentences, filtered_words, filtered_sentences, stop_words_per_sentence, num_stop_words_per_sentence, avg_stop_words_per_sentence, total_words, num_stop_words, max_stop_words_per_sentence, min_stop_words_per_sentence, avg_stop_words_per_sentence_avg, num_words_per_sentence, total_adjectives, avg_adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_sentiment_analysis(text):\n",
    "    doc = nlp(text)\n",
    "    s_sentiment_scores = []\n",
    "\n",
    "    # Sentiment analysis using Stanza library\n",
    "    for sentence in doc.sentences:\n",
    "        s_sentiment_scores.append(sentence.sentiment)\n",
    "    \n",
    "    return s_sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_sentiment_analysis(sentences):\n",
    "    # initialize the Vader sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    v_scores_list = []\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        v_scores = analyzer.polarity_scores(sentence)\n",
    "        v_score_list = [v_scores['neg'], v_scores['neu'], v_scores['pos']]\n",
    "        v_scores_list.append(v_score_list)\n",
    "    \n",
    "    # Vader scores\n",
    "    v_scores_array = np.array(v_scores_list)\n",
    "    v_avg_scores = np.mean(v_scores_array, axis=0)\n",
    "    v_max_scores = np.max(v_scores_array, axis=0)\n",
    "    v_min_scores = np.min(v_scores_array, axis=0)\n",
    "    v_std_scores = np.std(v_scores_array, axis=0)\n",
    "\n",
    "    return v_avg_scores, v_max_scores, v_min_scores, v_std_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpqa_sentiment_analysis(article):\n",
    "    mpqa_scores = []\n",
    "\n",
    "    for word in article.split():\n",
    "        word = word.strip().lower()\n",
    "        if word in lexicon.word.tolist():\n",
    "            polarity = lexicon[lexicon.word == word].polarity.values[0]\n",
    "            mpqa_scores.append(polarity)\n",
    "        \n",
    "    # MPQA scores\n",
    "    mpqa_avg_score = np.mean(mpqa_scores)\n",
    "    mpqa_max_score = np.max(mpqa_scores)\n",
    "    mpqa_min_score = np.min(mpqa_scores)\n",
    "    mpqa_sd_score = np.std(mpqa_scores)\n",
    "\n",
    "    return mpqa_avg_score, mpqa_max_score, mpqa_min_score, mpqa_sd_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiwordnet_sentiment_analysis(article):\n",
    "    sentiwordnet_final_score = 0\n",
    "\n",
    "    # Loop through each word in the text\n",
    "    sentiment_score = 0\n",
    "    num_synsets = 0\n",
    "\n",
    "    for word in article.split():\n",
    "        synsets = wn.synsets(word)\n",
    "        if len(synsets) > 0:\n",
    "            synset = synsets[0]\n",
    "            senti_synset = swn.senti_synset(synset.name())\n",
    "            sentiment_score += senti_synset.pos_score() - senti_synset.neg_score()\n",
    "            num_synsets += 1\n",
    "    \n",
    "    # Calculate final score        \n",
    "    if num_synsets > 0:\n",
    "        sentiwordnet_final_score = sentiment_score / num_synsets\n",
    "    else:\n",
    "        sentiwordnet_final_score = 0\n",
    "    \n",
    "    return sentiwordnet_final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_analysis(article):\n",
    "    read = Readability(article)\n",
    "    metrics = {}\n",
    "\n",
    "    # Flesch Kincaid\n",
    "    metrics['flesch_kincaid'] = read.flesch_kincaid()\n",
    "\n",
    "    # Flesch Reading Ease\n",
    "    metrics['flesch_reading'] = read.flesch()\n",
    "\n",
    "    # Dale Chall Readability\n",
    "    metrics['dale_chall'] = read.dale_chall()\n",
    "\n",
    "    # Automated Readability Index (ARI)\n",
    "    metrics['ari'] = read.ari()\n",
    "\n",
    "    # Coleman Liau Index\n",
    "    metrics['coleman_liau'] = read.coleman_liau()\n",
    "\n",
    "    # Gunning Fog\n",
    "    metrics['gunning_fog'] = read.gunning_fog()\n",
    "\n",
    "    # SMOG: at least 30 sentences required. Uncomment if needed.\n",
    "    # metrics['smog'] = read.smog()\n",
    "\n",
    "    # SPACHE\n",
    "    metrics['spache'] = read.spache()\n",
    "\n",
    "    # Linsear Write\n",
    "    metrics['linsear_write'] = read.linsear_write()\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_lda_algorithm(filtered_words):\n",
    "    # Gensim-LDA analysis\n",
    "    bigrams = list(nltk.bigrams(filtered_words))\n",
    "    lemmatized_bigrams = []\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for bigram in bigrams:\n",
    "        lemma1 = lemmatizer.lemmatize(bigram[0])\n",
    "        lemma2 = lemmatizer.lemmatize(bigram[1])\n",
    "        lemmatized_bigrams.append([lemma1, lemma2])\n",
    "    \n",
    "    # Create Dictionary \n",
    "    id2word = corpora.Dictionary(lemmatized_bigrams) \n",
    "\n",
    "    # Create Corpus \n",
    "    texts = lemmatized_bigrams\n",
    "\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "    doc_lda = lda_model[corpus]\n",
    "\n",
    "    # Compute perplexity\n",
    "    perplexity_lda = lda_model.log_perplexity(corpus)\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts = lemmatized_bigrams, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "    return lda_model, perplexity_lda, coherence_lda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_tree(node, depth):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return max([walk_tree(child, depth + 1) for child in node.children], default=depth)\n",
    "    else:\n",
    "        return depth\n",
    "    \n",
    "def build_dependency_tree(article):\n",
    "    doc = nlp_spacy(article)\n",
    "    depths = {}\n",
    "    tree_lengths = {}\n",
    "    for sent in doc.sents:\n",
    "        root = sent.root\n",
    "        depth = walk_tree(root, 0)\n",
    "        depths[root.orth_] = depth\n",
    "        tree_lengths[sent.text.strip()] = depth\n",
    "\n",
    "    lengths = list(tree_lengths.values())\n",
    "    avg_length = sum(lengths) / len(lengths)\n",
    "    max_length = max(lengths)\n",
    "    min_length = min(lengths)\n",
    "    max_depth = max(depths.values())\n",
    "    max_depth_words = [word for word, depth in depths.items() if depth == max_depth]\n",
    "    return tree_lengths, max_depth, max_depth_words, avg_length, max_length, min_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_article(url):\n",
    "    article, title, text =  get_article_info(url)\n",
    "    sentences, filtered_words, filtered_sentences, stop_words_per_sentence, num_stop_words_per_sentence, avg_stop_words_per_sentence, total_words, num_stop_words, max_stop_words_per_sentence, min_stop_words_per_sentence, avg_stop_words_per_sentence_avg, num_words_per_sentence, total_adjectives, avg_adjectives  = preprocess_text(article)\n",
    "    s_sentiment_scores = stanza_sentiment_analysis(text)\n",
    "    v_avg_scores, v_max_scores, v_min_scores, v_std_scores = vader_sentiment_analysis(sentences)\n",
    "    mpqa_avg_score, mpqa_max_score, mpqa_min_score, mpqa_sd_score = mpqa_sentiment_analysis(article)\n",
    "    sentiwordnet_final_score = sentiwordnet_sentiment_analysis(article)\n",
    "    metrics = readability_analysis(article)\n",
    "    lda_model, perplexity_lda, coherence_lda = gensim_lda_algorithm(filtered_words)\n",
    "    tree_lengths, max_depth, max_depth_words, avg_length, max_length, min_length = build_dependency_tree(article)\n",
    "    return {\n",
    "        'title': title,\n",
    "        'num_stop_words': num_stop_words,\n",
    "        'total_words': total_words,\n",
    "        'max_stop_words_per_sentence': max_stop_words_per_sentence,\n",
    "        'min_stop_words_per_sentence': min_stop_words_per_sentence,\n",
    "        'avg_stop_words_per_sentence': avg_stop_words_per_sentence,\n",
    "        'avg_stop_words_per_sentence_avg': avg_stop_words_per_sentence_avg,\n",
    "        'filtered_sentences': filtered_sentences,\n",
    "        'stop_words_per_sentence': stop_words_per_sentence,\n",
    "        'num_words_per_sentence': num_words_per_sentence,\n",
    "        'num_stop_words_per_sentence': num_stop_words_per_sentence,\n",
    "        'total_adjectives': total_adjectives,\n",
    "        'avg_adjectives': avg_adjectives,\n",
    "        's_sentiment_scores': s_sentiment_scores,\n",
    "        'v_avg_scores': v_avg_scores,\n",
    "        'v_max_scores': v_max_scores,\n",
    "        'v_min_scores': v_min_scores,\n",
    "        'v_std_scores': v_std_scores,\n",
    "        'mpqa_avg_score': mpqa_avg_score,\n",
    "        'mpqa_max_score': mpqa_max_score,\n",
    "        'mpqa_min_score': mpqa_min_score,\n",
    "        'mpqa_sd_score': mpqa_sd_score,\n",
    "        'sentiwordnet_final_score': sentiwordnet_final_score,\n",
    "        'metrics': metrics,\n",
    "        'lda_model': lda_model,\n",
    "        'perplexity_lda': perplexity_lda,\n",
    "        'coherence_lda': coherence_lda,\n",
    "        'tree_lengths': tree_lengths,\n",
    "        'max_depth': max_depth,\n",
    "        'max_depth_words': max_depth_words,\n",
    "        'avg_length': avg_length,\n",
    "        'max_length': max_length,\n",
    "        'min_length': min_length\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores_old(urls, directory):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    for url in urls:\n",
    "        results = process_article(url)\n",
    "        # Write preprocessed article to a separate file for each URL\n",
    "        file_path = f'{directory}/{results[\"title\"]}.txt'\n",
    "\n",
    "        with open(file_path, 'w') as f:\n",
    "            # Save the information for each sentence to the file\n",
    "            for i, sentence in enumerate(results['filtered_sentences']):\n",
    "                f.write(f\"Sentence {i+1}: {sentence}\\n\")\n",
    "                f.write(f\"Total words: {results['num_words_per_sentence'][i]}\\n\")\n",
    "                f.write(f\"Filtered words: {sentence.split()}\\n\")\n",
    "                f.write(f\"Number of filtered words: {len(sentence.split())}\\n\")\n",
    "                f.write(f\"Stop words: {results['stop_words_per_sentence'][i]}\\n\")\n",
    "                f.write(f\"Number of stop words: {results['num_stop_words_per_sentence'][i]}\\n\")\n",
    "                f.write(f\"Average number of stop words per sentence: {round(results['avg_stop_words_per_sentence'][i], 2)}\\n\")\n",
    "                f.write(f\"Sentiment score: {results['s_sentiment_scores'][i]}\\n\\n\")\n",
    "                #f.write(f\"Depth: {results['tree_lengths'][i]}\\n\")\n",
    "                \n",
    "\n",
    "            # Save the general statistics on stop words to the file\n",
    "            f.write(f\"Total number of words: {results['total_words']}\\n\")\n",
    "            f.write(f\"Total number of stop words: {results['num_stop_words']}\\n\")\n",
    "            f.write(f\"Maximum number of stop words per sentence: {results['max_stop_words_per_sentence']}\\n\")\n",
    "            f.write(f\"Minimum number of stop words per sentence: {results['min_stop_words_per_sentence']}\\n\")\n",
    "            f.write(f\"Average number of stop words per article: {round(results['avg_stop_words_per_sentence_avg'], 2)}\\n\")\n",
    "\n",
    "            # Print POS tagging operations\n",
    "            f.write(f\"Total adjectives: {results['total_adjectives']}\\n\")\n",
    "            f.write(f\"Average number of adjectives in the article: {results['avg_adjectives']:.2f}\\n\\n\")\n",
    "            \n",
    "            # Stanza sentiment scores\n",
    "            f.write(f\"Stanza Average of sentiment score for all sentences: {sum(results['s_sentiment_scores']) / len(results['s_sentiment_scores'])}\\n\")\n",
    "            f.write(f\"Stanza Maximum sentiment score: {max(results['s_sentiment_scores'])}\\n\")\n",
    "            f.write(f\"Stanza Minimum sentiment score: {min(results['s_sentiment_scores'])}\\n\")\n",
    "            f.write(f\"Stanza Standard deviation: {statistics.stdev(results['s_sentiment_scores'])}\\n\\n\")\n",
    "\n",
    "            # Vader sentiment scores\n",
    "            f.write(f\"Vader average scores: {results['v_avg_scores']}\\n\")\n",
    "            f.write(f\"Vader maximum scores: {results['v_max_scores']}\\n\")\n",
    "            f.write(f\"Vader minimum scores: {results['v_min_scores']}\\n\")\n",
    "            f.write(f\"Vader standard deviation scores: {results['v_std_scores']}\\n\\n\")\n",
    "\n",
    "            # MPQA sentiment scores\n",
    "            f.write(f\"MPQA average scores: {results['mpqa_avg_score']}\\n\")\n",
    "            f.write(f\"MPQA maximum scores: {results['mpqa_max_score']}\\n\")\n",
    "            f.write(f\"MPQA minimum scores: {results['mpqa_min_score']}\\n\")\n",
    "            f.write(f\"MPQA standard deviation scores: {results['mpqa_sd_score']}\\n\\n\")\n",
    "\n",
    "            # Sentiword sentiment scores\n",
    "            f.write(f\"Sentiwordnet score: {results['sentiwordnet_final_score']} (from -1 to 1, and score of 0 indicates a neutral sentiment.)\\n\\n\")\n",
    "            \n",
    "            # Flesch_Kincaid scores\n",
    "            f.write(f\"Flesch-Kincaid score: {results['metrics']['flesch_kincaid'].score}\\n\")\n",
    "            f.write(f\"The estimated reading level of the article is: {results['metrics']['flesch_kincaid'].grade_level}\\n\\n\") \n",
    "\n",
    "            # Flesch Reading ease scores\n",
    "            f.write(f\"Flesch Reading Ease score: {results['metrics']['flesch_reading'].score}\\n\")\n",
    "            f.write(f\"The article is classified as: {results['metrics']['flesch_reading'].ease}\\n\\n\")\n",
    "\n",
    "            # Print the Dale-Chall scores\n",
    "            f.write(f\"Dale-Chall Readability score: {results['metrics']['dale_chall'].score}\\n\")\n",
    "            # Print the estimated grade levels for comprehension\n",
    "            f.write(f\"The estimated comprehension level for different grade levels is: {results['metrics']['dale_chall'].grade_levels}\\n\\n\")\n",
    "\n",
    "            # Print the ARI scores\n",
    "            f.write(f\"Automated Readability Index (ARI) score: {results['metrics']['ari'].score}, which corresponds to a grade level of {results['metrics']['ari'].grade_levels}.\\n\")\n",
    "            f.write(f\"This means that the text can be read by someone who is around {results['metrics']['ari'].ages} years old.\\n\\n\")\n",
    "\n",
    "            # Print the Coleman-Liau scores\n",
    "            f.write(f\"Coleman-Liau Index Score: {results['metrics']['coleman_liau'].score}\\n\")\n",
    "            f.write(f\"Estimated Grade Level: {results['metrics']['coleman_liau'].grade_level}\\n\\n\")\n",
    "\n",
    "            # Print the Gunning Fog scores\n",
    "            f.write(f\"Gunning Fog score: {results['metrics']['gunning_fog'].score}\\n\")\n",
    "            f.write(f\"The estimated grade level for comprehension is: {results['metrics']['gunning_fog'].grade_level}\\n\\n\")\n",
    "\n",
    "            # Print the SMOG scores\n",
    "            #f.write(f\"SMOG score: {results['metrics']['smog'].score}. This corresponds to a grade level of {results['metrics']['smog'].grade_level}.\")\n",
    "            \n",
    "            # Print the SPACHE scores\n",
    "            f.write(f\"SPACHE score: {results['metrics']['spache'].score}\\n\")\n",
    "            f.write(f\"This corresponds to a grade level of {results['metrics']['spache'].grade_level}.\\n\\n\")\n",
    "\n",
    "            # Print the Linsear Write Index scores\n",
    "            f.write(f\"Linsear Write Index score: {results['metrics']['linsear_write'].score}\\n\")\n",
    "            f.write(\"Approximate grade level equivalent: {}\\n\\n\".format(results['metrics']['linsear_write'].grade_level))\n",
    "\n",
    "            # Gensim-LDA analysis\n",
    "            f.write(f\"Perplexity (how well the LDA model predicts the corpus) of the article: {results['perplexity_lda']}\\n\")\n",
    "            f.write(f\"Coherence (how coherent the topics are) of the article: {results['coherence_lda']}\\n\\n\")\n",
    "\n",
    "            # Dependency tree height\n",
    "            f.write(f\"Max tree depth: {results['max_depth']}\\n\")\n",
    "            f.write(f\"Words at max depth: {', '.join(results['max_depth_words'])}\\n\")\n",
    "            f.write(f\"Average tree length: {results['avg_length']:.2f}\\n\")\n",
    "            f.write(f\"Maximum tree length: {results['max_length']}\\n\")\n",
    "            f.write(f\"Minimum tree length: {results['min_length']}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(urls, directory):\n",
    "    # Preprocessed directory \n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    for url in urls:\n",
    "        results = process_article(url)\n",
    "\n",
    "        # Create a directory for the article\n",
    "        article_directory = f'{directory}/{results[\"title\"]}'\n",
    "        os.makedirs(article_directory, exist_ok=True)\n",
    "\n",
    "        # Create a list to store the information for each sentence\n",
    "        sentence_info = []\n",
    "\n",
    "        # Append the information for each sentence to the list\n",
    "        for i, sentence in enumerate(results['filtered_sentences']):\n",
    "            sentences_info = {\n",
    "                'Sentence': f'Sentence {i+1}',\n",
    "                'Total words': results['num_words_per_sentence'][i],\n",
    "                'Filtered words': sentence.split(),\n",
    "                'Number of filtered words': len(sentence.split()),\n",
    "                'Stop words': results['stop_words_per_sentence'][i],\n",
    "                'Number of stop words': results['num_stop_words_per_sentence'][i],\n",
    "                'Average number of stop words per sentence': round(results['avg_stop_words_per_sentence'][i], 2),\n",
    "                'Sentiment score': results['s_sentiment_scores'][i]\n",
    "            }\n",
    "            sentence_info.append(sentences_info)\n",
    "\n",
    "        # Create a list to store the general statistics on stop words\n",
    "        general_stats = [\n",
    "            {\n",
    "                'Total number of words': results['total_words'],\n",
    "                'Total number of stop words': results['num_stop_words'],\n",
    "                'Maximum number of stop words per sentence': results['max_stop_words_per_sentence'],\n",
    "                'Minimum number of stop words per sentence': results['min_stop_words_per_sentence'],\n",
    "                'Average number of stop words per article': round(results['avg_stop_words_per_sentence_avg'], 2)\n",
    "            }\n",
    "        ]   \n",
    "\n",
    "        # Create a list to store the POS tagging operations\n",
    "        pos_tagging_ops = [\n",
    "            {\n",
    "                'Total adjectives': results['total_adjectives'],\n",
    "                'Average number of adjectives in the article': round(results['avg_adjectives'], 2)\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Create a list to store the sentiment scores\n",
    "        sentiment_scores = [\n",
    "            {\n",
    "                'Stanza Average of sentiment score for all sentences': sum(results['s_sentiment_scores']) / len(results['s_sentiment_scores']),\n",
    "                'Stanza Maximum sentiment score': max(results['s_sentiment_scores']),\n",
    "                'Stanza Minimum sentiment score': min(results['s_sentiment_scores']),\n",
    "                'Stanza Standard deviation': statistics.stdev(results['s_sentiment_scores']),\n",
    "                'Vader average scores': results['v_avg_scores'],\n",
    "                'Vader maximum scores': results['v_max_scores'],\n",
    "                'Vader minimum scores': results['v_min_scores'],\n",
    "                'Vader standard deviation scores': results['v_std_scores'],\n",
    "                'MPQA average scores': results['mpqa_avg_score'],\n",
    "                'MPQA maximum scores': results['mpqa_max_score'],\n",
    "                'MPQA minimum scores': results['mpqa_min_score'],\n",
    "                'MPQA standard deviation scores': results['mpqa_sd_score'],\n",
    "                'Sentiwordnet score': results['sentiwordnet_final_score']\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Create a list to store the Flesch-Kincaid scores\n",
    "        readability_scores = [\n",
    "            {\n",
    "                'Flesch-Kincaid score': results['metrics']['flesch_kincaid'].score,\n",
    "                'Estimated reading level of the article': results['metrics']['flesch_kincaid'].grade_level,\n",
    "                'Flesch-Reading score': results['metrics']['flesch_reading'].score,\n",
    "                'The article is classified as': results['metrics']['flesch_reading'].ease,\n",
    "                'Dale-Chall Readability score': results['metrics']['dale_chall'].score,\n",
    "                'The estimated comprehension level for different grade levels': results['metrics']['dale_chall'].grade_levels,\n",
    "                'Automated Readability Index (ARI) score': results['metrics']['ari'].score, \n",
    "                'It corresponds to a grade level of': results['metrics']['ari'].grade_levels,\n",
    "                'This means that the text can be read by someone who is around': results['metrics']['ari'].ages,\n",
    "                'Coleman-Liau Index Score': results['metrics']['coleman_liau'].score,\n",
    "                'Estimated Grade Level': results['metrics']['coleman_liau'].grade_level,\n",
    "                'Gunning Fog score': results['metrics']['gunning_fog'].score,\n",
    "                'The estimated grade level for comprehension is': results['metrics']['gunning_fog'].grade_level,\n",
    "                'SPACHE score': results['metrics']['spache'].score,\n",
    "                'This corresponds to a grade level of': results['metrics']['spache'].grade_level,\n",
    "                'Linsear Write Index score': results['metrics']['linsear_write'].score,\n",
    "                'Approximate grade level equivalent': results['metrics']['linsear_write'].grade_level,\n",
    "                'Perplexity (how well the LDA model predicts the corpus) of the article': results['perplexity_lda'],\n",
    "                'Coherence (how coherent the topics are) of the article': results['coherence_lda']\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Create a list to store the dependency tree scores\n",
    "        dependency_tree_scores = [\n",
    "            {\n",
    "                'Max tree depth': results['max_depth'],\n",
    "                'Words at max depth': ', '.join(results['max_depth_words']),\n",
    "                'Average tree length': results['avg_length'],\n",
    "                'Maximum tree length': results['max_length'],\n",
    "                'Minimum tree length': results['min_length']\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        with open(f'{article_directory}/sentences_info.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "            fieldnames = ['Sentence', 'Total words', 'Filtered words', 'Number of filtered words', 'Stop words', 'Number of stop words', 'Average number of stop words per sentence', 'Sentiment score']\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            if file.tell() == 0:\n",
    "                writer.writeheader()\n",
    "            for sentence in sentence_info:\n",
    "                if isinstance(sentence, dict):\n",
    "                    writer.writerow(sentence)\n",
    "        # Set the path to the CSV file\n",
    "        csv_path = f'{article_directory}/sentences_info.csv'\n",
    "\n",
    "        # Load the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "        display(df) \n",
    "\n",
    "        # Write the general statistics on stop words to a CSV file\n",
    "        with open(f'{article_directory}/general_stats.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "            fieldnames = ['Total number of words', 'Total number of stop words', 'Maximum number of stop words per sentence', 'Minimum number of stop words per sentence', 'Average number of stop words per article']\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            if file.tell() == 0:\n",
    "                writer.writeheader()\n",
    "            for stats in general_stats:\n",
    "                writer.writerow(stats)\n",
    "        # Set the path to the CSV file\n",
    "        csv_path = f'{article_directory}/general_stats.csv'\n",
    "\n",
    "        # Load the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "        display(df) \n",
    "\n",
    "        # Write the general statistics on POS tagging to a CSV file\n",
    "        with open(f'{article_directory}/pos_tagging.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "            fieldnames = ['Total adjectives', 'Average number of adjectives in the article']\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            if file.tell() == 0:\n",
    "                writer.writeheader()\n",
    "            for stats in pos_tagging_ops:\n",
    "                writer.writerow(stats)\n",
    "        # Set the path to the CSV file\n",
    "        csv_path = f'{article_directory}/pos_tagging.csv'\n",
    "\n",
    "        # Load the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "        display(df) \n",
    "        \n",
    "\n",
    "        # Write the sentiment scores to a CSV file\n",
    "        with open(f'{article_directory}/sentiment_scores.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "            fieldnames = ['Stanza Average of sentiment score for all sentences', 'Stanza Maximum sentiment score', 'Stanza Minimum sentiment score', 'Stanza Standard deviation', 'Vader average scores', 'Vader maximum scores', 'Vader minimum scores', 'Vader standard deviation scores', 'MPQA average scores', 'MPQA maximum scores', 'MPQA minimum scores', 'MPQA standard deviation scores', 'Sentiwordnet score']\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            if file.tell() == 0:\n",
    "                writer.writeheader()\n",
    "            for stats in sentiment_scores:\n",
    "                writer.writerow(stats)\n",
    "        # Set the path to the CSV file\n",
    "        csv_path = f'{article_directory}/sentiment_scores.csv'\n",
    "\n",
    "        # Load the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "        display(df) \n",
    "        \n",
    "        # Write the readability scores to a CSV file\n",
    "        with open(f'{article_directory}/readability_scores.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "            fieldnames = ['Flesch-Kincaid score', 'Estimated reading level of the article', 'Flesch-Reading score', 'The article is classified as', 'Dale-Chall Readability score', 'The estimated comprehension level for different grade levels', 'Automated Readability Index (ARI) score', 'It corresponds to a grade level of', 'This means that the text can be read by someone who is around', 'Coleman-Liau Index Score', 'Estimated Grade Level', 'Gunning Fog score', 'The estimated grade level for comprehension is', 'SPACHE score', 'This corresponds to a grade level of', 'Linsear Write Index score', 'Approximate grade level equivalent', 'Perplexity (how well the LDA model predicts the corpus) of the article', 'Coherence (how coherent the topics are) of the article']\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            if file.tell() == 0:\n",
    "                writer.writeheader()\n",
    "            for stats in readability_scores:\n",
    "                writer.writerow(stats)\n",
    "        # Set the path to the CSV file\n",
    "        csv_path = f'{article_directory}/readability_scores.csv'\n",
    "\n",
    "        # Load the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "        display(df) \n",
    "        \n",
    "        # Write the dependency tree scores to a CSV file\n",
    "        with open(f'{article_directory}/dependency_tree_scores.csv', mode='a', newline='', encoding='utf-8') as file:\n",
    "            fieldnames = ['Max tree depth', 'Words at max depth', ', ', 'Average tree length', 'Maximum tree length', 'Minimum tree length']\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            if file.tell() == 0:\n",
    "                writer.writeheader()\n",
    "            for stats in dependency_tree_scores:\n",
    "                writer.writerow(stats)\n",
    "        # Set the path to the CSV file\n",
    "        csv_path = f'{article_directory}/dependency_tree_scores.csv'\n",
    "\n",
    "        # Load the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "        display(df) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Total words</th>\n",
       "      <th>Filtered words</th>\n",
       "      <th>Number of filtered words</th>\n",
       "      <th>Stop words</th>\n",
       "      <th>Number of stop words</th>\n",
       "      <th>Average number of stop words per sentence</th>\n",
       "      <th>Sentiment score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence 1</td>\n",
       "      <td>38</td>\n",
       "      <td>['Republicans', 'respond', 'IRS', 'whistleblow...</td>\n",
       "      <td>23</td>\n",
       "      <td>['after', 'is', 'being', 'of', 'are', 'for', '...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence 2</td>\n",
       "      <td>35</td>\n",
       "      <td>['Lawmakers', 'Capitol', 'Hill', 'calling', 'B...</td>\n",
       "      <td>22</td>\n",
       "      <td>['on', 'are', 'for', 'the', 'to', 'be', 'for',...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence 3</td>\n",
       "      <td>26</td>\n",
       "      <td>['congressional', 'outcries', 'come', 'whistle...</td>\n",
       "      <td>16</td>\n",
       "      <td>['The', 'as', 'a', 'the', 'an', 'into', 'is', ...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence 4</td>\n",
       "      <td>14</td>\n",
       "      <td>['whistleblower', 'also', 'alleges', '``', 'cl...</td>\n",
       "      <td>10</td>\n",
       "      <td>['The', 'of', 'in', 'the']</td>\n",
       "      <td>4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence 5</td>\n",
       "      <td>41</td>\n",
       "      <td>['``', '’', 'deeply', 'concerning', 'Biden', '...</td>\n",
       "      <td>30</td>\n",
       "      <td>['It', 's', 'that', 'the', 'be', 'by', 'to', '...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence 6</td>\n",
       "      <td>28</td>\n",
       "      <td>['Comer', ',', 'R-Ky.', ',', 'also', 'said', '...</td>\n",
       "      <td>24</td>\n",
       "      <td>['have', 'the', 'to', 'from']</td>\n",
       "      <td>4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence 7</td>\n",
       "      <td>37</td>\n",
       "      <td>['HUNTER', 'BIDEN', 'INVESTIGATION', 'MISHANDL...</td>\n",
       "      <td>27</td>\n",
       "      <td>['BEING', 'OF', 'The', 'on', 'and', 'has', 'be...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence 8</td>\n",
       "      <td>40</td>\n",
       "      <td>['’', 'clear', 'investigation', 'Hunter', 'mem...</td>\n",
       "      <td>25</td>\n",
       "      <td>['It', 's', 'from', 'our', 'that', 'and', 'oth...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence 9</td>\n",
       "      <td>19</td>\n",
       "      <td>['``', '’', 'wondering', 'along', 'heck', 'DOJ...</td>\n",
       "      <td>8</td>\n",
       "      <td>['We', 've', 'been', 'all', 'where', 'the', 't...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence 10</td>\n",
       "      <td>21</td>\n",
       "      <td>['appears', 'Biden', 'Administration', 'may', ...</td>\n",
       "      <td>12</td>\n",
       "      <td>['Now', 'it', 'the', 'have', 'been', 'to', 'th...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sentence 11</td>\n",
       "      <td>27</td>\n",
       "      <td>['Comer', 'added', ',', '``', 'House', 'Oversi...</td>\n",
       "      <td>18</td>\n",
       "      <td>['The', 'will', 'to', 'in', 'the', 'who', 'be'...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sentence 12</td>\n",
       "      <td>30</td>\n",
       "      <td>['Oversight', 'Committee', 'also', 'continue',...</td>\n",
       "      <td>18</td>\n",
       "      <td>['The', 'will', 'to', 'our', 'into', 'the', 's...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentence 13</td>\n",
       "      <td>10</td>\n",
       "      <td>['Americans', 'demand', 'answers', ',', 'trans...</td>\n",
       "      <td>9</td>\n",
       "      <td>['and']</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sentence 14</td>\n",
       "      <td>14</td>\n",
       "      <td>['President', 'Biden', '’', 'son', 'Hunter', '...</td>\n",
       "      <td>10</td>\n",
       "      <td>['s', 'has', 'been', 'under']</td>\n",
       "      <td>4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentence 15</td>\n",
       "      <td>17</td>\n",
       "      <td>['investigation', 'concerns', 'presence', 'sus...</td>\n",
       "      <td>14</td>\n",
       "      <td>['The', 'the', 'of']</td>\n",
       "      <td>3</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sentence 16</td>\n",
       "      <td>11</td>\n",
       "      <td>['led', 'Delaware', 'U.S.', 'Attorney', 'David...</td>\n",
       "      <td>7</td>\n",
       "      <td>['It', 'is', 'being', 'by']</td>\n",
       "      <td>4</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sentence 17</td>\n",
       "      <td>29</td>\n",
       "      <td>['Across', 'Congress', ',', 'Sen.', 'Ted', 'Cr...</td>\n",
       "      <td>23</td>\n",
       "      <td>['on', 'for', 'to', 'the', 'the', 's']</td>\n",
       "      <td>6</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sentence 18</td>\n",
       "      <td>26</td>\n",
       "      <td>['``', '’', 'national', 'security', 'reason', ...</td>\n",
       "      <td>17</td>\n",
       "      <td>['There', 's', 'no', 'to', 'them', 'during', '...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sentence 19</td>\n",
       "      <td>21</td>\n",
       "      <td>['``', '’', 'reason', 'whatsoever', 'want', 'p...</td>\n",
       "      <td>10</td>\n",
       "      <td>['There', 's', 'no', 'other', 'than', 'if', 'y...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sentence 20</td>\n",
       "      <td>32</td>\n",
       "      <td>['SIX', 'ADDITIONAL', 'BIDEN', 'FAMILY', 'MEMB...</td>\n",
       "      <td>22</td>\n",
       "      <td>['HAVE', 'FROM', 'do', 'not', 'or', 'but', 'be...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sentence 21</td>\n",
       "      <td>25</td>\n",
       "      <td>['``', 'U.S.', 'Department', 'Treasury', 'need...</td>\n",
       "      <td>19</td>\n",
       "      <td>['The', 'of', 'to', 'on', 'the', 'on']</td>\n",
       "      <td>6</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sentence 22</td>\n",
       "      <td>31</td>\n",
       "      <td>['``', 'Janet', 'Yellen', ',', 'choice', ':', ...</td>\n",
       "      <td>19</td>\n",
       "      <td>['you', 'have', 'a', 'You', 'are', 'up', 'of',...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sentence 23</td>\n",
       "      <td>32</td>\n",
       "      <td>['According', 'Cruz', ',', 'American', 'public...</td>\n",
       "      <td>17</td>\n",
       "      <td>['to', 'the', 'should', 'be', 'to', 'the', 'is...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sentence 24</td>\n",
       "      <td>38</td>\n",
       "      <td>['BIDEN', 'FAMILY', 'RECEIVED', '$', '1M', 'HU...</td>\n",
       "      <td>28</td>\n",
       "      <td>['MORE', 'THAN', 'FROM', 'AFTER', 'The', 'the'...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence 25</td>\n",
       "      <td>14</td>\n",
       "      <td>['added', ',', '``', 'benign', ',', 'release',...</td>\n",
       "      <td>9</td>\n",
       "      <td>['He', 'If', 'this', 'is', 'the']</td>\n",
       "      <td>5</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sentence 26</td>\n",
       "      <td>19</td>\n",
       "      <td>['``', 'Secretary', 'Yellen', ',', 'release', ...</td>\n",
       "      <td>17</td>\n",
       "      <td>['on', 'the']</td>\n",
       "      <td>2</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sentence 27</td>\n",
       "      <td>3</td>\n",
       "      <td>['continued', '.']</td>\n",
       "      <td>2</td>\n",
       "      <td>['He']</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sentence 28</td>\n",
       "      <td>18</td>\n",
       "      <td>['``', '’', 'release', 'reports', ',', 'compli...</td>\n",
       "      <td>9</td>\n",
       "      <td>['If', 'she', 'doesn', 't', 'those', 'she', 'i...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sentence 29</td>\n",
       "      <td>52</td>\n",
       "      <td>['letter', 'dated', 'April', '19', ',', '2023'...</td>\n",
       "      <td>36</td>\n",
       "      <td>['In', 'a', 'of', 'that', 'a', 'of', 'his', 'h...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sentence 30</td>\n",
       "      <td>26</td>\n",
       "      <td>['client', 'allegedly', 'aware', 'facts', 'cas...</td>\n",
       "      <td>15</td>\n",
       "      <td>['And', 'that', 'the', 'is', 'of', 'of', 'the'...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Sentence 31</td>\n",
       "      <td>23</td>\n",
       "      <td>['CLICK', 'GET', 'FOX', 'NEWS', 'APP', 'Lytle'...</td>\n",
       "      <td>18</td>\n",
       "      <td>['HERE', 'TO', 'THE', 'for', 'the']</td>\n",
       "      <td>5</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Sentence 32</td>\n",
       "      <td>20</td>\n",
       "      <td>['Neither', 'Hunter', 'Biden', 'member', 'fami...</td>\n",
       "      <td>10</td>\n",
       "      <td>['nor', 'any', 'of', 'his', 'has', 'been', 'wi...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Sentence 33</td>\n",
       "      <td>13</td>\n",
       "      <td>['Fox', 'News', \"'\", 'Patrick', 'Ward', ',', '...</td>\n",
       "      <td>11</td>\n",
       "      <td>['to', 'this']</td>\n",
       "      <td>2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence  Total words   \n",
       "0    Sentence 1           38  \\\n",
       "1    Sentence 2           35   \n",
       "2    Sentence 3           26   \n",
       "3    Sentence 4           14   \n",
       "4    Sentence 5           41   \n",
       "5    Sentence 6           28   \n",
       "6    Sentence 7           37   \n",
       "7    Sentence 8           40   \n",
       "8    Sentence 9           19   \n",
       "9   Sentence 10           21   \n",
       "10  Sentence 11           27   \n",
       "11  Sentence 12           30   \n",
       "12  Sentence 13           10   \n",
       "13  Sentence 14           14   \n",
       "14  Sentence 15           17   \n",
       "15  Sentence 16           11   \n",
       "16  Sentence 17           29   \n",
       "17  Sentence 18           26   \n",
       "18  Sentence 19           21   \n",
       "19  Sentence 20           32   \n",
       "20  Sentence 21           25   \n",
       "21  Sentence 22           31   \n",
       "22  Sentence 23           32   \n",
       "23  Sentence 24           38   \n",
       "24  Sentence 25           14   \n",
       "25  Sentence 26           19   \n",
       "26  Sentence 27            3   \n",
       "27  Sentence 28           18   \n",
       "28  Sentence 29           52   \n",
       "29  Sentence 30           26   \n",
       "30  Sentence 31           23   \n",
       "31  Sentence 32           20   \n",
       "32  Sentence 33           13   \n",
       "\n",
       "                                       Filtered words   \n",
       "0   ['Republicans', 'respond', 'IRS', 'whistleblow...  \\\n",
       "1   ['Lawmakers', 'Capitol', 'Hill', 'calling', 'B...   \n",
       "2   ['congressional', 'outcries', 'come', 'whistle...   \n",
       "3   ['whistleblower', 'also', 'alleges', '``', 'cl...   \n",
       "4   ['``', '’', 'deeply', 'concerning', 'Biden', '...   \n",
       "5   ['Comer', ',', 'R-Ky.', ',', 'also', 'said', '...   \n",
       "6   ['HUNTER', 'BIDEN', 'INVESTIGATION', 'MISHANDL...   \n",
       "7   ['’', 'clear', 'investigation', 'Hunter', 'mem...   \n",
       "8   ['``', '’', 'wondering', 'along', 'heck', 'DOJ...   \n",
       "9   ['appears', 'Biden', 'Administration', 'may', ...   \n",
       "10  ['Comer', 'added', ',', '``', 'House', 'Oversi...   \n",
       "11  ['Oversight', 'Committee', 'also', 'continue',...   \n",
       "12  ['Americans', 'demand', 'answers', ',', 'trans...   \n",
       "13  ['President', 'Biden', '’', 'son', 'Hunter', '...   \n",
       "14  ['investigation', 'concerns', 'presence', 'sus...   \n",
       "15  ['led', 'Delaware', 'U.S.', 'Attorney', 'David...   \n",
       "16  ['Across', 'Congress', ',', 'Sen.', 'Ted', 'Cr...   \n",
       "17  ['``', '’', 'national', 'security', 'reason', ...   \n",
       "18  ['``', '’', 'reason', 'whatsoever', 'want', 'p...   \n",
       "19  ['SIX', 'ADDITIONAL', 'BIDEN', 'FAMILY', 'MEMB...   \n",
       "20  ['``', 'U.S.', 'Department', 'Treasury', 'need...   \n",
       "21  ['``', 'Janet', 'Yellen', ',', 'choice', ':', ...   \n",
       "22  ['According', 'Cruz', ',', 'American', 'public...   \n",
       "23  ['BIDEN', 'FAMILY', 'RECEIVED', '$', '1M', 'HU...   \n",
       "24  ['added', ',', '``', 'benign', ',', 'release',...   \n",
       "25  ['``', 'Secretary', 'Yellen', ',', 'release', ...   \n",
       "26                                 ['continued', '.']   \n",
       "27  ['``', '’', 'release', 'reports', ',', 'compli...   \n",
       "28  ['letter', 'dated', 'April', '19', ',', '2023'...   \n",
       "29  ['client', 'allegedly', 'aware', 'facts', 'cas...   \n",
       "30  ['CLICK', 'GET', 'FOX', 'NEWS', 'APP', 'Lytle'...   \n",
       "31  ['Neither', 'Hunter', 'Biden', 'member', 'fami...   \n",
       "32  ['Fox', 'News', \"'\", 'Patrick', 'Ward', ',', '...   \n",
       "\n",
       "    Number of filtered words   \n",
       "0                         23  \\\n",
       "1                         22   \n",
       "2                         16   \n",
       "3                         10   \n",
       "4                         30   \n",
       "5                         24   \n",
       "6                         27   \n",
       "7                         25   \n",
       "8                          8   \n",
       "9                         12   \n",
       "10                        18   \n",
       "11                        18   \n",
       "12                         9   \n",
       "13                        10   \n",
       "14                        14   \n",
       "15                         7   \n",
       "16                        23   \n",
       "17                        17   \n",
       "18                        10   \n",
       "19                        22   \n",
       "20                        19   \n",
       "21                        19   \n",
       "22                        17   \n",
       "23                        28   \n",
       "24                         9   \n",
       "25                        17   \n",
       "26                         2   \n",
       "27                         9   \n",
       "28                        36   \n",
       "29                        15   \n",
       "30                        18   \n",
       "31                        10   \n",
       "32                        11   \n",
       "\n",
       "                                           Stop words  Number of stop words   \n",
       "0   ['after', 'is', 'being', 'of', 'are', 'for', '...                    15  \\\n",
       "1   ['on', 'are', 'for', 'the', 'to', 'be', 'for',...                    13   \n",
       "2   ['The', 'as', 'a', 'the', 'an', 'into', 'is', ...                    10   \n",
       "3                          ['The', 'of', 'in', 'the']                     4   \n",
       "4   ['It', 's', 'that', 'the', 'be', 'by', 'to', '...                    11   \n",
       "5                       ['have', 'the', 'to', 'from']                     4   \n",
       "6   ['BEING', 'OF', 'The', 'on', 'and', 'has', 'be...                    10   \n",
       "7   ['It', 's', 'from', 'our', 'that', 'and', 'oth...                    15   \n",
       "8   ['We', 've', 'been', 'all', 'where', 'the', 't...                    11   \n",
       "9   ['Now', 'it', 'the', 'have', 'been', 'to', 'th...                     9   \n",
       "10  ['The', 'will', 'to', 'in', 'the', 'who', 'be'...                     9   \n",
       "11  ['The', 'will', 'to', 'our', 'into', 'the', 's...                    12   \n",
       "12                                            ['and']                     1   \n",
       "13                      ['s', 'has', 'been', 'under']                     4   \n",
       "14                               ['The', 'the', 'of']                     3   \n",
       "15                        ['It', 'is', 'being', 'by']                     4   \n",
       "16             ['on', 'for', 'to', 'the', 'the', 's']                     6   \n",
       "17  ['There', 's', 'no', 'to', 'them', 'during', '...                     9   \n",
       "18  ['There', 's', 'no', 'other', 'than', 'if', 'y...                    11   \n",
       "19  ['HAVE', 'FROM', 'do', 'not', 'or', 'but', 'be...                    10   \n",
       "20             ['The', 'of', 'to', 'on', 'the', 'on']                     6   \n",
       "21  ['you', 'have', 'a', 'You', 'are', 'up', 'of',...                    12   \n",
       "22  ['to', 'the', 'should', 'be', 'to', 'the', 'is...                    15   \n",
       "23  ['MORE', 'THAN', 'FROM', 'AFTER', 'The', 'the'...                    10   \n",
       "24                  ['He', 'If', 'this', 'is', 'the']                     5   \n",
       "25                                      ['on', 'the']                     2   \n",
       "26                                             ['He']                     1   \n",
       "27  ['If', 'she', 'doesn', 't', 'those', 'she', 'i...                     9   \n",
       "28  ['In', 'a', 'of', 'that', 'a', 'of', 'his', 'h...                    16   \n",
       "29  ['And', 'that', 'the', 'is', 'of', 'of', 'the'...                    11   \n",
       "30                ['HERE', 'TO', 'THE', 'for', 'the']                     5   \n",
       "31  ['nor', 'any', 'of', 'his', 'has', 'been', 'wi...                    10   \n",
       "32                                     ['to', 'this']                     2   \n",
       "\n",
       "    Average number of stop words per sentence  Sentiment score  \n",
       "0                                        0.39                0  \n",
       "1                                        0.37                0  \n",
       "2                                        0.38                1  \n",
       "3                                        0.29                0  \n",
       "4                                        0.27                1  \n",
       "5                                        0.14                1  \n",
       "6                                        0.27                1  \n",
       "7                                        0.38                1  \n",
       "8                                        0.58                1  \n",
       "9                                        0.43                1  \n",
       "10                                       0.33                0  \n",
       "11                                       0.40                1  \n",
       "12                                       0.10                1  \n",
       "13                                       0.29                1  \n",
       "14                                       0.18                1  \n",
       "15                                       0.36                1  \n",
       "16                                       0.21                1  \n",
       "17                                       0.35                1  \n",
       "18                                       0.52                1  \n",
       "19                                       0.31                1  \n",
       "20                                       0.24                0  \n",
       "21                                       0.39                1  \n",
       "22                                       0.47                1  \n",
       "23                                       0.26                1  \n",
       "24                                       0.36                0  \n",
       "25                                       0.11                0  \n",
       "26                                       0.33                0  \n",
       "27                                       0.50                1  \n",
       "28                                       0.31                1  \n",
       "29                                       0.42                1  \n",
       "30                                       0.22                1  \n",
       "31                                       0.50                1  \n",
       "32                                       0.15                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total number of words</th>\n",
       "      <th>Total number of stop words</th>\n",
       "      <th>Maximum number of stop words per sentence</th>\n",
       "      <th>Minimum number of stop words per sentence</th>\n",
       "      <th>Average number of stop words per article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>830</td>\n",
       "      <td>275</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total number of words  Total number of stop words   \n",
       "0                    830                         275  \\\n",
       "\n",
       "   Maximum number of stop words per sentence   \n",
       "0                                         16  \\\n",
       "\n",
       "   Minimum number of stop words per sentence   \n",
       "0                                          1  \\\n",
       "\n",
       "   Average number of stop words per article  \n",
       "0                                      0.33  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total adjectives</th>\n",
       "      <th>Average number of adjectives in the article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total adjectives  Average number of adjectives in the article\n",
       "0                61                                         0.07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stanza Average of sentiment score for all sentences</th>\n",
       "      <th>Stanza Maximum sentiment score</th>\n",
       "      <th>Stanza Minimum sentiment score</th>\n",
       "      <th>Stanza Standard deviation</th>\n",
       "      <th>Vader average scores</th>\n",
       "      <th>Vader maximum scores</th>\n",
       "      <th>Vader minimum scores</th>\n",
       "      <th>Vader standard deviation scores</th>\n",
       "      <th>MPQA average scores</th>\n",
       "      <th>MPQA maximum scores</th>\n",
       "      <th>MPQA minimum scores</th>\n",
       "      <th>MPQA standard deviation scores</th>\n",
       "      <th>Sentiwordnet score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.794872</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.409074</td>\n",
       "      <td>[0.07775758 0.86757576 0.05457576]</td>\n",
       "      <td>[0.294 1.    0.346]</td>\n",
       "      <td>[0.    0.494 0.   ]</td>\n",
       "      <td>[0.08812698 0.1209152  0.07819983]</td>\n",
       "      <td>-0.186441</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.853201</td>\n",
       "      <td>-0.002717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stanza Average of sentiment score for all sentences   \n",
       "0                                           0.794872    \\\n",
       "\n",
       "   Stanza Maximum sentiment score  Stanza Minimum sentiment score   \n",
       "0                               1                               0  \\\n",
       "\n",
       "   Stanza Standard deviation                Vader average scores   \n",
       "0                   0.409074  [0.07775758 0.86757576 0.05457576]  \\\n",
       "\n",
       "  Vader maximum scores Vader minimum scores   \n",
       "0  [0.294 1.    0.346]  [0.    0.494 0.   ]  \\\n",
       "\n",
       "      Vader standard deviation scores  MPQA average scores   \n",
       "0  [0.08812698 0.1209152  0.07819983]            -0.186441  \\\n",
       "\n",
       "   MPQA maximum scores  MPQA minimum scores  MPQA standard deviation scores   \n",
       "0                    1                   -1                        0.853201  \\\n",
       "\n",
       "   Sentiwordnet score  \n",
       "0           -0.002717  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flesch-Kincaid score</th>\n",
       "      <th>Estimated reading level of the article</th>\n",
       "      <th>Flesch-Reading score</th>\n",
       "      <th>The article is classified as</th>\n",
       "      <th>Dale-Chall Readability score</th>\n",
       "      <th>The estimated comprehension level for different grade levels</th>\n",
       "      <th>Automated Readability Index (ARI) score</th>\n",
       "      <th>It corresponds to a grade level of</th>\n",
       "      <th>This means that the text can be read by someone who is around</th>\n",
       "      <th>Coleman-Liau Index Score</th>\n",
       "      <th>Estimated Grade Level</th>\n",
       "      <th>Gunning Fog score</th>\n",
       "      <th>The estimated grade level for comprehension is</th>\n",
       "      <th>SPACHE score</th>\n",
       "      <th>This corresponds to a grade level of</th>\n",
       "      <th>Linsear Write Index score</th>\n",
       "      <th>Approximate grade level equivalent</th>\n",
       "      <th>Perplexity (how well the LDA model predicts the corpus) of the article</th>\n",
       "      <th>Coherence (how coherent the topics are) of the article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.151031</td>\n",
       "      <td>14</td>\n",
       "      <td>33.182285</td>\n",
       "      <td>difficult</td>\n",
       "      <td>11.398428</td>\n",
       "      <td>['college_graduate']</td>\n",
       "      <td>13.763572</td>\n",
       "      <td>['college_graduate']</td>\n",
       "      <td>[24, 100]</td>\n",
       "      <td>12.406126</td>\n",
       "      <td>12</td>\n",
       "      <td>15.229192</td>\n",
       "      <td>college</td>\n",
       "      <td>8.425877</td>\n",
       "      <td>8</td>\n",
       "      <td>16.636364</td>\n",
       "      <td>17</td>\n",
       "      <td>-4.498551</td>\n",
       "      <td>0.580393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Flesch-Kincaid score  Estimated reading level of the article   \n",
       "0             14.151031                                      14  \\\n",
       "\n",
       "   Flesch-Reading score The article is classified as   \n",
       "0             33.182285                    difficult  \\\n",
       "\n",
       "   Dale-Chall Readability score   \n",
       "0                     11.398428  \\\n",
       "\n",
       "  The estimated comprehension level for different grade levels   \n",
       "0                               ['college_graduate']            \\\n",
       "\n",
       "   Automated Readability Index (ARI) score It corresponds to a grade level of   \n",
       "0                                13.763572               ['college_graduate']  \\\n",
       "\n",
       "  This means that the text can be read by someone who is around   \n",
       "0                                          [24, 100]             \\\n",
       "\n",
       "   Coleman-Liau Index Score  Estimated Grade Level  Gunning Fog score   \n",
       "0                 12.406126                     12          15.229192  \\\n",
       "\n",
       "  The estimated grade level for comprehension is  SPACHE score   \n",
       "0                                        college      8.425877  \\\n",
       "\n",
       "   This corresponds to a grade level of  Linsear Write Index score   \n",
       "0                                     8                  16.636364  \\\n",
       "\n",
       "   Approximate grade level equivalent   \n",
       "0                                  17  \\\n",
       "\n",
       "   Perplexity (how well the LDA model predicts the corpus) of the article   \n",
       "0                                          -4.498551                       \\\n",
       "\n",
       "   Coherence (how coherent the topics are) of the article  \n",
       "0                                           0.580393       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max tree depth</th>\n",
       "      <th>Words at max depth</th>\n",
       "      <th>,</th>\n",
       "      <th>Average tree length</th>\n",
       "      <th>Maximum tree length</th>\n",
       "      <th>Minimum tree length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>calling, be</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.575758</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Max tree depth Words at max depth  ,   Average tree length   \n",
       "0              10        calling, be NaN             5.575758  \\\n",
       "\n",
       "   Maximum tree length  Minimum tree length  \n",
       "0                   12                    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Total words</th>\n",
       "      <th>Filtered words</th>\n",
       "      <th>Number of filtered words</th>\n",
       "      <th>Stop words</th>\n",
       "      <th>Number of stop words</th>\n",
       "      <th>Average number of stop words per sentence</th>\n",
       "      <th>Sentiment score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence 1</td>\n",
       "      <td>13</td>\n",
       "      <td>['Alabama', 'education', 'director', 'ousted',...</td>\n",
       "      <td>11</td>\n",
       "      <td>['over', 'on']</td>\n",
       "      <td>2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence 2</td>\n",
       "      <td>50</td>\n",
       "      <td>['Kay', 'Ivey', 'Friday', 'announced', 'replac...</td>\n",
       "      <td>33</td>\n",
       "      <td>['on', 'she', 'her', 'of', 'over', 'the', 'of'...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence 3</td>\n",
       "      <td>30</td>\n",
       "      <td>['Barbara', 'Cooper', 'forced', 'head', 'Alaba...</td>\n",
       "      <td>17</td>\n",
       "      <td>['was', 'out', 'as', 'as', 'of', 'the', 'of', ...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence 4</td>\n",
       "      <td>28</td>\n",
       "      <td>['Ivey', 'spokesperson', 'Gina', 'Maiola', 'id...</td>\n",
       "      <td>22</td>\n",
       "      <td>['the', 'as', 'the', 'for', 'the', 'of']</td>\n",
       "      <td>6</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence 5</td>\n",
       "      <td>10</td>\n",
       "      <td>['MONTGOMERY', ',', 'Ala.', '(', 'AP', ')', '—...</td>\n",
       "      <td>10</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence 6</td>\n",
       "      <td>50</td>\n",
       "      <td>['Kay', 'Ivey', 'Friday', 'announced', 'replac...</td>\n",
       "      <td>33</td>\n",
       "      <td>['on', 'she', 'her', 'of', 'over', 'the', 'of'...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence 7</td>\n",
       "      <td>30</td>\n",
       "      <td>['Barbara', 'Cooper', 'forced', 'head', 'Alaba...</td>\n",
       "      <td>17</td>\n",
       "      <td>['was', 'out', 'as', 'as', 'of', 'the', 'of', ...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence 8</td>\n",
       "      <td>28</td>\n",
       "      <td>['Ivey', 'spokesperson', 'Gina', 'Maiola', 'id...</td>\n",
       "      <td>22</td>\n",
       "      <td>['the', 'as', 'the', 'for', 'the', 'of']</td>\n",
       "      <td>6</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence 9</td>\n",
       "      <td>15</td>\n",
       "      <td>['Maiola', 'said', 'understands', 'books', 're...</td>\n",
       "      <td>8</td>\n",
       "      <td>['she', 'that', 'the', 'have', 'been', 'from',...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence 10</td>\n",
       "      <td>30</td>\n",
       "      <td>['“', 'education', 'Alabama', '’', 'children',...</td>\n",
       "      <td>16</td>\n",
       "      <td>['The', 'of', 's', 'is', 'my', 'as', 'and', 't...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sentence 11</td>\n",
       "      <td>49</td>\n",
       "      <td>['Let', 'crystal', 'clear', ':', 'Woke', 'conc...</td>\n",
       "      <td>27</td>\n",
       "      <td>['me', 'be', 'that', 'have', 'to', 'do', 'with...</td>\n",
       "      <td>22</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sentence 12</td>\n",
       "      <td>36</td>\n",
       "      <td>['Ivey', \"'s\", 'statement', 'comes', 'conserva...</td>\n",
       "      <td>26</td>\n",
       "      <td>['as', 'have', 'a', 'out', 'of', 'with', 'as',...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentence 13</td>\n",
       "      <td>50</td>\n",
       "      <td>['governor', '’', 'office', 'said', 'Ivey', 'f...</td>\n",
       "      <td>31</td>\n",
       "      <td>['The', 's', 'to', 'a', 'to', 'this', 'and', '...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sentence 14</td>\n",
       "      <td>9</td>\n",
       "      <td>['Cooper', 'could', 'immediately', 'reached', ...</td>\n",
       "      <td>6</td>\n",
       "      <td>['not', 'be', 'for']</td>\n",
       "      <td>3</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentence 15</td>\n",
       "      <td>10</td>\n",
       "      <td>['book', 'guide', 'early', 'childhood', 'educa...</td>\n",
       "      <td>6</td>\n",
       "      <td>['The', 'is', 'a', 'for']</td>\n",
       "      <td>4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sentence 16</td>\n",
       "      <td>9</td>\n",
       "      <td>['curriculum', 'taught', 'children', '.']</td>\n",
       "      <td>4</td>\n",
       "      <td>['It', 'is', 'not', 'a', 'to']</td>\n",
       "      <td>5</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sentence 17</td>\n",
       "      <td>49</td>\n",
       "      <td>['governor', '’', 'office', ',', 'press', 'rel...</td>\n",
       "      <td>34</td>\n",
       "      <td>['The', 's', 'in', 'a', 'from', 'the', 'and', ...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sentence 18</td>\n",
       "      <td>29</td>\n",
       "      <td>['sections', ',', 'according', 'copy', '881-pa...</td>\n",
       "      <td>19</td>\n",
       "      <td>['Those', 'to', 'a', 'of', 'the', 'by', 'The',...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sentence 19</td>\n",
       "      <td>16</td>\n",
       "      <td>['Story', 'continues', '“', 'Early', 'childhoo...</td>\n",
       "      <td>14</td>\n",
       "      <td>['and', 'that']</td>\n",
       "      <td>2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sentence 20</td>\n",
       "      <td>36</td>\n",
       "      <td>['Children', 'families', '(', 'e.g.', ',', 'si...</td>\n",
       "      <td>29</td>\n",
       "      <td>['from', 'all', 'to', 'and', 'that', 'and', 't...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sentence 21</td>\n",
       "      <td>37</td>\n",
       "      <td>['section', 'structural', 'racism', 'states', ...</td>\n",
       "      <td>25</td>\n",
       "      <td>['The', 'on', 'that', 'and', 'has', 'and', 'th...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sentence 22</td>\n",
       "      <td>45</td>\n",
       "      <td>['early', 'education', 'system', 'immune', 'fo...</td>\n",
       "      <td>24</td>\n",
       "      <td>['The', 'is', 'not', 'to', 'these', 'It', 'is'...</td>\n",
       "      <td>21</td>\n",
       "      <td>0.47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sentence 23</td>\n",
       "      <td>19</td>\n",
       "      <td>['NAEYC', 'national', 'accrediting', 'board', ...</td>\n",
       "      <td>13</td>\n",
       "      <td>['is', 'a', 'that', 'to', 'and', 'for']</td>\n",
       "      <td>6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sentence 24</td>\n",
       "      <td>28</td>\n",
       "      <td>['emailed', 'response', 'Associated', 'Press',...</td>\n",
       "      <td>16</td>\n",
       "      <td>['In', 'an', 'to', 'The', 'the', 'did', 'not',...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence 25</td>\n",
       "      <td>37</td>\n",
       "      <td>['“', 'nearly', 'four', 'decades', ',', 'partn...</td>\n",
       "      <td>24</td>\n",
       "      <td>['For', 'and', 'in', 'with', 'of', 'of', 'and'...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sentence 26</td>\n",
       "      <td>43</td>\n",
       "      <td>['curriculum', ',', 'responsive', ',', 'educat...</td>\n",
       "      <td>26</td>\n",
       "      <td>['While', 'not', 'a', 'it', 'is', 'a', 'and', ...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sentence 27</td>\n",
       "      <td>9</td>\n",
       "      <td>['Cooper', 'member', 'NAEYC', 'board', '.']</td>\n",
       "      <td>5</td>\n",
       "      <td>['is', 'a', 'of', 'the']</td>\n",
       "      <td>4</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sentence 28</td>\n",
       "      <td>61</td>\n",
       "      <td>['previously', 'published', 'statement', 'orga...</td>\n",
       "      <td>43</td>\n",
       "      <td>['In', 'a', 'on', 'the', 'about', 'the', 'of',...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sentence 29</td>\n",
       "      <td>15</td>\n",
       "      <td>['program', 'high', 'ratings', 'National', 'In...</td>\n",
       "      <td>9</td>\n",
       "      <td>['The', 'has', 'won', 'from', 'the', 'for']</td>\n",
       "      <td>6</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence  Total words   \n",
       "0    Sentence 1           13  \\\n",
       "1    Sentence 2           50   \n",
       "2    Sentence 3           30   \n",
       "3    Sentence 4           28   \n",
       "4    Sentence 5           10   \n",
       "5    Sentence 6           50   \n",
       "6    Sentence 7           30   \n",
       "7    Sentence 8           28   \n",
       "8    Sentence 9           15   \n",
       "9   Sentence 10           30   \n",
       "10  Sentence 11           49   \n",
       "11  Sentence 12           36   \n",
       "12  Sentence 13           50   \n",
       "13  Sentence 14            9   \n",
       "14  Sentence 15           10   \n",
       "15  Sentence 16            9   \n",
       "16  Sentence 17           49   \n",
       "17  Sentence 18           29   \n",
       "18  Sentence 19           16   \n",
       "19  Sentence 20           36   \n",
       "20  Sentence 21           37   \n",
       "21  Sentence 22           45   \n",
       "22  Sentence 23           19   \n",
       "23  Sentence 24           28   \n",
       "24  Sentence 25           37   \n",
       "25  Sentence 26           43   \n",
       "26  Sentence 27            9   \n",
       "27  Sentence 28           61   \n",
       "28  Sentence 29           15   \n",
       "\n",
       "                                       Filtered words   \n",
       "0   ['Alabama', 'education', 'director', 'ousted',...  \\\n",
       "1   ['Kay', 'Ivey', 'Friday', 'announced', 'replac...   \n",
       "2   ['Barbara', 'Cooper', 'forced', 'head', 'Alaba...   \n",
       "3   ['Ivey', 'spokesperson', 'Gina', 'Maiola', 'id...   \n",
       "4   ['MONTGOMERY', ',', 'Ala.', '(', 'AP', ')', '—...   \n",
       "5   ['Kay', 'Ivey', 'Friday', 'announced', 'replac...   \n",
       "6   ['Barbara', 'Cooper', 'forced', 'head', 'Alaba...   \n",
       "7   ['Ivey', 'spokesperson', 'Gina', 'Maiola', 'id...   \n",
       "8   ['Maiola', 'said', 'understands', 'books', 're...   \n",
       "9   ['“', 'education', 'Alabama', '’', 'children',...   \n",
       "10  ['Let', 'crystal', 'clear', ':', 'Woke', 'conc...   \n",
       "11  ['Ivey', \"'s\", 'statement', 'comes', 'conserva...   \n",
       "12  ['governor', '’', 'office', 'said', 'Ivey', 'f...   \n",
       "13  ['Cooper', 'could', 'immediately', 'reached', ...   \n",
       "14  ['book', 'guide', 'early', 'childhood', 'educa...   \n",
       "15          ['curriculum', 'taught', 'children', '.']   \n",
       "16  ['governor', '’', 'office', ',', 'press', 'rel...   \n",
       "17  ['sections', ',', 'according', 'copy', '881-pa...   \n",
       "18  ['Story', 'continues', '“', 'Early', 'childhoo...   \n",
       "19  ['Children', 'families', '(', 'e.g.', ',', 'si...   \n",
       "20  ['section', 'structural', 'racism', 'states', ...   \n",
       "21  ['early', 'education', 'system', 'immune', 'fo...   \n",
       "22  ['NAEYC', 'national', 'accrediting', 'board', ...   \n",
       "23  ['emailed', 'response', 'Associated', 'Press',...   \n",
       "24  ['“', 'nearly', 'four', 'decades', ',', 'partn...   \n",
       "25  ['curriculum', ',', 'responsive', ',', 'educat...   \n",
       "26        ['Cooper', 'member', 'NAEYC', 'board', '.']   \n",
       "27  ['previously', 'published', 'statement', 'orga...   \n",
       "28  ['program', 'high', 'ratings', 'National', 'In...   \n",
       "\n",
       "    Number of filtered words   \n",
       "0                         11  \\\n",
       "1                         33   \n",
       "2                         17   \n",
       "3                         22   \n",
       "4                         10   \n",
       "5                         33   \n",
       "6                         17   \n",
       "7                         22   \n",
       "8                          8   \n",
       "9                         16   \n",
       "10                        27   \n",
       "11                        26   \n",
       "12                        31   \n",
       "13                         6   \n",
       "14                         6   \n",
       "15                         4   \n",
       "16                        34   \n",
       "17                        19   \n",
       "18                        14   \n",
       "19                        29   \n",
       "20                        25   \n",
       "21                        24   \n",
       "22                        13   \n",
       "23                        16   \n",
       "24                        24   \n",
       "25                        26   \n",
       "26                         5   \n",
       "27                        43   \n",
       "28                         9   \n",
       "\n",
       "                                           Stop words  Number of stop words   \n",
       "0                                      ['over', 'on']                     2  \\\n",
       "1   ['on', 'she', 'her', 'of', 'over', 'the', 'of'...                    17   \n",
       "2   ['was', 'out', 'as', 'as', 'of', 'the', 'of', ...                    13   \n",
       "3            ['the', 'as', 'the', 'for', 'the', 'of']                     6   \n",
       "4                                                  []                     0   \n",
       "5   ['on', 'she', 'her', 'of', 'over', 'the', 'of'...                    17   \n",
       "6   ['was', 'out', 'as', 'as', 'of', 'the', 'of', ...                    13   \n",
       "7            ['the', 'as', 'the', 'for', 'the', 'of']                     6   \n",
       "8   ['she', 'that', 'the', 'have', 'been', 'from',...                     7   \n",
       "9   ['The', 'of', 's', 'is', 'my', 'as', 'and', 't...                    14   \n",
       "10  ['me', 'be', 'that', 'have', 'to', 'do', 'with...                    22   \n",
       "11  ['as', 'have', 'a', 'out', 'of', 'with', 'as',...                    10   \n",
       "12  ['The', 's', 'to', 'a', 'to', 'this', 'and', '...                    19   \n",
       "13                               ['not', 'be', 'for']                     3   \n",
       "14                          ['The', 'is', 'a', 'for']                     4   \n",
       "15                     ['It', 'is', 'not', 'a', 'to']                     5   \n",
       "16  ['The', 's', 'in', 'a', 'from', 'the', 'and', ...                    15   \n",
       "17  ['Those', 'to', 'a', 'of', 'the', 'by', 'The',...                    10   \n",
       "18                                    ['and', 'that']                     2   \n",
       "19  ['from', 'all', 'to', 'and', 'that', 'and', 't...                     7   \n",
       "20  ['The', 'on', 'that', 'and', 'has', 'and', 'th...                    12   \n",
       "21  ['The', 'is', 'not', 'to', 'these', 'It', 'is'...                    21   \n",
       "22            ['is', 'a', 'that', 'to', 'and', 'for']                     6   \n",
       "23  ['In', 'an', 'to', 'The', 'the', 'did', 'not',...                    12   \n",
       "24  ['For', 'and', 'in', 'with', 'of', 'of', 'and'...                    13   \n",
       "25  ['While', 'not', 'a', 'it', 'is', 'a', 'and', ...                    17   \n",
       "26                           ['is', 'a', 'of', 'the']                     4   \n",
       "27  ['In', 'a', 'on', 'the', 'about', 'the', 'of',...                    18   \n",
       "28        ['The', 'has', 'won', 'from', 'the', 'for']                     6   \n",
       "\n",
       "    Average number of stop words per sentence  Sentiment score  \n",
       "0                                        0.15                0  \n",
       "1                                        0.34                1  \n",
       "2                                        0.43                1  \n",
       "3                                        0.21                1  \n",
       "4                                        0.00                2  \n",
       "5                                        0.34                1  \n",
       "6                                        0.43                0  \n",
       "7                                        0.21                1  \n",
       "8                                        0.47                1  \n",
       "9                                        0.47                1  \n",
       "10                                       0.45                1  \n",
       "11                                       0.28                1  \n",
       "12                                       0.38                1  \n",
       "13                                       0.33                1  \n",
       "14                                       0.40                1  \n",
       "15                                       0.56                1  \n",
       "16                                       0.31                1  \n",
       "17                                       0.34                1  \n",
       "18                                       0.12                1  \n",
       "19                                       0.19                1  \n",
       "20                                       0.32                1  \n",
       "21                                       0.47                2  \n",
       "22                                       0.32                1  \n",
       "23                                       0.43                2  \n",
       "24                                       0.35                2  \n",
       "25                                       0.40                1  \n",
       "26                                       0.44                1  \n",
       "27                                       0.30                1  \n",
       "28                                       0.40                2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total number of words</th>\n",
       "      <th>Total number of stop words</th>\n",
       "      <th>Maximum number of stop words per sentence</th>\n",
       "      <th>Minimum number of stop words per sentence</th>\n",
       "      <th>Average number of stop words per article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>871</td>\n",
       "      <td>301</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total number of words  Total number of stop words   \n",
       "0                    871                         301  \\\n",
       "\n",
       "   Maximum number of stop words per sentence   \n",
       "0                                         22  \\\n",
       "\n",
       "   Minimum number of stop words per sentence   \n",
       "0                                          0  \\\n",
       "\n",
       "   Average number of stop words per article  \n",
       "0                                      0.34  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total adjectives</th>\n",
       "      <th>Average number of adjectives in the article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total adjectives  Average number of adjectives in the article\n",
       "0                59                                         0.07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stanza Average of sentiment score for all sentences</th>\n",
       "      <th>Stanza Maximum sentiment score</th>\n",
       "      <th>Stanza Minimum sentiment score</th>\n",
       "      <th>Stanza Standard deviation</th>\n",
       "      <th>Vader average scores</th>\n",
       "      <th>Vader maximum scores</th>\n",
       "      <th>Vader minimum scores</th>\n",
       "      <th>Vader standard deviation scores</th>\n",
       "      <th>MPQA average scores</th>\n",
       "      <th>MPQA maximum scores</th>\n",
       "      <th>MPQA minimum scores</th>\n",
       "      <th>MPQA standard deviation scores</th>\n",
       "      <th>Sentiwordnet score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.103448</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.488791</td>\n",
       "      <td>[0.04834483 0.90672414 0.045     ]</td>\n",
       "      <td>[0.314 1.    0.222]</td>\n",
       "      <td>[0.    0.686 0.   ]</td>\n",
       "      <td>[0.07102126 0.09209886 0.0765709 ]</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.839771</td>\n",
       "      <td>0.007337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stanza Average of sentiment score for all sentences   \n",
       "0                                           1.103448    \\\n",
       "\n",
       "   Stanza Maximum sentiment score  Stanza Minimum sentiment score   \n",
       "0                               2                               0  \\\n",
       "\n",
       "   Stanza Standard deviation                Vader average scores   \n",
       "0                   0.488791  [0.04834483 0.90672414 0.045     ]  \\\n",
       "\n",
       "  Vader maximum scores Vader minimum scores   \n",
       "0  [0.314 1.    0.222]  [0.    0.686 0.   ]  \\\n",
       "\n",
       "      Vader standard deviation scores  MPQA average scores   \n",
       "0  [0.07102126 0.09209886 0.0765709 ]             0.095238  \\\n",
       "\n",
       "   MPQA maximum scores  MPQA minimum scores  MPQA standard deviation scores   \n",
       "0                    1                   -1                        0.839771  \\\n",
       "\n",
       "   Sentiwordnet score  \n",
       "0            0.007337  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flesch-Kincaid score</th>\n",
       "      <th>Estimated reading level of the article</th>\n",
       "      <th>Flesch-Reading score</th>\n",
       "      <th>The article is classified as</th>\n",
       "      <th>Dale-Chall Readability score</th>\n",
       "      <th>The estimated comprehension level for different grade levels</th>\n",
       "      <th>Automated Readability Index (ARI) score</th>\n",
       "      <th>It corresponds to a grade level of</th>\n",
       "      <th>This means that the text can be read by someone who is around</th>\n",
       "      <th>Coleman-Liau Index Score</th>\n",
       "      <th>Estimated Grade Level</th>\n",
       "      <th>Gunning Fog score</th>\n",
       "      <th>The estimated grade level for comprehension is</th>\n",
       "      <th>SPACHE score</th>\n",
       "      <th>This corresponds to a grade level of</th>\n",
       "      <th>Linsear Write Index score</th>\n",
       "      <th>Approximate grade level equivalent</th>\n",
       "      <th>Perplexity (how well the LDA model predicts the corpus) of the article</th>\n",
       "      <th>Coherence (how coherent the topics are) of the article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.966121</td>\n",
       "      <td>16</td>\n",
       "      <td>27.572873</td>\n",
       "      <td>very_confusing</td>\n",
       "      <td>11.101677</td>\n",
       "      <td>['college_graduate']</td>\n",
       "      <td>17.013763</td>\n",
       "      <td>['college_graduate']</td>\n",
       "      <td>[24, 100]</td>\n",
       "      <td>13.961606</td>\n",
       "      <td>14</td>\n",
       "      <td>17.216441</td>\n",
       "      <td>college_graduate</td>\n",
       "      <td>8.544684</td>\n",
       "      <td>9</td>\n",
       "      <td>19.775862</td>\n",
       "      <td>20</td>\n",
       "      <td>-4.354084</td>\n",
       "      <td>0.506633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Flesch-Kincaid score  Estimated reading level of the article   \n",
       "0             15.966121                                      16  \\\n",
       "\n",
       "   Flesch-Reading score The article is classified as   \n",
       "0             27.572873               very_confusing  \\\n",
       "\n",
       "   Dale-Chall Readability score   \n",
       "0                     11.101677  \\\n",
       "\n",
       "  The estimated comprehension level for different grade levels   \n",
       "0                               ['college_graduate']            \\\n",
       "\n",
       "   Automated Readability Index (ARI) score It corresponds to a grade level of   \n",
       "0                                17.013763               ['college_graduate']  \\\n",
       "\n",
       "  This means that the text can be read by someone who is around   \n",
       "0                                          [24, 100]             \\\n",
       "\n",
       "   Coleman-Liau Index Score  Estimated Grade Level  Gunning Fog score   \n",
       "0                 13.961606                     14          17.216441  \\\n",
       "\n",
       "  The estimated grade level for comprehension is  SPACHE score   \n",
       "0                               college_graduate      8.544684  \\\n",
       "\n",
       "   This corresponds to a grade level of  Linsear Write Index score   \n",
       "0                                     9                  19.775862  \\\n",
       "\n",
       "   Approximate grade level equivalent   \n",
       "0                                  20  \\\n",
       "\n",
       "   Perplexity (how well the LDA model predicts the corpus) of the article   \n",
       "0                                          -4.354084                       \\\n",
       "\n",
       "   Coherence (how coherent the topics are) of the article  \n",
       "0                                           0.506633       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max tree depth</th>\n",
       "      <th>Words at max depth</th>\n",
       "      <th>,</th>\n",
       "      <th>Average tree length</th>\n",
       "      <th>Maximum tree length</th>\n",
       "      <th>Minimum tree length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>ousted, comes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.296296</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Max tree depth Words at max depth  ,   Average tree length   \n",
       "0              13      ousted, comes NaN             7.296296  \\\n",
       "\n",
       "   Maximum tree length  Minimum tree length  \n",
       "0                   13                    2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Total words</th>\n",
       "      <th>Filtered words</th>\n",
       "      <th>Number of filtered words</th>\n",
       "      <th>Stop words</th>\n",
       "      <th>Number of stop words</th>\n",
       "      <th>Average number of stop words per sentence</th>\n",
       "      <th>Sentiment score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence 1</td>\n",
       "      <td>39</td>\n",
       "      <td>['Samantha', 'Cameron', ':', '‘', 'remind', 'D...</td>\n",
       "      <td>27</td>\n",
       "      <td>['I', 'have', 'to', 'to', 'of', 'the', 'up', '...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence 2</td>\n",
       "      <td>21</td>\n",
       "      <td>['first', 'activity', 'morning', 'used', 'Asht...</td>\n",
       "      <td>11</td>\n",
       "      <td>['Her', 'of', 'the', 'to', 'be', 'but', 'these...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence 3</td>\n",
       "      <td>87</td>\n",
       "      <td>['“', 'know', 'lot', 'milk', 'yields', ',', '”...</td>\n",
       "      <td>57</td>\n",
       "      <td>['I', 'a', 'about', 'she', 'down', 'of', 'on',...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence 4</td>\n",
       "      <td>21</td>\n",
       "      <td>['first', 'activity', 'morning', 'used', 'Asht...</td>\n",
       "      <td>11</td>\n",
       "      <td>['Her', 'of', 'the', 'to', 'be', 'but', 'these...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence 5</td>\n",
       "      <td>68</td>\n",
       "      <td>['“', 'know', 'lot', 'milk', 'yields', ',', '”...</td>\n",
       "      <td>41</td>\n",
       "      <td>['I', 'a', 'about', 'she', 'down', 'of', 'on',...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Sentence 136</td>\n",
       "      <td>8</td>\n",
       "      <td>['Building', 'fashion', 'brands', 'long', 'gam...</td>\n",
       "      <td>6</td>\n",
       "      <td>['is', 'a']</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Sentence 137</td>\n",
       "      <td>59</td>\n",
       "      <td>['“', 'spend', 'lot', 'time', 'developing', 'd...</td>\n",
       "      <td>34</td>\n",
       "      <td>['You', 'have', 'to', 'a', 'of', 'your', 'and'...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Sentence 138</td>\n",
       "      <td>23</td>\n",
       "      <td>['“', 'best', 'advice', 'anyone', 'ever', 'gav...</td>\n",
       "      <td>16</td>\n",
       "      <td>['The', 'me', 'is', 'to', 'down', 'into', 'she']</td>\n",
       "      <td>7</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Sentence 139</td>\n",
       "      <td>9</td>\n",
       "      <td>['“', 'Thinking', 'far', 'ahead', 'intimidatin...</td>\n",
       "      <td>6</td>\n",
       "      <td>['too', 'can', 'be']</td>\n",
       "      <td>3</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Sentence 140</td>\n",
       "      <td>24</td>\n",
       "      <td>['always', 'find', 'five', 'reasons', 'somethi...</td>\n",
       "      <td>12</td>\n",
       "      <td>['You', 'can', 'not', 'to', 'do', 'but', 'you'...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentence  Total words   \n",
       "0      Sentence 1           39  \\\n",
       "1      Sentence 2           21   \n",
       "2      Sentence 3           87   \n",
       "3      Sentence 4           21   \n",
       "4      Sentence 5           68   \n",
       "..            ...          ...   \n",
       "135  Sentence 136            8   \n",
       "136  Sentence 137           59   \n",
       "137  Sentence 138           23   \n",
       "138  Sentence 139            9   \n",
       "139  Sentence 140           24   \n",
       "\n",
       "                                        Filtered words   \n",
       "0    ['Samantha', 'Cameron', ':', '‘', 'remind', 'D...  \\\n",
       "1    ['first', 'activity', 'morning', 'used', 'Asht...   \n",
       "2    ['“', 'know', 'lot', 'milk', 'yields', ',', '”...   \n",
       "3    ['first', 'activity', 'morning', 'used', 'Asht...   \n",
       "4    ['“', 'know', 'lot', 'milk', 'yields', ',', '”...   \n",
       "..                                                 ...   \n",
       "135  ['Building', 'fashion', 'brands', 'long', 'gam...   \n",
       "136  ['“', 'spend', 'lot', 'time', 'developing', 'd...   \n",
       "137  ['“', 'best', 'advice', 'anyone', 'ever', 'gav...   \n",
       "138  ['“', 'Thinking', 'far', 'ahead', 'intimidatin...   \n",
       "139  ['always', 'find', 'five', 'reasons', 'somethi...   \n",
       "\n",
       "     Number of filtered words   \n",
       "0                          27  \\\n",
       "1                          11   \n",
       "2                          57   \n",
       "3                          11   \n",
       "4                          41   \n",
       "..                        ...   \n",
       "135                         6   \n",
       "136                        34   \n",
       "137                        16   \n",
       "138                         6   \n",
       "139                        12   \n",
       "\n",
       "                                            Stop words  Number of stop words   \n",
       "0    ['I', 'have', 'to', 'to', 'of', 'the', 'up', '...                    12  \\\n",
       "1    ['Her', 'of', 'the', 'to', 'be', 'but', 'these...                    10   \n",
       "2    ['I', 'a', 'about', 'she', 'down', 'of', 'on',...                    30   \n",
       "3    ['Her', 'of', 'the', 'to', 'be', 'but', 'these...                    10   \n",
       "4    ['I', 'a', 'about', 'she', 'down', 'of', 'on',...                    27   \n",
       "..                                                 ...                   ...   \n",
       "135                                        ['is', 'a']                     2   \n",
       "136  ['You', 'have', 'to', 'a', 'of', 'your', 'and'...                    25   \n",
       "137   ['The', 'me', 'is', 'to', 'down', 'into', 'she']                     7   \n",
       "138                               ['too', 'can', 'be']                     3   \n",
       "139  ['You', 'can', 'not', 'to', 'do', 'but', 'you'...                    12   \n",
       "\n",
       "     Average number of stop words per sentence  Sentiment score  \n",
       "0                                         0.31                0  \n",
       "1                                         0.48                0  \n",
       "2                                         0.34                0  \n",
       "3                                         0.48                1  \n",
       "4                                         0.40                0  \n",
       "..                                         ...              ...  \n",
       "135                                       0.25                0  \n",
       "136                                       0.42                1  \n",
       "137                                       0.30                0  \n",
       "138                                       0.33                2  \n",
       "139                                       0.50                2  \n",
       "\n",
       "[140 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total number of words</th>\n",
       "      <th>Total number of stop words</th>\n",
       "      <th>Maximum number of stop words per sentence</th>\n",
       "      <th>Minimum number of stop words per sentence</th>\n",
       "      <th>Average number of stop words per article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3722</td>\n",
       "      <td>1470</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total number of words  Total number of stop words   \n",
       "0                   3722                        1470  \\\n",
       "\n",
       "   Maximum number of stop words per sentence   \n",
       "0                                         36  \\\n",
       "\n",
       "   Minimum number of stop words per sentence   \n",
       "0                                          0  \\\n",
       "\n",
       "   Average number of stop words per article  \n",
       "0                                      0.39  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total adjectives</th>\n",
       "      <th>Average number of adjectives in the article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>315</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total adjectives  Average number of adjectives in the article\n",
       "0               315                                         0.08"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stanza Average of sentiment score for all sentences</th>\n",
       "      <th>Stanza Maximum sentiment score</th>\n",
       "      <th>Stanza Minimum sentiment score</th>\n",
       "      <th>Stanza Standard deviation</th>\n",
       "      <th>Vader average scores</th>\n",
       "      <th>Vader maximum scores</th>\n",
       "      <th>Vader minimum scores</th>\n",
       "      <th>Vader standard deviation scores</th>\n",
       "      <th>MPQA average scores</th>\n",
       "      <th>MPQA maximum scores</th>\n",
       "      <th>MPQA minimum scores</th>\n",
       "      <th>MPQA standard deviation scores</th>\n",
       "      <th>Sentiwordnet score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.815476</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.644088</td>\n",
       "      <td>[0.04357857 0.85687857 0.09958571]</td>\n",
       "      <td>[0.549 1.    0.75 ]</td>\n",
       "      <td>[0.   0.25 0.  ]</td>\n",
       "      <td>[0.09920355 0.14929652 0.12940926]</td>\n",
       "      <td>0.183746</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.811675</td>\n",
       "      <td>0.012795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stanza Average of sentiment score for all sentences   \n",
       "0                                           0.815476    \\\n",
       "\n",
       "   Stanza Maximum sentiment score  Stanza Minimum sentiment score   \n",
       "0                               2                               0  \\\n",
       "\n",
       "   Stanza Standard deviation                Vader average scores   \n",
       "0                   0.644088  [0.04357857 0.85687857 0.09958571]  \\\n",
       "\n",
       "  Vader maximum scores Vader minimum scores   \n",
       "0  [0.549 1.    0.75 ]     [0.   0.25 0.  ]  \\\n",
       "\n",
       "      Vader standard deviation scores  MPQA average scores   \n",
       "0  [0.09920355 0.14929652 0.12940926]             0.183746  \\\n",
       "\n",
       "   MPQA maximum scores  MPQA minimum scores  MPQA standard deviation scores   \n",
       "0                    1                   -1                        0.811675  \\\n",
       "\n",
       "   Sentiwordnet score  \n",
       "0            0.012795  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flesch-Kincaid score</th>\n",
       "      <th>Estimated reading level of the article</th>\n",
       "      <th>Flesch-Reading score</th>\n",
       "      <th>The article is classified as</th>\n",
       "      <th>Dale-Chall Readability score</th>\n",
       "      <th>The estimated comprehension level for different grade levels</th>\n",
       "      <th>Automated Readability Index (ARI) score</th>\n",
       "      <th>It corresponds to a grade level of</th>\n",
       "      <th>This means that the text can be read by someone who is around</th>\n",
       "      <th>Coleman-Liau Index Score</th>\n",
       "      <th>Estimated Grade Level</th>\n",
       "      <th>Gunning Fog score</th>\n",
       "      <th>The estimated grade level for comprehension is</th>\n",
       "      <th>SPACHE score</th>\n",
       "      <th>This corresponds to a grade level of</th>\n",
       "      <th>Linsear Write Index score</th>\n",
       "      <th>Approximate grade level equivalent</th>\n",
       "      <th>Perplexity (how well the LDA model predicts the corpus) of the article</th>\n",
       "      <th>Coherence (how coherent the topics are) of the article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.619815</td>\n",
       "      <td>11</td>\n",
       "      <td>60.218339</td>\n",
       "      <td>standard</td>\n",
       "      <td>9.881705</td>\n",
       "      <td>['college']</td>\n",
       "      <td>10.675124</td>\n",
       "      <td>['11']</td>\n",
       "      <td>[16, 17]</td>\n",
       "      <td>7.975991</td>\n",
       "      <td>8</td>\n",
       "      <td>12.584205</td>\n",
       "      <td>college</td>\n",
       "      <td>7.194891</td>\n",
       "      <td>7</td>\n",
       "      <td>14.635714</td>\n",
       "      <td>15</td>\n",
       "      <td>-4.537957</td>\n",
       "      <td>0.609045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Flesch-Kincaid score  Estimated reading level of the article   \n",
       "0             10.619815                                      11  \\\n",
       "\n",
       "   Flesch-Reading score The article is classified as   \n",
       "0             60.218339                     standard  \\\n",
       "\n",
       "   Dale-Chall Readability score   \n",
       "0                      9.881705  \\\n",
       "\n",
       "  The estimated comprehension level for different grade levels   \n",
       "0                                        ['college']            \\\n",
       "\n",
       "   Automated Readability Index (ARI) score It corresponds to a grade level of   \n",
       "0                                10.675124                             ['11']  \\\n",
       "\n",
       "  This means that the text can be read by someone who is around   \n",
       "0                                           [16, 17]             \\\n",
       "\n",
       "   Coleman-Liau Index Score  Estimated Grade Level  Gunning Fog score   \n",
       "0                  7.975991                      8          12.584205  \\\n",
       "\n",
       "  The estimated grade level for comprehension is  SPACHE score   \n",
       "0                                        college      7.194891  \\\n",
       "\n",
       "   This corresponds to a grade level of  Linsear Write Index score   \n",
       "0                                     7                  14.635714  \\\n",
       "\n",
       "   Approximate grade level equivalent   \n",
       "0                                  15  \\\n",
       "\n",
       "   Perplexity (how well the LDA model predicts the corpus) of the article   \n",
       "0                                          -4.537957                       \\\n",
       "\n",
       "   Coherence (how coherent the topics are) of the article  \n",
       "0                                           0.609045       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max tree depth</th>\n",
       "      <th>Words at max depth</th>\n",
       "      <th>,</th>\n",
       "      <th>Average tree length</th>\n",
       "      <th>Maximum tree length</th>\n",
       "      <th>Minimum tree length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>wakes, received</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.982143</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Max tree depth Words at max depth  ,   Average tree length   \n",
       "0              15    wakes, received NaN             4.982143  \\\n",
       "\n",
       "   Maximum tree length  Minimum tree length  \n",
       "0                   15                    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "urls = ['https://www.foxnews.com/politics/republicans-respond-after-irs-whistleblower-says-hunter-biden-investigation-being-mishandled',\n",
    "        'https://news.yahoo.com/alabama-education-director-ousted-over-234450832.html',\n",
    "        'https://news.yahoo.com/samantha-cameron-remind-david-steer-050000235.html']\n",
    "\n",
    "calculate_scores(urls, directory='processed articles')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
