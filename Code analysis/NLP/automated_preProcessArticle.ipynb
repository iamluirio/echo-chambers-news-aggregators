{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Pre-processing Text Article, and saving the score in a txt file for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cba7059a154e13b57f919efc3f3305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 17:34:46 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-04-29 17:34:48 INFO: File exists: /home/pierluigi/stanza_resources/en/default.zip\n",
      "2023-04-29 17:34:53 INFO: Finished downloading models and saved to /home/pierluigi/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import stanza\n",
    "stanza.download('en')  # Download the English model\n",
    "\n",
    "from readability import Readability\n",
    "\n",
    "import spacy\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import newspaper\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 17:34:54 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37724b1007c94918b2e911f0066b9c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 17:34:54 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| sentiment | sstplus  |\n",
      "========================\n",
      "\n",
      "2023-04-29 17:34:54 INFO: Using device: cpu\n",
      "2023-04-29 17:34:54 INFO: Loading: tokenize\n",
      "2023-04-29 17:34:54 INFO: Loading: sentiment\n",
      "2023-04-29 17:34:55 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Setting the use_gpu=False, it uses the CPU instead of the GPU for calculating stuff, and also for printing the results. And it couldn't run out of memory.\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment', tokenize_no_ssplit=False, max_split_size_mb=15, use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MPQA lexicon\n",
    "lexicon = pd.read_csv(\"/home/pierluigi/Documents/echo_chambers_intership/Code analysis/NLP/Single modules/subjclueslen1-HLTEMNLP05.tff\", sep=\" \", header=None, \n",
    "                      names=[\"type\", \"len\", \"word\", \"pos\", \"stemmed\", \"polarity\", \"strength\"])\n",
    "\n",
    "lexicon[\"type\"] = lexicon[\"type\"].str[5:]\n",
    "lexicon[\"word\"] = lexicon[\"word\"].str[len(\"word1=\"):]\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].str[len(\"priorpolarity=\"):]\n",
    "cols_to_remove = [\"len\", \"pos\", \"stemmed\", \"strength\"]\n",
    "lexicon = lexicon.drop(columns=cols_to_remove)\n",
    "lexicon[\"type\"] = lexicon[\"type\"].replace(\"weaksubj\", 1)\n",
    "lexicon[\"type\"] = lexicon[\"type\"].replace(\"strongsubj\", 2)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"negative\", -1)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"positive\", 1)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"both\", 0)\n",
    "lexicon[\"polarity\"] = lexicon[\"polarity\"].replace(\"neutral\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_article(url):\n",
    "    # Create a newspaper Article object\n",
    "    article = newspaper.Article(url)\n",
    "\n",
    "    # Download and parse the article\n",
    "    article.download()\n",
    "    article.parse()\n",
    "\n",
    "    # Extract the title, subtitle, description, and main text\n",
    "    title = article.title.strip()\n",
    "    subtitle = article.meta_data.get(\"description\", \"\").strip()\n",
    "    description = article.meta_description.strip()\n",
    "    text = article.text.strip()\n",
    "\n",
    "    # Set the subtitle to the description if it is empty\n",
    "    if not subtitle:\n",
    "        subtitle = description.strip()\n",
    "\n",
    "    # Concatenate the extracted strings\n",
    "    article_text = f\"{title}\\n\\n{subtitle}\\n\\n{text}\"\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(article_text)\n",
    "    \n",
    "    # Identify the stop words for each sentence\n",
    "    num_stop_words_per_sentence = []\n",
    "    stop_words_per_sentence = []\n",
    "    filtered_sentences = []\n",
    "    num_words_per_sentence = []\n",
    "    avg_stop_words_per_sentence = []\n",
    "    total_words = 0\n",
    "\n",
    "    # Create a Porter stemmer object\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_sentences = []\n",
    "\n",
    "    # Process the text with the pipeline and extract the sentiment for each sentence\n",
    "    doc = nlp(text)\n",
    "    s_sentiment_scores = []\n",
    "\n",
    "    # initialize the Vader sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    v_scores_list = []\n",
    "\n",
    "    # MPQA analysis\n",
    "    mpqa_scores = []\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Tokenize the sentence into words\n",
    "        words = word_tokenize(sentence)\n",
    "        all_words = len(words)\n",
    "        total_words += all_words\n",
    "        \n",
    "        # Identify the stop words in the sentence\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words_found = [word for word in words if word.lower() in stop_words]\n",
    "        all_stop_words = len(stop_words_found)\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        \n",
    "        # Add the number of stop words and filtered sentence to the output\n",
    "        num_stop_words = all_words - len(filtered_words)\n",
    "        num_stop_words_per_sentence.append(num_stop_words)\n",
    "        stop_words_per_sentence.append(stop_words_found)\n",
    "        filtered_sentences.append(\" \".join(filtered_words))\n",
    "        num_words_per_sentence.append(all_words)\n",
    "        \n",
    "        # Calculate the average number of stop words per sentence\n",
    "        avg_stop_words_per_sentence.append(num_stop_words / all_words)\n",
    "\n",
    "        # Perform stemming on each word using the Porter stemmer\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "        # Combine the stemmed words back into a single string\n",
    "        stemmed_sentence = ' '.join(stemmed_words)\n",
    "        stemmed_sentences.append(stemmed_sentence)\n",
    "\n",
    "        v_scores = analyzer.polarity_scores(sentence)\n",
    "        v_score_list = [v_scores['neg'], v_scores['neu'], v_scores['pos']]\n",
    "        v_scores_list.append(v_score_list)\n",
    "\n",
    "    # Sentiment analysis using Stanza library\n",
    "    for sentence in doc.sentences:\n",
    "        s_sentiment_scores.append(sentence.sentiment)\n",
    "    \n",
    "    # Sentiwordnet scores\n",
    "    sentiwordnet_final_score = 0\n",
    "    \n",
    "    # Loop through each word in the text\n",
    "    sentiment_score = 0\n",
    "    num_synsets = 0\n",
    "\n",
    "    for word in article_text.split():\n",
    "        word = word.strip().lower()\n",
    "        if word in lexicon.word.tolist():\n",
    "            polarity = lexicon[lexicon.word == word].polarity.values[0]\n",
    "            mpqa_scores.append(polarity)\n",
    "        \n",
    "        synsets = wn.synsets(word)\n",
    "        if len(synsets) > 0:\n",
    "            synset = synsets[0]\n",
    "            senti_synset = swn.senti_synset(synset.name())\n",
    "            sentiment_score += senti_synset.pos_score() - senti_synset.neg_score()\n",
    "            num_synsets += 1\n",
    "\n",
    "    # Calculate summary statistics\n",
    "    num_stop_words = sum(num_stop_words_per_sentence)\n",
    "    num_sentences = len(sentences)\n",
    "    avg_stop_words_per_sentence_all = num_stop_words / num_sentences\n",
    "    max_stop_words_per_sentence = max(num_stop_words_per_sentence)\n",
    "    min_stop_words_per_sentence = min(num_stop_words_per_sentence)\n",
    "    avg_stop_words_per_word = num_stop_words / total_words\n",
    "    \n",
    "    # Calculate the average number of stop words per article\n",
    "    avg_stop_words_per_sentence_avg = sum(avg_stop_words_per_sentence) / len(avg_stop_words_per_sentence)\n",
    "    \n",
    "    # Vader scores\n",
    "    v_scores_array = np.array(v_scores_list)\n",
    "    v_avg_scores = np.mean(v_scores_array, axis=0)\n",
    "    v_max_scores = np.max(v_scores_array, axis=0)\n",
    "    v_min_scores = np.min(v_scores_array, axis=0)\n",
    "    v_std_scores = np.std(v_scores_array, axis=0)\n",
    "\n",
    "    # MPQA scores\n",
    "    mpqa_avg_score = np.mean(mpqa_scores)\n",
    "    mpqa_max_score = np.max(mpqa_scores)\n",
    "    mpqa_min_score = np.min(mpqa_scores)\n",
    "    mpqa_sd_score = np.std(mpqa_scores)\n",
    "\n",
    "    # Calculate final score        \n",
    "    if num_synsets > 0:\n",
    "        sentiwordnet_final_score = sentiment_score / num_synsets\n",
    "    else:\n",
    "        sentiwordnet_final_score = 0\n",
    "\n",
    "    # Determine the readability of the text article\n",
    "    read = Readability(article_text)\n",
    "\n",
    "    # Flesch Kincaid Grade Level\n",
    "    flesch_kincaid = read.flesch_kincaid()\n",
    "\n",
    "    # Flesch Reading Ease\n",
    "    flesch_reading = read.flesch()\n",
    "\n",
    "    # Dale Chall Readability\n",
    "    dale_chall = read.dale_chall()\n",
    "\n",
    "    # Automated Readability Index (ARI)\n",
    "    ari = read.ari()\n",
    "\n",
    "    # Coleman Liau Index\n",
    "    coleman_liau = read.coleman_liau()\n",
    "\n",
    "    # Gunning Fog\n",
    "    gunning_fog = read.gunning_fog()\n",
    "\n",
    "    # SMOG: at least 30 sentences required. Uncomment if needed.\n",
    "    #smog = read.smog()\n",
    "\n",
    "    # SPACHE\n",
    "    spache = read.spache()\n",
    "\n",
    "    # Linsear Write\n",
    "    linsear_write = read.linsear_write()\n",
    "\n",
    "\n",
    "    # Return the output\n",
    "    return {\n",
    "        'title': title,\n",
    "        'num_stop_words': num_stop_words,\n",
    "        'total_words': total_words,\n",
    "        'stop_words_found': stop_words_found,\n",
    "        'all_stop_words': all_stop_words,\n",
    "        'avg_stop_words_per_sentence_all': avg_stop_words_per_sentence_all,\n",
    "        'max_stop_words_per_sentence': max_stop_words_per_sentence,\n",
    "        'min_stop_words_per_sentence': min_stop_words_per_sentence,\n",
    "        'avg_stop_words_per_word': avg_stop_words_per_word,\n",
    "        'avg_stop_words_per_sentence': avg_stop_words_per_sentence,\n",
    "        'avg_stop_words_per_sentence_avg': avg_stop_words_per_sentence_avg,\n",
    "        'filtered_sentences': filtered_sentences,\n",
    "        'stop_words_per_sentence': stop_words_per_sentence,\n",
    "        'num_words_per_sentence': num_words_per_sentence,\n",
    "        'num_stop_words_per_sentence': num_stop_words_per_sentence,\n",
    "        's_sentiment_scores': s_sentiment_scores,\n",
    "        'v_avg_scores': v_avg_scores,\n",
    "        'v_max_scores': v_max_scores,\n",
    "        'v_min_scores': v_min_scores,\n",
    "        'v_std_scores': v_std_scores,\n",
    "        'mpqa_avg_score': mpqa_avg_score,\n",
    "        'mpqa_max_score': mpqa_max_score,\n",
    "        'mpqa_min_score': mpqa_min_score,\n",
    "        'mpqa_sd_score': mpqa_sd_score,\n",
    "        'sentiwordnet_final_score': sentiwordnet_final_score,\n",
    "        'flesch_kincaid_score': flesch_kincaid.score,\n",
    "        'flesch_kincaid_grade_level': flesch_kincaid.grade_level,\n",
    "        'flesch_reading_score': flesch_reading.score,\n",
    "        'flesch_reading_ease': flesch_reading.ease,\n",
    "        'dale_chall_score': dale_chall.score,\n",
    "        'dale_chall_grade_levels': dale_chall.grade_levels,\n",
    "        'ari_score': ari.score,\n",
    "        'ari_grade_level': ari.grade_levels,\n",
    "        'ari_ages': ari.ages,\n",
    "        'coleman_liau_score': coleman_liau.score,\n",
    "        'coleman_liau_grade_level': coleman_liau.grade_level,\n",
    "        'gunning_fog_score': gunning_fog.score,\n",
    "        'gunning_fog_grade_level': gunning_fog.grade_level,\n",
    "        #'smog_score': smog.score,\n",
    "        #'smog_grade_level': smog.grade_level,\n",
    "        'spache_score': spache.score,\n",
    "        'spache_grade_level': spache.grade_level,\n",
    "        'linsear_write_score': linsear_write.score,\n",
    "        'linsear_write_grade_level': linsear_write.grade_level\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_articles(urls, directory):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    for url in urls:\n",
    "        results = preprocess_article(url)\n",
    "        # Write preprocessed article to a separate file for each URL\n",
    "        file_path = f'{directory}/{results[\"title\"]}.txt'\n",
    "        \n",
    "        with open(file_path, 'w') as f:\n",
    "            # Save the information for each sentence to the file\n",
    "            for i, sentence in enumerate(results['filtered_sentences']):\n",
    "                f.write(f\"Sentence {i+1}: {sentence}\\n\")\n",
    "                f.write(f\"Total words: {results['num_words_per_sentence'][i]}\\n\")\n",
    "                f.write(f\"Filtered words: {sentence.split()}\\n\")\n",
    "                f.write(f\"Number of filtered words: {len(sentence.split())}\\n\")\n",
    "                f.write(f\"Stop words: {results['stop_words_per_sentence'][i]}\\n\")\n",
    "                f.write(f\"Number of stop words: {results['num_stop_words_per_sentence'][i]}\\n\")\n",
    "                f.write(f\"Average number of stop words per sentence: {round(results['avg_stop_words_per_sentence'][i], 2)}\\n\")\n",
    "                f.write(f\"Sentiment score: {results['s_sentiment_scores'][i]}\\n\\n\")\n",
    "\n",
    "            # Save the general statistics on stop words to the file\n",
    "            f.write(f\"Total number of words: {results['total_words']}\\n\")\n",
    "            f.write(f\"Total number of stop words: {results['num_stop_words']}\\n\")\n",
    "            f.write(f\"Maximum number of stop words per sentence: {results['max_stop_words_per_sentence']}\\n\")\n",
    "            f.write(f\"Minimum number of stop words per sentence: {results['min_stop_words_per_sentence']}\\n\")\n",
    "            f.write(f\"Average number of stop words per article: {round(results['avg_stop_words_per_sentence_avg'], 2)}\\n\\n\")\n",
    "            \n",
    "            # Stanza sentiment scores\n",
    "            f.write(f\"Stanza Average of sentiment score for all sentences: {sum(results['s_sentiment_scores']) / len(results['s_sentiment_scores'])}\\n\")\n",
    "            f.write(f\"Stanza Maximum sentiment score: {max(results['s_sentiment_scores'])}\\n\")\n",
    "            f.write(f\"Stanza Minimum sentiment score: {min(results['s_sentiment_scores'])}\\n\")\n",
    "            f.write(f\"Stanza Standard deviation: {statistics.stdev(results['s_sentiment_scores'])}\\n\\n\")\n",
    "\n",
    "            # Vader sentiment scores\n",
    "            f.write(f\"Vader average scores: {results['v_avg_scores']}\\n\")\n",
    "            f.write(f\"Vader maximum scores: {results['v_max_scores']}\\n\")\n",
    "            f.write(f\"Vader minimum scores: {results['v_min_scores']}\\n\")\n",
    "            f.write(f\"Vader standard deviation scores: {results['v_std_scores']}\\n\\n\")\n",
    "\n",
    "            # MPQA sentiment scores\n",
    "            f.write(f\"MPQA average scores: {results['mpqa_avg_score']}\\n\")\n",
    "            f.write(f\"MPQA maximum scores: {results['mpqa_max_score']}\\n\")\n",
    "            f.write(f\"MPQA minimum scores: {results['mpqa_min_score']}\\n\")\n",
    "            f.write(f\"MPQA standard deviation scores: {results['mpqa_sd_score']}\\n\\n\")\n",
    "\n",
    "            # Sentiword sentiment scores\n",
    "            f.write(f\"Sentiwordnet score: {results['sentiwordnet_final_score']} (from -1 to 1, and score of 0 indicates a neutral sentiment.)\\n\\n\")\n",
    "            \n",
    "            # Flesch_Kincaid scores\n",
    "            f.write(f\"Flesch-Kincaid score: {results['flesch_kincaid_score']}\\n\")\n",
    "            f.write(f\"The estimated reading level of the article is: {results['flesch_kincaid_grade_level']}\\n\\n\") \n",
    "\n",
    "            # Flesch Reading ease scores\n",
    "            f.write(f\"Flesch Reading Ease score: {results['flesch_reading_score']}\\n\")\n",
    "            f.write(f\"The article is classified as: {results['flesch_reading_ease']}\\n\\n\")\n",
    "\n",
    "            # Print the Dale-Chall scores\n",
    "            f.write(f\"Dale-Chall Readability score: {results['dale_chall_score']}\\n\")\n",
    "            # Print the estimated grade levels for comprehension\n",
    "            f.write(f\"The estimated comprehension level for different grade levels is: {results['dale_chall_grade_levels']}\\n\\n\")\n",
    "\n",
    "            # Print the ARI scores\n",
    "            f.write(f\"Automated Readability Index (ARI) score: {results['ari_score']}, which corresponds to a grade level of {results['ari_grade_level']}.\\n\")\n",
    "            f.write(f\"This means that the text can be read by someone who is around {results['ari_ages']} years old.\\n\\n\")\n",
    "\n",
    "            # Print the Coleman-Liau scores\n",
    "            f.write(f\"Coleman-Liau Index Score: {results['coleman_liau_score']}\\n\")\n",
    "            f.write(f\"Estimated Grade Level: {results['coleman_liau_grade_level']}\\n\\n\")\n",
    "\n",
    "            # Print the Gunning Fog scores\n",
    "            f.write(f\"Gunning Fog score: {results['gunning_fog_score']}\\n\")\n",
    "            f.write(f\"The estimated grade level for comprehension is: {results['gunning_fog_grade_level']}\\n\\n\")\n",
    "\n",
    "            # Print the SMOG scores\n",
    "            #f.write(f\"SMOG score: {results['smog_score']}. This corresponds to a grade level of {results['smog_grade_level']}.\")\n",
    "            \n",
    "            # Print the SPACHE scores\n",
    "            f.write(f\"SPACHE score: {results['spache_score']}\\n\")\n",
    "            f.write(f\"This corresponds to a grade level of {results['spache_grade_level']}.\\n\\n\")\n",
    "\n",
    "            # Print the Linsear Write Index scores\n",
    "            f.write(f\"Linsear Write Index score: {results['linsear_write_score']}\\n\")\n",
    "            f.write(\"Approximate grade level equivalent: {}\".format(results['linsear_write_grade_level']))\n",
    "            print(\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "urls = ['https://www.foxnews.com/politics/republicans-respond-after-irs-whistleblower-says-hunter-biden-investigation-being-mishandled',\n",
    "        'https://news.yahoo.com/alabama-education-director-ousted-over-234450832.html',\n",
    "        'https://news.yahoo.com/samantha-cameron-remind-david-steer-050000235.html']\n",
    "\n",
    "preprocess_articles(urls, directory='preprocessed articles')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
