{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling using Gensim-LDA in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is technique to extract the hidden topics from large volumes of text. Topic model is a probabilistic model which contain information about the text.\n",
    "\n",
    "Ex: If it is a news paper corpus it may have topics like economics, sports, politics, weather.\n",
    "\n",
    "Topic models are useful for purpose of document clustering, organizing large blocks of textual data, information retrieval from unstructured text and feature selection. Finding good topics depends on the quality of text processing , the choice of the topic modeling algorithm, the number of topics specified in the algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDAâ€™s approach to topic modeling is, it considers each document as a collection of topics and each topic as collection of keywords. Once you provide the algorithm with number of topics all it does is to rearrange the topic distribution within documents and key word distribution within the topics to obtain good composition of topic-keyword distribution.\n",
    "\n",
    "Topics are nothing but collection of prominent keywords or words with highest probability in topic , which helps to identify what the topics are about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as  pd\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import pyLDAvis.gensim_models as gensimvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.032*\"di\" + 0.016*\"il\" + 0.012*\"e\" + 0.011*\"che\" + 0.011*\"ha\" + 0.010*\"un\" + 0.009*\"con\" + 0.009*\"le\" + 0.009*\"la\" + 0.008*\"anche\"')\n",
      "(1, '0.028*\"di\" + 0.019*\"il\" + 0.016*\"e\" + 0.014*\"che\" + 0.014*\"le\" + 0.010*\"anche\" + 0.010*\"ha\" + 0.010*\"un\" + 0.009*\"con\" + 0.009*\"la\"')\n",
      "(2, '0.028*\"di\" + 0.018*\"il\" + 0.014*\"e\" + 0.012*\"ha\" + 0.012*\"le\" + 0.011*\"la\" + 0.010*\"anche\" + 0.010*\"che\" + 0.009*\"con\" + 0.008*\"cittadini\"')\n",
      "(3, '0.029*\"di\" + 0.021*\"e\" + 0.020*\"il\" + 0.013*\"le\" + 0.013*\"che\" + 0.011*\"la\" + 0.011*\"ha\" + 0.009*\"con\" + 0.009*\"anche\" + 0.009*\"un\"')\n",
      "(4, '0.041*\"di\" + 0.018*\"e\" + 0.018*\"il\" + 0.017*\"che\" + 0.013*\"le\" + 0.013*\"la\" + 0.012*\"con\" + 0.012*\"anche\" + 0.011*\"ha\" + 0.010*\"un\"')\n",
      "Coherence score: 0.1881614243252218\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DataFrame.drop() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m article_text \u001b[39m=\u001b[39m article\u001b[39m.\u001b[39mtext\n\u001b[1;32m     48\u001b[0m \u001b[39m# Generate the LDA model and topics\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m generate_lda_topics(article_text)\n",
      "Cell \u001b[0;32mIn[13], line 36\u001b[0m, in \u001b[0;36mgenerate_lda_topics\u001b[0;34m(text, num_topics)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCoherence score: \u001b[39m\u001b[39m{\u001b[39;00mcoherence_score\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[39m# Visualize the LDA model with pyLDAvis\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m vis \u001b[39m=\u001b[39m gensimvis\u001b[39m.\u001b[39;49mprepare(lda_model, corpus, dictionary)\n\u001b[1;32m     37\u001b[0m pyLDAvis\u001b[39m.\u001b[39mdisplay(vis)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyLDAvis/gensim_models.py:123\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mthe data structures needed for the visualization.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mSee `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m opts \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mmerge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n\u001b[0;32m--> 123\u001b[0m \u001b[39mreturn\u001b[39;00m pyLDAvis\u001b[39m.\u001b[39;49mprepare(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopts)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyLDAvis/_prepare.py:432\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[39m# Quick fix for red bar width bug.  We calculate the\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39m# term frequencies internally, using the topic term distributions and the\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m# topic frequencies, rather than using the user-supplied term frequencies.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39m# For a detailed discussion, see: https://github.com/cpsievert/LDAvis/pull/41\u001b[39;00m\n\u001b[1;32m    430\u001b[0m term_frequency \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(term_topic_freq, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 432\u001b[0m topic_info \u001b[39m=\u001b[39m _topic_info(topic_term_dists, topic_proportion,\n\u001b[1;32m    433\u001b[0m                          term_frequency, term_topic_freq, vocab, lambda_step, R,\n\u001b[1;32m    434\u001b[0m                          n_jobs, start_index)\n\u001b[1;32m    435\u001b[0m token_table \u001b[39m=\u001b[39m _token_table(topic_info, term_topic_freq, vocab, term_frequency, start_index)\n\u001b[1;32m    436\u001b[0m topic_coordinates \u001b[39m=\u001b[39m _topic_coordinates(mds, topic_term_dists, topic_proportion, start_index)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyLDAvis/_prepare.py:244\u001b[0m, in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs, start_index)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39m# Order the terms for the \"default\" view by decreasing saliency:\u001b[39;00m\n\u001b[1;32m    237\u001b[0m default_term_info \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\n\u001b[1;32m    238\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msaliency\u001b[39m\u001b[39m'\u001b[39m: saliency,\n\u001b[1;32m    239\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mTerm\u001b[39m\u001b[39m'\u001b[39m: vocab,\n\u001b[1;32m    240\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mFreq\u001b[39m\u001b[39m'\u001b[39m: term_frequency,\n\u001b[1;32m    241\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mTotal\u001b[39m\u001b[39m'\u001b[39m: term_frequency,\n\u001b[1;32m    242\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mCategory\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mDefault\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m    243\u001b[0m default_term_info \u001b[39m=\u001b[39m default_term_info\u001b[39m.\u001b[39;49msort_values(\n\u001b[0;32m--> 244\u001b[0m     by\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msaliency\u001b[39;49m\u001b[39m'\u001b[39;49m, ascending\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\u001b[39m.\u001b[39;49mhead(R)\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39msaliency\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    245\u001b[0m \u001b[39m# Rounding Freq and Total to integer values to match LDAvis code:\u001b[39;00m\n\u001b[1;32m    246\u001b[0m default_term_info[\u001b[39m'\u001b[39m\u001b[39mFreq\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfloor(default_term_info[\u001b[39m'\u001b[39m\u001b[39mFreq\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: DataFrame.drop() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# Load the 'en_core_web_sm' model from spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to preprocess the text data\n",
    "def preprocess(text):\n",
    "    # Create a spaCy doc object\n",
    "    doc = nlp(text)\n",
    "    # Remove stop words and punctuation, and lemmatize the tokens\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return tokens\n",
    "\n",
    "# Function to generate LDA model and topics\n",
    "def generate_lda_topics(text, num_topics=5):\n",
    "    # Preprocess the text data\n",
    "    tokens = preprocess(text)\n",
    "\n",
    "    # Create a dictionary from the preprocessed tokens\n",
    "    dictionary = Dictionary([tokens])\n",
    "\n",
    "    # Create a bag-of-words representation of the preprocessed text data\n",
    "    corpus = [dictionary.doc2bow(tokens)]\n",
    "\n",
    "    # Train the LDA model on the corpus\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "\n",
    "    # Print the topics and their top words\n",
    "    for topic in lda_model.show_topics():\n",
    "        print(topic)\n",
    "\n",
    "    # Compute the coherence score of the LDA model\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=[tokens], dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    print(f'Coherence score: {coherence_score}')\n",
    "\n",
    "    # Visualize the LDA model with pyLDAvis\n",
    "    vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "    pyLDAvis.display(vis)\n",
    "\n",
    "# Get the article URL from user input\n",
    "article_url = \"https://www.ilfattoquotidiano.it/2023/04/23/blitz-americano-in-sudan-evacuato-personale-dellambasciata-attacco-ai-francesi-ce-un-ferito-tajani-gli-italiani-stanno-bene/7139402/\"\n",
    "\n",
    "# Download and extract the article text using the newspaper library\n",
    "article = newspaper.Article(url=article_url)\n",
    "article.download()\n",
    "article.parse()\n",
    "article_text = article.text\n",
    "\n",
    "# Generate the LDA model and topics\n",
    "generate_lda_topics(article_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
