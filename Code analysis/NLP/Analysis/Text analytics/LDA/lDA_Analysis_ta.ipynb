{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9512b58df4274bb498c8b9d64d26513a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 10:34:34 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-06-08 10:34:35 INFO: File exists: /home/pierluigi/stanza_resources/en/default.zip\n",
      "2023-06-08 10:34:42 INFO: Finished downloading models and saved to /home/pierluigi/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from newsapi import NewsApiClient\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from openpyxl import load_workbook\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from readability.exceptions import ReadabilityException\n",
    "\n",
    "import stanza\n",
    "stanza.download('en')  # Download the English model\n",
    "\n",
    "from readability import Readability\n",
    "\n",
    "import spacy\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "import csv\n",
    "from tabulate import tabulate\n",
    "import newspaper\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from openpyxl import Workbook\n",
    "import openpyxl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_article_text(url):\n",
    "    # Set headers to mimic a web browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the URL with headers\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract the title, subtitle, description, and main text\n",
    "    title_element = soup.find('title')\n",
    "    title = title_element.text.strip() if title_element else \"\"\n",
    "\n",
    "    subtitle_element = soup.find('meta', attrs={'name': 'description'})\n",
    "    subtitle = subtitle_element['content'].strip() if subtitle_element and 'content' in subtitle_element.attrs else \"\"\n",
    "\n",
    "    description_element = soup.find('meta', attrs={'name': 'og:description'})\n",
    "    description = description_element['content'].strip() if description_element and 'content' in description_element.attrs else \"\"\n",
    "\n",
    "    # Find and exclude unwanted elements by class names or content patterns\n",
    "    unwanted_elements = soup.find_all(['script', 'style', 'a', 'div', 'span'], class_=['follow-us', 'newsletter', 'advertisement'])\n",
    "    patterns_to_exclude = ['next article', 'read next', 'correlated']\n",
    "    for element in unwanted_elements:\n",
    "        if any(pattern in str(element).lower() for pattern in patterns_to_exclude):\n",
    "            element.extract()\n",
    "\n",
    "    # Find and exclude footer container and \"All rights reserved\" text\n",
    "    footer_elements = soup.find_all(['footer', 'div'], class_=['footer', 'bottom-footer'])\n",
    "    for element in footer_elements:\n",
    "        element.extract()\n",
    "    all_rights_reserved_elements = soup.find_all(text=re.compile(r'\\bAll rights reserved\\b', re.IGNORECASE))\n",
    "    for element in all_rights_reserved_elements:\n",
    "        element.extract()\n",
    "\n",
    "    # Find the main text element(s) based on the HTML structure of the page\n",
    "    main_text_elements = soup.find_all('p')\n",
    "    main_text = \"\\n\\n\".join([element.text.strip() for element in main_text_elements if element.text.strip()])\n",
    "\n",
    "    # Set the subtitle to the description if it is empty\n",
    "    if not subtitle:\n",
    "        subtitle = description.strip()\n",
    "\n",
    "    # Concatenate the extracted strings\n",
    "    article_text = f\"{title}\\n\\n{subtitle}\\n\\n{main_text}\"\n",
    "\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_real_url(google_news_url):\n",
    "    response = requests.get(google_news_url, cookies = {'CONSENT' : 'YES+'})\n",
    "    real_url = response.url\n",
    "    return real_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_dict):\n",
    "    processed_data = {}\n",
    "    \n",
    "    for sheet_name, data_list in data_dict.items():\n",
    "        sheet_data = {}\n",
    "        \n",
    "        for index, url in enumerate(data_list, start=1):\n",
    "            try:\n",
    "                article_url = extract_real_url(url)\n",
    "                article_text = save_article_text(article_url)\n",
    "                key = f\"article{index}\"\n",
    "                sheet_data[key] = article_text\n",
    "            except (requests.exceptions.RequestException, requests.exceptions.HTTPError) as e:\n",
    "                # Handle the exception and continue with the next URL\n",
    "                print(f\"Error processing URL: {url}\")\n",
    "                print(f\"Error message: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        processed_data[sheet_name] = sheet_data\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Perform lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Generate bigrams\n",
    "    bigrams = list(nltk.bigrams(lemmatized_words))\n",
    "    lemmatized_bigrams = [bigram for bigram in bigrams]\n",
    "    \n",
    "    return lemmatized_bigrams\n",
    "\n",
    "def apply_lda_to_articles(data_dict, min_topics, max_topics):\n",
    "    lda_results = {}  # Dictionary to store the best number of topics for each sheet\n",
    "    \n",
    "    for sheet_name, sheet_data in data_dict.items():\n",
    "        sheet_results = []\n",
    "        \n",
    "        for article_key, article_text in sheet_data.items():\n",
    "            lemmatized_bigrams = preprocess_text(article_text)\n",
    "            \n",
    "            # Create Dictionary\n",
    "            id2word = corpora.Dictionary(lemmatized_bigrams)\n",
    "            \n",
    "            # Create Corpus\n",
    "            texts = lemmatized_bigrams\n",
    "            corpus = [id2word.doc2bow(text) for text in texts]\n",
    "            \n",
    "            if not corpus:\n",
    "                print(\"Error: No data in the corpus. Unable to compute LDA model for article:\", article_key)\n",
    "                continue\n",
    "            \n",
    "            best_num_topics = None\n",
    "            best_coherence = float('-inf')\n",
    "            \n",
    "            for num_topics in range(min_topics, max_topics + 1):\n",
    "                lda_model = gensim.models.LdaModel(corpus=corpus,\n",
    "                                                   id2word=id2word,\n",
    "                                                   num_topics=num_topics,\n",
    "                                                   random_state=100,\n",
    "                                                   update_every=1,\n",
    "                                                   chunksize=100,\n",
    "                                                   passes=10,\n",
    "                                                   alpha='auto',\n",
    "                                                   per_word_topics=True)\n",
    "                coherence_model_lda = CoherenceModel(model=lda_model, texts=lemmatized_bigrams, dictionary=id2word, coherence='c_v')\n",
    "                coherence = coherence_model_lda.get_coherence()\n",
    "                \n",
    "                if coherence > best_coherence:\n",
    "                    best_num_topics = {'num_topics': num_topics}\n",
    "                    best_coherence = coherence\n",
    "            \n",
    "            if best_num_topics is not None:\n",
    "                lda_results[(sheet_name, article_key)] = best_num_topics\n",
    "            else:\n",
    "                print(\"Error: Unable to determine the optimal number of topics for article:\", article_key)\n",
    "        \n",
    "        lda_results[sheet_name] = sheet_results\n",
    "    \n",
    "    return lda_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scores_to_excel_lda(processed_data_list, lda_scores_list, output_file_path):\n",
    "    output_workbook = openpyxl.Workbook()\n",
    "    processed_articles = 0\n",
    "    \n",
    "    for i, processed_data in enumerate(processed_data_list):\n",
    "        lda_scores = lda_scores_list[i]\n",
    "        sheet_name = f\"Sheet {i+1}\"\n",
    "        \n",
    "        if i == 0:\n",
    "            output_sheet = output_workbook.active\n",
    "            output_sheet.title = sheet_name\n",
    "        else:\n",
    "            output_sheet = output_workbook.create_sheet(title=sheet_name)\n",
    "        \n",
    "        output_sheet['A1'] = 'Date'\n",
    "        \n",
    "        row = 2\n",
    "        \n",
    "        for sheet_name, sheet_data in processed_data.items():\n",
    "            output_sheet.cell(row=row, column=1, value=sheet_name)\n",
    "            column = 2\n",
    "            \n",
    "            for article_index, (article_key, article_text) in enumerate(sheet_data.items()):\n",
    "                if (sheet_name, article_key) in lda_scores:\n",
    "                    scores = lda_scores[(sheet_name, article_key)]\n",
    "                    output_sheet.cell(row=1, column=column, value=f\"LDA Number of topics {article_index+1}\")\n",
    "                    output_sheet.cell(row=row, column=column, value=scores['num_topics'])\n",
    "                else:\n",
    "                    output_sheet.cell(row=1, column=column, value=f\"LDA Number of topics {article_index+1}\")\n",
    "                    output_sheet.cell(row=row, column=column, value='N/A')\n",
    "                \n",
    "                column += 1\n",
    "            row += 1\n",
    "            processed_articles += 1\n",
    "            print(f\"Sheet: {sheet_name} | Article: {article_key} done.\")\n",
    "\n",
    "    \n",
    "    output_workbook.save(output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excel_to_csv(excel_file):\n",
    "    # Load the Excel file\n",
    "    xls = pd.ExcelFile(excel_file)\n",
    "\n",
    "    # Get the sheet names\n",
    "    sheet_names = xls.sheet_names\n",
    "\n",
    "    # Iterate over each sheet and convert to CSV\n",
    "    for sheet_name in sheet_names:\n",
    "        # Read the sheet as a DataFrame\n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "\n",
    "        # Generate the CSV file name\n",
    "        csv_file = f\"{sheet_name}.csv\"\n",
    "\n",
    "        # Save the DataFrame as a CSV file\n",
    "        df.to_csv(csv_file, index=False)\n",
    "\n",
    "        print(f\"CSV file '{csv_file}' created for sheet '{sheet_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    workbook = load_workbook('/home/pierluigi/Documents/echo_chambers_intership/Code analysis/NLP/Analysis/Text analytics/Text Analytics.xlsx')\n",
    "\n",
    "    data_U1 = {}  # Dictionary to store data from range 1 - U1\n",
    "    data_U2 = {}  # Dictionary to store data from range 2 - U2\n",
    "    data_U3 = {}  # Dictionary to store data from range 3 - U3\n",
    "    data_U4 = {}  # Dictionary to store data from range 4 - U4\n",
    "\n",
    "    # Iterate over each sheet in the workbook\n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        worksheet = workbook[sheet_name]\n",
    "\n",
    "        # Specify the range of cells for data_U1\n",
    "        range1_start_cell = 'C2'  # Replace with the starting cell of range 1\n",
    "        range1_end_cell = 'L2'  # Replace with the ending cell of range 1\n",
    "        \n",
    "        data_list1 = []\n",
    "        \n",
    "        # Iterate over the cells within range 1\n",
    "        for row in worksheet[range1_start_cell:range1_end_cell]:\n",
    "            for cell in row:\n",
    "                data_list1.append(cell.value)\n",
    "        \n",
    "        data_U1[sheet_name] = data_list1\n",
    "\n",
    "        # Specify the range of cells for data_U2\n",
    "        range2_start_cell = 'C3'  # Replace with the starting cell of range 2\n",
    "        range2_end_cell = 'L3'  # Replace with the ending cell of range 2\n",
    "        \n",
    "        data_list2 = []\n",
    "        \n",
    "        # Iterate over the cells within range 2\n",
    "        for row in worksheet[range2_start_cell:range2_end_cell]:\n",
    "            for cell in row:\n",
    "                data_list2.append(cell.value)\n",
    "        \n",
    "        data_U2[sheet_name] = data_list2\n",
    "\n",
    "        # Specify the range of cells for data_U3\n",
    "        range3_start_cell = 'C4'  # Replace with the starting cell of range 3\n",
    "        range3_end_cell = 'L4'  # Replace with the ending cell of range 3\n",
    "        \n",
    "        data_list3 = []\n",
    "        \n",
    "        # Iterate over the cells within range 3\n",
    "        for row in worksheet[range3_start_cell:range3_end_cell]:\n",
    "            for cell in row:\n",
    "                data_list3.append(cell.value)\n",
    "        \n",
    "        data_U3[sheet_name] = data_list3\n",
    "\n",
    "        # Specify the range of cells for data_U4\n",
    "        range4_start_cell = 'C5'  # Replace with the starting cell of range 4\n",
    "        range4_end_cell = 'L5'  # Replace with the ending cell of range 4\n",
    "        \n",
    "        data_list4 = []\n",
    "        \n",
    "        # Iterate over the cells within range 4\n",
    "        for row in worksheet[range4_start_cell:range4_end_cell]:\n",
    "            for cell in row:\n",
    "                data_list4.append(cell.value)\n",
    "        \n",
    "        data_U4[sheet_name] = data_list4\n",
    "\n",
    "    workbook.close()\n",
    "\n",
    "    # Process the data\n",
    "    processed_data1 = process_data(data_U1)\n",
    "    processed_data2 = process_data(data_U2)\n",
    "    processed_data3 = process_data(data_U3)\n",
    "    processed_data4 = process_data(data_U4)\n",
    "\n",
    "    lda_scores1 = apply_lda_to_articles(processed_data1, 1, 10)\n",
    "    lda_scores2 = apply_lda_to_articles(processed_data2, 1, 10)\n",
    "    lda_scores3 = apply_lda_to_articles(processed_data3, 1, 10)\n",
    "    lda_scores4 = apply_lda_to_articles(processed_data4, 1, 10)\n",
    "\n",
    "    processed_data_list = [processed_data1, processed_data2, processed_data3, processed_data4]\n",
    "    lda_scores_list = [lda_scores1, lda_scores2, lda_scores3, lda_scores4]\n",
    "\n",
    "    output_file_path = '/home/pierluigi/Desktop/lda.xlsx'\n",
    "\n",
    "    save_scores_to_excel_lda(processed_data_list, lda_scores_list, output_file_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing URL: https://news.google.com/articles/CBMiSGh0dHBzOi8vd3d3LmxpZmVzdHlsZWFzaWEuY29tL2tsL3N0eWxlL2Zhc2hpb24vbWV0LWdhbGEtMjAyMy1iZXN0LW1lbWVzL9IBTGh0dHBzOi8vd3d3LmxpZmVzdHlsZWFzaWEuY29tL2tsL3N0eWxlL2Zhc2hpb24vbWV0LWdhbGEtMjAyMy1iZXN0LW1lbWVzL2FtcC8?hl=en-US&gl=US&ceid=US%3Aen\n",
      "Error message: 403 Client Error: Forbidden for url: https://www.lifestyleasia.com/kl/style/fashion/met-gala-2023-best-memes/\n",
      "Error processing URL: https://news.google.com/articles/CBMiUmh0dHBzOi8vc2NyZWVucmFudC5jb20vdmFuZGVycHVtcC1ydWxlcy10b20tc2FuZG92YWwtZGlzZGFpbi1rYXRpZS1kZWVwZXItbWVhbmluZy_SAQA?hl=en-US&gl=US&ceid=US%3Aen\n",
      "Error message: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Error processing URL: https://news.google.com/articles/CBMiUWh0dHBzOi8vbmV3cy55YWhvby5jb20vc2VuYXRlLXJlcHVibGljYW5zLWluY2x1ZGluZy1tY2Nvbm5lbGwtZGVidC0yMjAwMzg1ODYuaHRtbNIBWWh0dHBzOi8vbmV3cy55YWhvby5jb20vYW1waHRtbC9zZW5hdGUtcmVwdWJsaWNhbnMtaW5jbHVkaW5nLW1jY29ubmVsbC1kZWJ0LTIyMDAzODU4Ni5odG1s?hl=en-US&gl=US&ceid=US%3Aen\n",
      "Error message: 404 Client Error: Not Found for url: https://news.yahoo.com/senate-republicans-including-mcconnell-debt-220038586.html\n",
      "Error processing URL: https://news.google.com/articles/CBMiamh0dHBzOi8vd3d3LndhdnkuY29tL25ld3MvbWlsaXRhcnkva2lnZ2Fucy1kaXNwdXRlcy1jbGFpbXMtdGhhdC1idWRnZXQtcGxhbi13b3VsZC1zbGFzaC12ZXRlcmFucy1iZW5lZml0cy_SAW5odHRwczovL3d3dy53YXZ5LmNvbS9uZXdzL21pbGl0YXJ5L2tpZ2dhbnMtZGlzcHV0ZXMtY2xhaW1zLXRoYXQtYnVkZ2V0LXBsYW4td291bGQtc2xhc2gtdmV0ZXJhbnMtYmVuZWZpdHMvYW1wLw?hl=en-US&gl=US&ceid=US%3Aen\n",
      "Error message: 451 Client Error: Unavailable For Legal Reasons for url: https://www.wavy.com/news/military/kiggans-disputes-claims-that-budget-plan-would-slash-veterans-benefits/\n",
      "Sheet: 02052023 | Article: article10 done.\n",
      "Sheet: 03052023 | Article: article10 done.\n",
      "Sheet: 04052023 | Article: article10 done.\n",
      "Sheet: 02052023 | Article: article10 done.\n",
      "Sheet: 03052023 | Article: article10 done.\n",
      "Sheet: 04052023 | Article: article9 done.\n",
      "Sheet: 02052023 | Article: article10 done.\n",
      "Sheet: 03052023 | Article: article10 done.\n",
      "Sheet: 04052023 | Article: article10 done.\n",
      "Sheet: 02052023 | Article: article10 done.\n",
      "Sheet: 03052023 | Article: article10 done.\n",
      "Sheet: 04052023 | Article: article10 done.\n"
     ]
    }
   ],
   "source": [
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excel_to_csv(excel_file):\n",
    "    # Load the Excel file\n",
    "    xls = pd.ExcelFile(excel_file)\n",
    "\n",
    "    # Get the sheet names\n",
    "    sheet_names = xls.sheet_names\n",
    "\n",
    "    # Iterate over each sheet and convert to CSV\n",
    "    for sheet_name in sheet_names:\n",
    "        # Read the sheet as a DataFrame\n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "\n",
    "        # Generate the CSV file name\n",
    "        csv_file = f\"{sheet_name}.csv\"\n",
    "\n",
    "        # Save the DataFrame as a CSV file\n",
    "        df.to_csv(csv_file, index=False)\n",
    "\n",
    "        print(f\"CSV file '{csv_file}' created for sheet '{sheet_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'Sheet 1.csv' created for sheet 'Sheet 1'\n",
      "CSV file 'Sheet 2.csv' created for sheet 'Sheet 2'\n",
      "CSV file 'Sheet 3.csv' created for sheet 'Sheet 3'\n",
      "CSV file 'Sheet 4.csv' created for sheet 'Sheet 4'\n"
     ]
    }
   ],
   "source": [
    "excel_to_csv(\"/home/pierluigi/Desktop/lda.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
