{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_article_text(url):\n",
    "    # Set headers to mimic a web browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the URL with headers\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract the title, subtitle, description, and main text\n",
    "    title_element = soup.find('title')\n",
    "    title = title_element.text.strip() if title_element else \"\"\n",
    "\n",
    "    subtitle_element = soup.find('meta', attrs={'name': 'description'})\n",
    "    subtitle = subtitle_element['content'].strip() if subtitle_element and 'content' in subtitle_element.attrs else \"\"\n",
    "\n",
    "    description_element = soup.find('meta', attrs={'name': 'og:description'})\n",
    "    description = description_element['content'].strip() if description_element and 'content' in description_element.attrs else \"\"\n",
    "\n",
    "    # Find and exclude unwanted elements by class names or content patterns\n",
    "    unwanted_elements = soup.find_all(['script', 'style', 'a', 'div', 'span'], class_=['follow-us', 'newsletter', 'advertisement'])\n",
    "    patterns_to_exclude = ['next article', 'read next', 'correlated']\n",
    "    for element in unwanted_elements:\n",
    "        if any(pattern in str(element).lower() for pattern in patterns_to_exclude):\n",
    "            element.extract()\n",
    "\n",
    "    # Find and exclude footer container and \"All rights reserved\" text\n",
    "    footer_elements = soup.find_all(['footer', 'div'], class_=['footer', 'bottom-footer'])\n",
    "    for element in footer_elements:\n",
    "        element.extract()\n",
    "    all_rights_reserved_elements = soup.find_all(text=re.compile(r'\\bAll rights reserved\\b', re.IGNORECASE))\n",
    "    for element in all_rights_reserved_elements:\n",
    "        element.extract()\n",
    "\n",
    "    # Find the main text element(s) based on the HTML structure of the page\n",
    "    main_text_elements = soup.find_all('p')\n",
    "    main_text = \"\\n\\n\".join([element.text.strip() for element in main_text_elements if element.text.strip()])\n",
    "\n",
    "    # Set the subtitle to the description if it is empty\n",
    "    if not subtitle:\n",
    "        subtitle = description.strip()\n",
    "\n",
    "    # Concatenate the extracted strings\n",
    "    article_text = f\"{title}\\n\\n{subtitle}\\n\\n{main_text}\"\n",
    "\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Perform lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Generate bigrams\n",
    "    bigrams = list(nltk.bigrams(lemmatized_words))\n",
    "    lemmatized_bigrams = [bigram for bigram in bigrams]\n",
    "    \n",
    "    return lemmatized_bigrams\n",
    "\n",
    "def apply_lda_to_articles(article_text, min_topics, max_topics):\n",
    "    lemmatized_bigrams = preprocess_text(article_text)\n",
    "            \n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(lemmatized_bigrams)\n",
    "    \n",
    "    # Create Corpus\n",
    "    texts = lemmatized_bigrams\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    \n",
    "    if not corpus:\n",
    "        print(\"Error: No data in the corpus. Unable to compute LDA model.\")\n",
    "        return None\n",
    "    \n",
    "    best_num_topics = None\n",
    "    best_coherence = float('-inf')\n",
    "    \n",
    "    for num_topics in range(min_topics, max_topics + 1):\n",
    "        lda_model = gensim.models.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=lemmatized_bigrams, dictionary=id2word, coherence='c_v')\n",
    "        coherence = coherence_model_lda.get_coherence()\n",
    "        \n",
    "        if coherence > best_coherence:\n",
    "            best_num_topics = num_topics\n",
    "            best_coherence = coherence\n",
    "    \n",
    "    if best_num_topics is not None:\n",
    "        return best_num_topics\n",
    "    else:\n",
    "        print(\"Error: Unable to determine the optimal number of topics.\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics based on article coherence:  3\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.foxnews.com/politics/biden-vetoes-bill-cancelling-student-loan-handout\"\n",
    "\n",
    "article = save_article_text(url)\n",
    "num_topics = apply_lda_to_articles(article, 1, 10)\n",
    "\n",
    "print(f'Number of topics based on article coherence: ', num_topics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
