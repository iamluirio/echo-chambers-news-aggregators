{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech (POS) tagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process in natural language processing (NLP) that involves labeling each word in a text corpus with its corresponding part of speech, such as noun, verb, adjective, etc. The NLTK (Natural Language Toolkit) is a popular Python library for NLP tasks, including POS tagging.\n",
    "\n",
    "NLTK provides several methods for POS tagging, including the default tagger, regular expression tagger, unigram tagger, and bigram tagger, among others. These taggers use various techniques, such as rule-based approaches, statistical models, and machine learning algorithms, to assign the appropriate POS tags to words in a text corpus.\n",
    "\n",
    "The accuracy of POS tagging depends on the quality of the training data and the effectiveness of the tagging algorithm used. POS tagging is a crucial step in many NLP applications, such as text classification, sentiment analysis, and information extraction, as it helps to identify the syntactic structure and meaning of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/pierluigi/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920fec5fa701433c837290e0647cb3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 16:41:49 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-04-27 16:41:50 INFO: File exists: /home/pierluigi/stanza_resources/en/default.zip\n",
      "2023-04-27 16:41:55 INFO: Finished downloading models and saved to /home/pierluigi/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import stanza\n",
    "stanza.download('en')  # Download the English model\n",
    "\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.foxnews.com/politics/republicans-respond-after-irs-whistleblower-says-hunter-biden-investigation-being-mishandled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(url):\n",
    "    # Create a newspaper Article object\n",
    "    article = newspaper.Article(url)\n",
    "\n",
    "    # Download and parse the article\n",
    "    article.download()\n",
    "    article.parse()\n",
    "\n",
    "    # Extract the title, subtitle, description, and main text\n",
    "    title = article.title.strip()\n",
    "    subtitle = article.meta_data.get(\"description\", \"\").strip()\n",
    "    description = article.meta_description.strip()\n",
    "    text = article.text.strip()\n",
    "\n",
    "    # Set the subtitle to the description if it is empty\n",
    "    if not subtitle:\n",
    "        subtitle = description.strip()\n",
    "\n",
    "    # Concatenate the extracted strings\n",
    "    article_text = f\"{title}\\n\\n{subtitle}\\n\\n{text}\"\n",
    "\n",
    "    # Return the concatenated string\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = get_article_info(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging Sentences and identifying adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: 2\n",
      "Sentence 2: 2\n",
      "Sentence 3: 1\n",
      "Sentence 4: 1\n",
      "Sentence 5: 1\n",
      "Sentence 6: 3\n",
      "Sentence 7: 3\n",
      "Sentence 8: 6\n",
      "Sentence 9: 0\n",
      "Sentence 10: 0\n",
      "Sentence 11: 2\n",
      "Sentence 12: 1\n",
      "Sentence 13: 0\n",
      "Sentence 14: 1\n",
      "Sentence 15: 3\n",
      "Sentence 16: 0\n",
      "Sentence 17: 0\n",
      "Sentence 18: 2\n",
      "Sentence 19: 2\n",
      "Sentence 20: 2\n",
      "Sentence 21: 2\n",
      "Sentence 22: 2\n",
      "Sentence 23: 5\n",
      "Sentence 24: 3\n",
      "Sentence 25: 1\n",
      "Sentence 26: 2\n",
      "Sentence 27: 0\n",
      "Sentence 28: 2\n",
      "Sentence 29: 7\n",
      "Sentence 30: 5\n",
      "Sentence 31: 0\n",
      "Sentence 32: 0\n",
      "Sentence 33: 0\n"
     ]
    }
   ],
   "source": [
    "sentence = nltk.sent_tokenize(article)\n",
    "num_adjectives_list = []\n",
    "for sent in sentence:\n",
    "    tagged_words = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "    num_adjectives = len([word for word, tag in tagged_words if tag.startswith('JJ')])\n",
    "    num_adjectives_list.append(num_adjectives)\n",
    "\n",
    "for i, sent in enumerate(sentence):\n",
    "    print(f\"Sentence {i+1}: {num_adjectives_list[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 830\n",
      "Total adjectives: 61\n",
      "Average number of adjectives in the article: 0.07 in this article.\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(article)\n",
    "\n",
    "total_adjectives = 0\n",
    "total_words = 0\n",
    "\n",
    "for sent in sentences:\n",
    "    words = word_tokenize(sent)\n",
    "    tagged_words = pos_tag(words)\n",
    "    num_adjectives = len([word for word, tag in tagged_words if tag.startswith('JJ')])\n",
    "    total_adjectives += num_adjectives\n",
    "    total_words += len(words)\n",
    "\n",
    "avg_adjectives = total_adjectives / total_words\n",
    "\n",
    "print(f\"Total words: {total_words}\")\n",
    "print(f\"Total adjectives: {total_adjectives}\")\n",
    "print(f\"Average number of adjectives in the article: {avg_adjectives:.2f} in this article.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
